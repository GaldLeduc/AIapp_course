{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Model Monitor\n",
    "This notebook shows how to:\n",
    "* Host a machine learning model in Amazon SageMaker and capture inference requests, results, and metadata \n",
    "* Analyze a training dataset to generate baseline constraints\n",
    "* Monitor a live endpoint or batch transforms for violations against constraints\n",
    "\n",
    "---\n",
    "## Background\n",
    "\n",
    "Amazon SageMaker provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. Amazon SageMaker is a fully-managed service that encompasses the entire machine learning workflow. You can label and prepare your data, choose an algorithm, train a model, and then tune and optimize it for deployment. You can deploy your models to production with Amazon SageMaker to make predictions and lower costs than was previously possible.\n",
    "\n",
    "In addition, Amazon SageMaker enables you to capture the input, output and metadata for invocations of the models that you deploy. It also enables you to analyze the data and monitor its quality. In this notebook, you learn how Amazon SageMaker enables these capabilities.\n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "To get started, make sure you have these prerequisites completed.\n",
    "\n",
    "* Specify an AWS Region to host your model.\n",
    "* An IAM role ARN exists that is used to give Amazon SageMaker access to your data in Amazon Simple Storage Service (Amazon S3). See the documentation for how to fine tune the permissions needed. \n",
    "* Create an S3 bucket used to store the data used to train your model, any additional model data, and the data captured from model invocations. For demonstration purposes, you are using the same bucket for these. In reality, you might want to separate them with different security policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "isConfigCell": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoleArn: arn:aws:iam::369074678854:role/sagemaker-immersion-day-SageMakerExecutionRole-2UAKB1ISM2UJ\n",
      "Demo Bucket: sagemaker-us-east-1-369074678854\n",
      "Capture path: s3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/datacapture\n",
      "Report path: s3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/reports\n",
      "Preproc Code path: s3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/code/preprocessor.py\n",
      "Postproc Code path: s3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/code/postprocessor.py\n",
      "CPU times: user 1.1 s, sys: 266 ms, total: 1.36 s\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# cell 01\n",
    "\n",
    "# Handful of configuration\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "from sagemaker import get_execution_role, session\n",
    "\n",
    "region= boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))\n",
    "\n",
    "# You can use a different bucket, but make sure the role you chose for this notebook\n",
    "# has the s3:PutObject permissions. This is the bucket into which the data is captured\n",
    "bucket =  session.Session(boto3.Session()).default_bucket()\n",
    "print(\"Demo Bucket: {}\".format(bucket))\n",
    "prefix = 'sagemaker/DEMO-ModelMonitor'\n",
    "\n",
    "data_capture_prefix = '{}/datacapture'.format(prefix)\n",
    "s3_capture_upload_path = 's3://{}/{}'.format(bucket, data_capture_prefix)\n",
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "s3_report_path = 's3://{}/{}'.format(bucket,reports_prefix)\n",
    "code_prefix = '{}/code'.format(prefix)\n",
    "s3_code_preprocessor_uri = 's3://{}/{}/{}'.format(bucket,code_prefix, 'preprocessor.py')\n",
    "s3_code_postprocessor_uri = 's3://{}/{}/{}'.format(bucket,code_prefix, 'postprocessor.py')\n",
    "\n",
    "print(\"Capture path: {}\".format(s3_capture_upload_path))\n",
    "print(\"Report path: {}\".format(s3_report_path))\n",
    "print(\"Preproc Code path: {}\".format(s3_code_preprocessor_uri))\n",
    "print(\"Postproc Code path: {}\".format(s3_code_postprocessor_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can quickly verify that the execution role for this notebook has the necessary permissions to proceed. Put a simple test object into the S3 bucket you speciÔ¨Åed above. If this command fails, update the role to have `s3:PutObject` permission on the bucket and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! You are all set to proceed.\n"
     ]
    }
   ],
   "source": [
    "# cell 02\n",
    "# Upload some test files\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\"test_upload/test.txt\").upload_file('test_data/upload-test-file.txt')\n",
    "print(\"Success! You are all set to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Option 1: Model monitoring with Real time endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PART A: Capturing real-time inference data from Amazon SageMaker endpoints\n",
    "Create an endpoint to showcase the data capture capability in action.\n",
    "\n",
    "### Upload the pre-trained model to Amazon S3\n",
    "This code uploads a pre-trained XGBoost model that is ready for you to deploy. This model was trained using the XGB Churn Prediction Notebook in SageMaker. You can also use your own pre-trained model in this step. If you already have a pretrained model in Amazon S3, you can add it instead by specifying the s3_key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 03\n",
    "model_file = open(\"model/xgb-churn-prediction-model.tar.gz\", 'rb')\n",
    "s3_key = os.path.join(prefix, 'xgb-churn-prediction-model.tar.gz')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(s3_key).upload_fileobj(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the model to Amazon SageMaker\n",
    "Start with deploying a pre-trained churn prediction model. Here, you create the model object with the image and model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 04\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "model_name = \"DEMO-xgb-churn-pred-model-monitor-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = 'https://{}.s3-{}.amazonaws.com/{}/xgb-churn-prediction-model.tar.gz'.format(bucket, region, prefix)\n",
    "image_uri = retrieve(region=boto3.Session().region_name, framework='xgboost', version='0.90-2')\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=model_url, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable data capture for monitoring the model data quality, you specify the new capture option called `DataCaptureConfig`. You can capture the request payload, the response payload or both with this configuration. The capture config applies to all variants. Go ahead with the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName=DEMO-xgb-churn-pred-model-monitor-2023-05-14-04-01-38\n",
      "------!"
     ]
    }
   ],
   "source": [
    "# cell 05\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "endpoint_name = 'DEMO-xgb-churn-pred-model-monitor-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"EndpointName={}\".format(endpoint_name))\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "                        enable_capture=True,\n",
    "                        sampling_percentage=100,\n",
    "                        destination_s3_uri=s3_capture_upload_path)\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1,\n",
    "                instance_type='ml.m4.xlarge',\n",
    "                endpoint_name=endpoint_name,\n",
    "                data_capture_config=data_capture_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Invoke the deployed model\n",
    "\n",
    "You can now send data to this endpoint to get inferences in real time. Because you enabled the data capture in the previous steps, the request and response payload, along with some additional metadata, is saved in the Amazon Simple Storage Service (Amazon S3) location you have specified in the DataCaptureConfig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step invokes the endpoint with included sample data for about 2 minutes. Data is captured based on the sampling percentage specified and the capture continues until the data capture option is turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending test traffic to the endpoint DEMO-xgb-churn-pred-model-monitor-2023-05-14-04-01-38. \n",
      "Please wait...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# cell 06\n",
    "from sagemaker.predictor import Predictor\n",
    "import sagemaker\n",
    "import time\n",
    "\n",
    "predictor = Predictor(endpoint_name=endpoint_name, serializer=sagemaker.serializers.CSVSerializer())\n",
    "\n",
    "# get a subset of test data for a quick test\n",
    "!head -120 test_data/test-dataset-input-cols.csv > test_data/test_sample.csv\n",
    "print(\"Sending test traffic to the endpoint {}. \\nPlease wait...\".format(endpoint_name))\n",
    "\n",
    "with open('test_data/test_sample.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = predictor.predict(data=payload)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "print(\"Done!\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### View captured data\n",
    "\n",
    "Now list the data capture files stored in Amazon S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Capture Files:\n",
      "sagemaker/DEMO-ModelMonitor/datacapture/DEMO-xgb-churn-pred-model-monitor-2023-05-14-04-01-38/AllTraffic/2023/05/14/04/05-11-449-b996c993-fb8e-4a9d-8538-b0ac18ba143e.jsonl\n",
      " sagemaker/DEMO-ModelMonitor/datacapture/DEMO-xgb-churn-pred-model-monitor-2023-05-14-04-01-38/AllTraffic/2023/05/14/04/06-11-764-7f1fda69-f5db-4ef5-9863-41caf67cff31.jsonl\n"
     ]
    }
   ],
   "source": [
    "# cell 07\n",
    "s3_client = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}/{}'.format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, view the contents of a single capture file. Here you should see all the data captured in an Amazon SageMaker specific JSON-line formatted file. Take a quick peek at the first few lines in the captured file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"92,0,176.3,85,93.4,125,207.2,107,9.6,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"0.039806101471185684\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"bd85d99d-aa47-4e74-8f68-c40acb4c4d74\",\"inferenceTime\":\"2023-05-14T04:06:11Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"138,0,46.5,104,186.0,114,167.5,95,9.6,4,4,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"0.9562002420425415\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"6297110c-9663-4c94-a908-f43ef6c4e709\",\"inferenceTime\":\"2023-05-14T04:06:12Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"93,0,176.1,103,199.7,130,263.9,96,8.5,6,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"0.007474285550415516\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"777b4d01-3125-4684-a1e6-1bde9c1df145\",\"inferenceTime\":\"2023-05-14T04:06:12Z\"},\"eventVersion\":\"0\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cell 08\n",
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the contents of a single line is present below in a formatted JSON file so that you can observe a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"92,0,176.3,85,93.4,125,207.2,107,9.6,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,0\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"0.039806101471185684\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"bd85d99d-aa47-4e74-8f68-c40acb4c4d74\",\n",
      "    \"inferenceTime\": \"2023-05-14T04:06:11Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# cell 09\n",
    "import json\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each inference request is captured in one line in the jsonl file. The line contains both the input and output merged together. In the example, you provided the ContentType as `text/csv` which is reflected in the `observedContentType` value. Also, you expose the encoding that you used to encode the input and output payloads in the capture format with the `encoding` value.\n",
    "\n",
    "To recap, you observed how you can enable capturing the input or output payloads to an endpoint with a new parameter. You have also observed what the captured format looks like in Amazon S3. Next, continue to explore how Amazon SageMaker helps with monitoring the data collected in Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PART B: Model Monitor - Baseling and continuous monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to collecting the data, Amazon SageMaker provides the capability for you to monitor and evaluate the data observed by the endpoints. For this:\n",
    "1. Create a baseline with which you compare the realtime traffic. \n",
    "1. Once a baseline is ready, setup a schedule to continously evaluate and compare against the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Constraint suggestion with baseline/training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset with which you trained the model is usually a good baseline dataset. Note that the training dataset data schema and the inference dataset schema should exactly match (i.e. the number and order of the features).\n",
    "\n",
    "From the training dataset you can ask Amazon SageMaker to suggest a set of baseline `constraints` and generate descriptive `statistics` to explore the data. For this example, upload the training dataset that was used to train the pre-trained model included in this example. If you already have it in Amazon S3, you can directly point to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/baselining/data\n",
      "Baseline results uri: s3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/baselining/results\n"
     ]
    }
   ],
   "source": [
    "# cell 10\n",
    "# copy over the training dataset to Amazon S3 (if you already have it in Amazon S3, you could reuse it)\n",
    "baseline_prefix = prefix + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(bucket,baseline_data_prefix)\n",
    "baseline_results_uri = 's3://{}/{}'.format(bucket, baseline_results_prefix)\n",
    "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 11\n",
    "training_data_file = open(\"test_data/training-dataset-with-header.csv\", 'rb')\n",
    "s3_key = os.path.join(baseline_prefix, 'data', 'training-dataset-with-header.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(s3_key).upload_fileobj(training_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create a baselining job with training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the training data ready in Amazon S3, start a job to `suggest` constraints. `DefaultModelMonitor.suggest_baseline(..)` starts a `ProcessingJob` using an Amazon SageMaker provided Model Monitor container to generate the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2023-05-14-04-37-48-709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................\u001b[34m2023-05-14 04:42:04,336 - matplotlib.font_manager - INFO - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:04.876862: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:04.876893: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:06.423848: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:06.423879: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:06.423904: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-106-114.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:06.424183: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:07,980 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:369074678854:processing-job/baseline-suggestion-job-2023-05-14-04-37-48-709', 'ProcessingJobName': 'baseline-suggestion-job-2023-05-14-04-37-48-709', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/baselining/data/training-dataset-with-header.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/baselining/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::369074678854:role/sagemaker-immersion-day-SageMakerExecutionRole-2UAKB1ISM2UJ', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:07,980 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:07,980 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:07,980 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:07,980 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:08,037 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:08,038 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:08,038 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:08,046 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:08,046 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:08,046 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:08,510 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.106.114\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hado\u001b[0m\n",
      "\u001b[34mop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_362\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:08,517 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:08,520 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-a52735aa-afaf-446d-9621-5d5bc2c8e150\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,007 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,018 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,019 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,021 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,025 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,026 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,026 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,026 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,056 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,067 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,067 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,070 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,074 INFO blockmanagement.BlockManager: The block deletion will start around 2023 May 14 04:42:09\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,075 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,075 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,076 INFO util.GSet: 2.0% max memory 3.1 GB = 63.9 MB\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,076 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,153 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,157 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,157 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,157 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,158 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,158 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,158 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,158 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,158 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,158 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,158 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,158 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,184 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,184 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,184 INFO util.GSet: 1.0% max memory 3.1 GB = 31.9 MB\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,184 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,186 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,186 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,186 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,186 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,190 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,194 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,194 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,194 INFO util.GSet: 0.25% max memory 3.1 GB = 8.0 MB\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,194 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,200 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,200 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,200 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,203 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,203 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,205 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,205 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,205 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 980.9 KB\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,205 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,226 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1316526380-10.2.106.114-1684039329219\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,237 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,244 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,320 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,331 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,334 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.106.114\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:09,345 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:11,401 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:11,402 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:13,460 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:13,461 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:15,535 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:15,535 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:17,634 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:17,634 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:19,756 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:19,760 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:29,764 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:31,303 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:31,681 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:31,719 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:31,728 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,148 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,169 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,170 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,170 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,170 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,192 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11536, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,209 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,210 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,260 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,260 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,260 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,261 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,261 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,548 INFO util.Utils: Successfully started service 'sparkDriver' on port 34049.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,576 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,610 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,627 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,628 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,660 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,681 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-2b95b6e1-9310-46da-8016-72d3faa983a4\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,697 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,735 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:32,769 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.106.114:34049/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1684039352144\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:33,327 INFO client.RMProxy: Connecting to ResourceManager at /10.2.106.114:8032\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:34,084 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:34,084 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:34,090 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15731 MB per container)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:34,091 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:34,091 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:34,091 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:34,097 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:34,185 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:35,975 INFO yarn.Client: Uploading resource file:/tmp/spark-ece4696b-ab96-408a-9fa4-3c1d1cb9098c/__spark_libs__449040048768911364.zip -> hdfs://10.2.106.114/user/root/.sparkStaging/application_1684039334974_0001/__spark_libs__449040048768911364.zip\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:37,599 INFO yarn.Client: Uploading resource file:/tmp/spark-ece4696b-ab96-408a-9fa4-3c1d1cb9098c/__spark_conf__8713127414935691396.zip -> hdfs://10.2.106.114/user/root/.sparkStaging/application_1684039334974_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:37,654 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:37,654 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:37,654 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:37,654 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:37,654 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:37,687 INFO yarn.Client: Submitting application application_1684039334974_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:37,905 INFO impl.YarnClientImpl: Submitted application application_1684039334974_0001\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:38,909 INFO yarn.Client: Application report for application_1684039334974_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:38,913 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1684039357805\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1684039334974_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:39,917 INFO yarn.Client: Application report for application_1684039334974_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:40,920 INFO yarn.Client: Application report for application_1684039334974_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:41,926 INFO yarn.Client: Application report for application_1684039334974_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:42,832 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1684039334974_0001), /proxy/application_1684039334974_0001\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:42,931 INFO yarn.Client: Application report for application_1684039334974_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:42,932 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.106.114\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1684039357805\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1684039334974_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:42,934 INFO cluster.YarnClientSchedulerBackend: Application application_1684039334974_0001 has started running.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:42,943 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37399.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:42,943 INFO netty.NettyBlockTransferService: Server created on 10.2.106.114:37399\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:42,945 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:42,953 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.106.114, 37399, None)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:42,957 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.106.114:37399 with 1458.6 MiB RAM, BlockManagerId(driver, 10.2.106.114, 37399, None)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:42,963 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.106.114, 37399, None)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:42,973 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.106.114, 37399, None)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:43,101 INFO util.log: Logging initialized @13137ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:44,376 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:48,681 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.106.114:48140) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:42:48,853 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:40443 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 40443, None)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:03,224 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:03,438 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:03,502 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:03,513 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:04,647 INFO datasources.InMemoryFileIndex: It took 41 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:04,844 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,150 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,153 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.106.114:37399 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,157 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,470 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,472 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,475 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 375873\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,522 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,538 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,538 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,539 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,540 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,552 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,602 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,605 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,606 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.106.114:37399 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,607 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,622 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,622 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,662 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4641 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:05,952 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:40443 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:06,850 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:40443 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:07,285 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1638 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:07,287 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:07,302 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.729 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:07,311 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:07,314 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:07,316 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.794277 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:07,550 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.106.114:37399 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:07,551 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:40443 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:07,564 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.2.106.114:37399 in memory (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:07,566 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:40443 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,514 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,516 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,519 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Churn: string, Account Length: string, VMail Message: string, Day Mins: string, Day Calls: string ... 68 more fields>\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,560 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,786 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,800 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,801 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.106.114:37399 (size: 39.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,802 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,816 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,853 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,855 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:100) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,855 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:100)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,855 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,858 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,859 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,917 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 29.8 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,919 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,920 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.106.114:37399 (size: 11.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,920 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,921 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,921 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,924 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:09,966 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:40443 (size: 11.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:10,982 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:40443 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:11,305 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:40443 (size: 188.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:11,465 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1543 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:11,465 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:11,466 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:100) finished in 1.598 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:11,468 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:11,471 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:11,471 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:100, took 1.617785 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:11,901 INFO codegen.CodeGenerator: Code generated in 304.558079 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,576 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,580 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,581 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,581 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,583 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,585 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,610 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 125.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,612 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,613 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.106.114:37399 (size: 37.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,614 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,616 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,616 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,624 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:12,655 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:40443 (size: 37.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,672 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1048 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,672 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,674 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.085 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,675 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,676 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,676 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,677 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,785 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,787 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,787 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,787 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,787 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,788 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,801 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.7 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,804 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,805 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.106.114:37399 (size: 49.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,807 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,807 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,807 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,810 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,827 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:40443 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:13,872 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,229 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 420 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,229 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,230 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.436 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,230 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,230 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,230 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.445643 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,258 INFO codegen.CodeGenerator: Code generated in 20.599393 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,541 INFO codegen.CodeGenerator: Code generated in 31.595238 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,651 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,653 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,653 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,653 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,654 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,655 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,687 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 49.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,690 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,691 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.106.114:37399 (size: 19.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,691 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,692 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,692 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,694 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:14,716 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:40443 (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,154 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 460 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,155 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.495 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,155 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,155 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,156 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,156 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.504578 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,466 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.106.114:37399 in memory (size: 11.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,468 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:40443 in memory (size: 11.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,514 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:40443 in memory (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,525 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.106.114:37399 in memory (size: 49.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,556 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.106.114:37399 in memory (size: 19.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,565 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:40443 in memory (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,593 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.106.114:37399 in memory (size: 37.6 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,595 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:40443 in memory (size: 37.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,797 INFO codegen.CodeGenerator: Code generated in 108.278596 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,805 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,805 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,806 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,806 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,807 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,808 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,817 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 87.0 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,819 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,819 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.106.114:37399 (size: 27.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,820 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,820 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,820 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,821 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,836 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:40443 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,925 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 103 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,926 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,927 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.118 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,928 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,928 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,928 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:15,928 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,139 INFO codegen.CodeGenerator: Code generated in 87.603176 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,159 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,161 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,161 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,162 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,162 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,163 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,167 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 67.4 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,177 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,178 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.106.114:37399 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,179 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,179 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,179 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,181 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,196 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:40443 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,213 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,385 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 203 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,385 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,386 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.220 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,387 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,388 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,388 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.228828 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,513 INFO codegen.CodeGenerator: Code generated in 84.397823 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,694 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,698 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,699 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,699 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,700 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,700 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,703 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,721 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 41.9 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,723 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,724 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,724 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,725 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,725 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,727 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:16,741 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,007 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1280 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,009 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,010 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.306 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,010 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,011 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,011 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,012 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,013 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,020 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,022 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,023 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,025 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,027 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,027 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,030 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,040 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,046 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,105 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 76 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,113 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,114 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.098 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,125 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,125 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,125 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.430664 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,316 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,316 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,316 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,317 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,317 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,318 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,323 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 94.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,325 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,325 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.106.114:37399 (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,326 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,326 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,326 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,328 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,336 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:40443 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,498 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 171 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,498 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,500 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.181 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,504 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,505 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,505 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,505 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,561 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,563 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,563 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,563 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,563 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,564 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,571 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 179.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,574 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,575 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.106.114:37399 (size: 49.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,575 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,576 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,576 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,577 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,591 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:40443 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,607 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,718 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 141 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,719 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,720 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.154 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,720 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,720 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,720 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.158884 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,911 INFO codegen.CodeGenerator: Code generated in 18.501837 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,943 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,944 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,944 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,944 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,945 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,946 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,955 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 49.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,957 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,957 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.106.114:37399 (size: 19.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,958 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,959 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,959 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,961 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:18,976 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:40443 (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,097 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 137 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,098 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.150 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,099 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,100 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,100 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,100 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.157292 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,404 INFO codegen.CodeGenerator: Code generated in 104.726567 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,453 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,455 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,455 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,455 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,456 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,456 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,466 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 86.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,468 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,468 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.106.114:37399 (size: 27.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,469 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,470 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,470 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,471 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,481 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:40443 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,484 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.106.114:37399 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,526 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:40443 (size: 27.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,539 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.106.114:37399 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,544 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:40443 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,559 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.2.106.114:37399 in memory (size: 19.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,564 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:40443 in memory (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,581 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.106.114:37399 in memory (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,584 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:40443 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,637 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.106.114:37399 in memory (size: 27.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,638 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 167 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,638 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,639 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.182 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,640 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,640 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,641 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,641 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,647 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:40443 in memory (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,729 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:40443 in memory (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,734 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.106.114:37399 in memory (size: 49.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,755 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.106.114:37399 in memory (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,756 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:40443 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,824 INFO codegen.CodeGenerator: Code generated in 68.498819 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,835 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,836 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,837 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,837 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,837 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,838 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,840 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.8 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,842 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,842 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.106.114:37399 (size: 19.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,843 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,843 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,844 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,845 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,859 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:40443 (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,863 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,954 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 109 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,955 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,956 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.116 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,956 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,956 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:19,957 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.121933 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,048 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,050 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,050 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,051 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,051 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,051 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,052 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,058 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 41.9 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,060 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,060 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,061 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,062 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,062 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,063 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,082 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,199 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 136 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,200 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,201 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.147 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,201 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,201 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,202 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,202 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,202 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,204 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,206 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,207 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,207 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,208 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,209 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,210 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,223 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,227 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,272 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 62 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,272 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,275 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.070 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,275 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,276 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,276 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.227759 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,548 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,548 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,549 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,549 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,550 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,551 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,556 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 94.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,558 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,558 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.106.114:37399 (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,559 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,559 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,560 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,561 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,579 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:40443 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,869 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 308 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,869 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,870 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.318 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,871 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,871 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,872 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,872 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,930 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,932 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,932 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,932 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,933 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,933 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,940 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 179.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,942 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,943 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.2.106.114:37399 (size: 49.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,943 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,943 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,943 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,945 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,954 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:40443 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:20,964 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,040 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 95 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,040 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,041 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.107 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,041 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,041 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,042 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.110978 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,137 INFO codegen.CodeGenerator: Code generated in 9.923248 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,167 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,169 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,171 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,172 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,172 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,173 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,183 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 48.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,187 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,189 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.2.106.114:37399 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,193 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,193 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,194 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,195 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,205 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:40443 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,312 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 117 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,312 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,312 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.138 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,313 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,314 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,315 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.146988 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,565 INFO codegen.CodeGenerator: Code generated in 36.217353 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,572 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,572 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,573 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,573 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,574 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,574 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,579 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 87.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,581 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,581 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.2.106.114:37399 (size: 27.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,582 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,585 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,586 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,587 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,600 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:40443 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,706 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 119 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,706 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,707 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.131 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,707 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,707 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,707 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,707 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,809 INFO codegen.CodeGenerator: Code generated in 40.311075 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,820 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,821 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,821 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,821 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,822 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,822 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,824 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 67.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,826 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,826 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.2.106.114:37399 (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,827 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,828 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,828 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,829 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,840 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:40443 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,844 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,887 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 58 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,887 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,888 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.066 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,888 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,888 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,888 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.067947 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,988 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,989 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,989 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,989 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,990 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,990 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:21,991 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,001 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:40443 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,006 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.2.106.114:37399 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,023 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 41.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,024 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,024 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,025 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,025 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,025 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,026 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,038 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,043 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:40443 in memory (size: 27.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,050 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.2.106.114:37399 in memory (size: 27.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,094 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.2.106.114:37399 in memory (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,096 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:40443 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,105 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 79 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,105 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,108 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.115 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,108 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,108 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,109 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,109 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,109 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,111 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,112 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,112 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,113 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,113 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,113 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,114 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,122 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,127 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,128 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:40443 in memory (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,134 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.2.106.114:37399 in memory (size: 19.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,147 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 33 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,147 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,147 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.037 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,147 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,148 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,148 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.160095 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,158 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.2.106.114:37399 in memory (size: 30.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,161 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:40443 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,189 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.2.106.114:37399 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,189 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:40443 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,198 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.2.106.114:37399 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,198 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:40443 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,210 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.2.106.114:37399 in memory (size: 27.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,211 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:40443 in memory (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,219 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:40443 in memory (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,225 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.2.106.114:37399 in memory (size: 49.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,315 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,316 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,316 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,316 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,316 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,317 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,321 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 94.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,322 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,323 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.2.106.114:37399 (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,323 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,324 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,324 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,325 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,337 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:40443 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,467 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 142 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,467 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,469 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.152 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,470 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,470 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,470 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,470 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,510 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,511 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,511 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,511 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,511 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,511 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,518 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 179.7 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,520 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,521 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.2.106.114:37399 (size: 49.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,521 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,521 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,521 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,523 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,535 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:40443 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,545 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,658 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 136 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,658 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,659 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.147 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,659 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,660 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,660 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.150003 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,755 INFO codegen.CodeGenerator: Code generated in 12.697728 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,787 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,788 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,788 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,788 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,789 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,789 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,804 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 48.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,806 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,806 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.2.106.114:37399 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,809 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,809 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,809 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,810 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,827 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:40443 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,913 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 103 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,913 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,914 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 0.124 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,915 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,915 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:22,916 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 0.128803 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,113 INFO codegen.CodeGenerator: Code generated in 43.494155 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,120 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,120 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,120 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,120 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,121 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,121 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,125 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 87.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,127 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,127 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.2.106.114:37399 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,127 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,128 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,128 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,129 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,142 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:40443 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,214 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 84 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,214 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,214 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.092 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,215 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,215 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,215 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,215 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,307 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,308 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,308 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,308 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,308 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,309 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,310 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 67.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,312 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,313 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.2.106.114:37399 (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,313 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,314 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,314 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,316 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,338 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:40443 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,342 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,348 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 32 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,348 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,348 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.039 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,349 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,349 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,351 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.041467 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,411 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,412 INFO scheduler.DAGScheduler: Registering RDD 150 (countByKey at ColumnProfiler.scala:592) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,412 INFO scheduler.DAGScheduler: Got job 25 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,412 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,412 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,412 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,413 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,419 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 41.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,420 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,421 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,421 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,422 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,422 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,423 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,432 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,483 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 60 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,483 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,484 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (countByKey at ColumnProfiler.scala:592) finished in 0.070 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,484 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,484 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,484 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 37)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,484 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,485 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,486 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,488 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,488 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,490 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,490 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,492 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,493 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,506 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,510 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,519 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 26 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,519 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,519 INFO scheduler.DAGScheduler: ResultStage 37 (countByKey at ColumnProfiler.scala:592) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,520 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,520 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,521 INFO scheduler.DAGScheduler: Job 25 finished: countByKey at ColumnProfiler.scala:592, took 0.109530 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,616 INFO scheduler.DAGScheduler: Registering RDD 156 (collect at AnalysisRunner.scala:326) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,617 INFO scheduler.DAGScheduler: Got map stage job 26 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,617 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,617 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,618 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,618 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,622 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 94.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,623 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,624 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.2.106.114:37399 (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,624 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,625 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,625 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,630 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,647 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:40443 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,775 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 145 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,775 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,776 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326) finished in 0.157 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,776 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,777 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,777 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,778 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,809 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,810 INFO scheduler.DAGScheduler: Got job 27 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,810 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,811 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,811 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,811 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,819 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 179.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,821 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,821 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.2.106.114:37399 (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,822 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,823 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,823 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,824 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,838 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:40443 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,854 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,960 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 136 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,960 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,961 INFO scheduler.DAGScheduler: ResultStage 40 (collect at AnalysisRunner.scala:326) finished in 0.149 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,965 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,965 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:23,965 INFO scheduler.DAGScheduler: Job 27 finished: collect at AnalysisRunner.scala:326, took 0.155826 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,096 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,097 INFO scheduler.DAGScheduler: Got job 28 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,097 INFO scheduler.DAGScheduler: Final stage: ResultStage 41 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,097 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,098 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,099 INFO scheduler.DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,106 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 48.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,109 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,109 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.2.106.114:37399 (size: 19.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,110 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,110 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,110 INFO cluster.YarnScheduler: Adding task set 41.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,113 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 32) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,126 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:40443 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,187 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 32) in 74 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,187 INFO cluster.YarnScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,187 INFO scheduler.DAGScheduler: ResultStage 41 (treeReduce at KLLRunner.scala:107) finished in 0.087 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,188 INFO scheduler.DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,188 INFO cluster.YarnScheduler: Killing all running tasks in stage 41: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,188 INFO scheduler.DAGScheduler: Job 28 finished: treeReduce at KLLRunner.scala:107, took 0.091343 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,353 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:40443 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,364 INFO scheduler.DAGScheduler: Registering RDD 174 (collect at AnalysisRunner.scala:326) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,364 INFO scheduler.DAGScheduler: Got map stage job 29 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,364 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,364 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,366 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,366 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,369 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.2.106.114:37399 in memory (size: 19.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,371 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 87.4 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,372 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,373 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.2.106.114:37399 (size: 27.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,373 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,373 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,373 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,374 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,390 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:40443 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,394 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.2.106.114:37399 in memory (size: 19.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,402 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:40443 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,407 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.2.106.114:37399 in memory (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,408 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:40443 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,413 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 39 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,413 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,414 INFO scheduler.DAGScheduler: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326) finished in 0.046 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,414 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,414 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,414 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,414 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,436 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:40443 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,443 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.2.106.114:37399 in memory (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,446 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.2.106.114:37399 in memory (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,450 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:40443 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,453 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.2.106.114:37399 in memory (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,454 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:40443 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,456 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.2.106.114:37399 in memory (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,458 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:40443 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,462 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.2.106.114:37399 in memory (size: 49.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,464 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:40443 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,468 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.2.106.114:37399 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,469 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:40443 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,473 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.2.106.114:37399 in memory (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,477 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:40443 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,477 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,478 INFO scheduler.DAGScheduler: Got job 30 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,478 INFO scheduler.DAGScheduler: Final stage: ResultStage 44 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,478 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,478 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,479 INFO scheduler.DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,480 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 67.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,482 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,482 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.2.106.114:37399 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,483 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,483 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,483 INFO cluster.YarnScheduler: Adding task set 44.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,485 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 44.0 (TID 34) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,490 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.2.106.114:37399 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,491 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:40443 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,494 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:40443 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,496 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.2.106.114:37399 in memory (size: 49.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,498 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:40443 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,499 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,505 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 44.0 (TID 34) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,505 INFO cluster.YarnScheduler: Removed TaskSet 44.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,506 INFO scheduler.DAGScheduler: ResultStage 44 (collect at AnalysisRunner.scala:326) finished in 0.026 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,506 INFO scheduler.DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,507 INFO cluster.YarnScheduler: Killing all running tasks in stage 44: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,507 INFO scheduler.DAGScheduler: Job 30 finished: collect at AnalysisRunner.scala:326, took 0.029824 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,554 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,555 INFO scheduler.DAGScheduler: Registering RDD 185 (countByKey at ColumnProfiler.scala:592) as input to shuffle 14\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,555 INFO scheduler.DAGScheduler: Got job 31 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,555 INFO scheduler.DAGScheduler: Final stage: ResultStage 46 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,555 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,556 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,556 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,561 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 41.9 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,562 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,562 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,563 INFO spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,563 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,563 INFO cluster.YarnScheduler: Adding task set 45.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,564 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 35) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,577 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,625 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 35) in 61 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,625 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,626 INFO scheduler.DAGScheduler: ShuffleMapStage 45 (countByKey at ColumnProfiler.scala:592) finished in 0.069 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,627 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,627 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,627 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 46)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,628 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,628 INFO scheduler.DAGScheduler: Submitting ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,629 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,631 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,632 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,632 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,633 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,633 INFO cluster.YarnScheduler: Adding task set 46.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,634 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 46.0 (TID 36) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,648 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,652 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,659 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 46.0 (TID 36) in 25 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,659 INFO cluster.YarnScheduler: Removed TaskSet 46.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,659 INFO scheduler.DAGScheduler: ResultStage 46 (countByKey at ColumnProfiler.scala:592) finished in 0.030 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,659 INFO scheduler.DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,659 INFO cluster.YarnScheduler: Killing all running tasks in stage 46: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,660 INFO scheduler.DAGScheduler: Job 31 finished: countByKey at ColumnProfiler.scala:592, took 0.105010 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,798 INFO scheduler.DAGScheduler: Registering RDD 191 (collect at AnalysisRunner.scala:326) as input to shuffle 15\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,798 INFO scheduler.DAGScheduler: Got map stage job 32 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,798 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,798 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,799 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,799 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,804 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 94.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,806 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,807 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.2.106.114:37399 (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,807 INFO spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,808 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,808 INFO cluster.YarnScheduler: Adding task set 47.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,809 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 47.0 (TID 37) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,818 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on algo-1:40443 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,943 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 47.0 (TID 37) in 134 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,943 INFO cluster.YarnScheduler: Removed TaskSet 47.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,944 INFO scheduler.DAGScheduler: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326) finished in 0.143 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,944 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,944 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,944 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,944 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,971 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,971 INFO scheduler.DAGScheduler: Got job 33 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,972 INFO scheduler.DAGScheduler: Final stage: ResultStage 49 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,972 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,972 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,972 INFO scheduler.DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,980 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 179.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,982 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,982 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.2.106.114:37399 (size: 49.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,983 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,983 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,983 INFO cluster.YarnScheduler: Adding task set 49.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,985 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 49.0 (TID 38) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:24,994 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on algo-1:40443 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,006 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,062 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 49.0 (TID 38) in 77 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,062 INFO cluster.YarnScheduler: Removed TaskSet 49.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,063 INFO scheduler.DAGScheduler: ResultStage 49 (collect at AnalysisRunner.scala:326) finished in 0.090 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,063 INFO scheduler.DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,064 INFO cluster.YarnScheduler: Killing all running tasks in stage 49: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,064 INFO scheduler.DAGScheduler: Job 33 finished: collect at AnalysisRunner.scala:326, took 0.093136 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,178 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,179 INFO scheduler.DAGScheduler: Got job 34 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,179 INFO scheduler.DAGScheduler: Final stage: ResultStage 50 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,179 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,179 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,179 INFO scheduler.DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[204] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,188 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 48.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,190 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,190 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.2.106.114:37399 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,190 INFO spark.SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,193 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[204] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,193 INFO cluster.YarnScheduler: Adding task set 50.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,194 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 50.0 (TID 39) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,203 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on algo-1:40443 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,253 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 50.0 (TID 39) in 59 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,253 INFO cluster.YarnScheduler: Removed TaskSet 50.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,254 INFO scheduler.DAGScheduler: ResultStage 50 (treeReduce at KLLRunner.scala:107) finished in 0.074 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,254 INFO scheduler.DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,254 INFO cluster.YarnScheduler: Killing all running tasks in stage 50: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,254 INFO scheduler.DAGScheduler: Job 34 finished: treeReduce at KLLRunner.scala:107, took 0.076084 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,371 INFO scheduler.DAGScheduler: Registering RDD 209 (collect at AnalysisRunner.scala:326) as input to shuffle 16\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,371 INFO scheduler.DAGScheduler: Got map stage job 35 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,371 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 51 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,371 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,372 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,373 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[209] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,377 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 87.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,379 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,379 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.2.106.114:37399 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,380 INFO spark.SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,380 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[209] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,380 INFO cluster.YarnScheduler: Adding task set 51.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,382 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 51.0 (TID 40) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,391 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on algo-1:40443 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,413 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 51.0 (TID 40) in 32 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,413 INFO cluster.YarnScheduler: Removed TaskSet 51.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,414 INFO scheduler.DAGScheduler: ShuffleMapStage 51 (collect at AnalysisRunner.scala:326) finished in 0.039 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,414 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,414 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,414 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,414 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,458 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,459 INFO scheduler.DAGScheduler: Got job 36 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,459 INFO scheduler.DAGScheduler: Final stage: ResultStage 53 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,459 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,459 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,460 INFO scheduler.DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[212] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,462 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 67.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,464 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,464 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.2.106.114:37399 (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,465 INFO spark.SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,465 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[212] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,466 INFO cluster.YarnScheduler: Adding task set 53.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,467 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 53.0 (TID 41) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,475 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on algo-1:40443 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,481 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,490 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 53.0 (TID 41) in 23 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,490 INFO cluster.YarnScheduler: Removed TaskSet 53.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,490 INFO scheduler.DAGScheduler: ResultStage 53 (collect at AnalysisRunner.scala:326) finished in 0.029 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,491 INFO scheduler.DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,491 INFO cluster.YarnScheduler: Killing all running tasks in stage 53: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,492 INFO scheduler.DAGScheduler: Job 36 finished: collect at AnalysisRunner.scala:326, took 0.033200 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,532 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,532 INFO scheduler.DAGScheduler: Registering RDD 220 (countByKey at ColumnProfiler.scala:592) as input to shuffle 17\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,532 INFO scheduler.DAGScheduler: Got job 37 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,532 INFO scheduler.DAGScheduler: Final stage: ResultStage 55 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,532 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 54)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,532 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 54)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,534 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[220] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,539 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 41.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,540 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,541 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,541 INFO spark.SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,541 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[220] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,541 INFO cluster.YarnScheduler: Adding task set 54.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,542 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 54.0 (TID 42) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,551 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,586 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 54.0 (TID 42) in 44 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,586 INFO cluster.YarnScheduler: Removed TaskSet 54.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,587 INFO scheduler.DAGScheduler: ShuffleMapStage 54 (countByKey at ColumnProfiler.scala:592) finished in 0.053 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,587 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,587 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,587 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 55)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,587 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,588 INFO scheduler.DAGScheduler: Submitting ResultStage 55 (ShuffledRDD[221] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,589 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,590 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,590 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,591 INFO spark.SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,591 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (ShuffledRDD[221] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,591 INFO cluster.YarnScheduler: Adding task set 55.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,592 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 55.0 (TID 43) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,601 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,603 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,611 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 55.0 (TID 43) in 19 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,611 INFO cluster.YarnScheduler: Removed TaskSet 55.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,612 INFO scheduler.DAGScheduler: ResultStage 55 (countByKey at ColumnProfiler.scala:592) finished in 0.024 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,612 INFO scheduler.DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,612 INFO cluster.YarnScheduler: Killing all running tasks in stage 55: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,613 INFO scheduler.DAGScheduler: Job 37 finished: countByKey at ColumnProfiler.scala:592, took 0.080931 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,685 INFO scheduler.DAGScheduler: Registering RDD 226 (collect at AnalysisRunner.scala:326) as input to shuffle 18\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,685 INFO scheduler.DAGScheduler: Got map stage job 38 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,685 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 56 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,686 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,686 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,687 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 56 (MapPartitionsRDD[226] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,691 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 94.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,693 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,693 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.2.106.114:37399 (size: 30.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,693 INFO spark.SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,694 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[226] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,694 INFO cluster.YarnScheduler: Adding task set 56.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,695 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 56.0 (TID 44) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,706 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on algo-1:40443 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,786 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 56.0 (TID 44) in 92 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,786 INFO cluster.YarnScheduler: Removed TaskSet 56.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,787 INFO scheduler.DAGScheduler: ShuffleMapStage 56 (collect at AnalysisRunner.scala:326) finished in 0.099 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,787 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,787 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,787 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,787 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,826 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,827 INFO scheduler.DAGScheduler: Got job 39 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,827 INFO scheduler.DAGScheduler: Final stage: ResultStage 58 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,827 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 57)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,827 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,827 INFO scheduler.DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[229] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,833 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 179.7 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,835 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,836 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.2.106.114:37399 (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,836 INFO spark.SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,836 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[229] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,836 INFO cluster.YarnScheduler: Adding task set 58.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,837 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 58.0 (TID 45) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,849 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on algo-1:40443 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,859 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,941 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 58.0 (TID 45) in 104 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,941 INFO cluster.YarnScheduler: Removed TaskSet 58.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,941 INFO scheduler.DAGScheduler: ResultStage 58 (collect at AnalysisRunner.scala:326) finished in 0.113 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,943 INFO scheduler.DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,943 INFO cluster.YarnScheduler: Killing all running tasks in stage 58: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:25,944 INFO scheduler.DAGScheduler: Job 39 finished: collect at AnalysisRunner.scala:326, took 0.116893 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,023 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,024 INFO scheduler.DAGScheduler: Got job 40 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,024 INFO scheduler.DAGScheduler: Final stage: ResultStage 59 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,024 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,026 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,026 INFO scheduler.DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[239] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,074 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.2.106.114:37399 in memory (size: 30.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,074 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on algo-1:40443 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,086 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 48.9 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,087 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,088 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.2.106.114:37399 (size: 19.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,089 INFO spark.SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,090 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[239] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,090 INFO cluster.YarnScheduler: Adding task set 59.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,091 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 59.0 (TID 46) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,097 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.2.106.114:37399 in memory (size: 27.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,100 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on algo-1:40443 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,104 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.2.106.114:37399 in memory (size: 30.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,105 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on algo-1:40443 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,109 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on algo-1:40443 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,116 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.2.106.114:37399 in memory (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,117 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on algo-1:40443 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,120 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.2.106.114:37399 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,121 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on algo-1:40443 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,124 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 10.2.106.114:37399 in memory (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,136 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on algo-1:40443 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,140 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on 10.2.106.114:37399 in memory (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,141 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on algo-1:40443 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,143 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.2.106.114:37399 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,145 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on algo-1:40443 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,148 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.2.106.114:37399 in memory (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,150 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on algo-1:40443 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,158 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on algo-1:40443 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,160 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.2.106.114:37399 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,162 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.2.106.114:37399 in memory (size: 49.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,164 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on algo-1:40443 in memory (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,170 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.2.106.114:37399 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,172 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on algo-1:40443 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,175 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 10.2.106.114:37399 in memory (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,175 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on algo-1:40443 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,190 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 59.0 (TID 46) in 99 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,190 INFO cluster.YarnScheduler: Removed TaskSet 59.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,191 INFO scheduler.DAGScheduler: ResultStage 59 (treeReduce at KLLRunner.scala:107) finished in 0.128 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,192 INFO scheduler.DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,192 INFO cluster.YarnScheduler: Killing all running tasks in stage 59: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,192 INFO scheduler.DAGScheduler: Job 40 finished: treeReduce at KLLRunner.scala:107, took 0.168775 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,378 INFO scheduler.DAGScheduler: Registering RDD 244 (collect at AnalysisRunner.scala:326) as input to shuffle 19\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,379 INFO scheduler.DAGScheduler: Got map stage job 41 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,379 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 60 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,379 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,380 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,380 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 60 (MapPartitionsRDD[244] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,384 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 87.4 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,385 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,386 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.2.106.114:37399 (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,386 INFO spark.SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,386 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 60 (MapPartitionsRDD[244] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,386 INFO cluster.YarnScheduler: Adding task set 60.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,387 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 60.0 (TID 47) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,395 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on algo-1:40443 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,414 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 60.0 (TID 47) in 27 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,414 INFO cluster.YarnScheduler: Removed TaskSet 60.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,415 INFO scheduler.DAGScheduler: ShuffleMapStage 60 (collect at AnalysisRunner.scala:326) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,415 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,415 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,415 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,415 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,466 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,466 INFO scheduler.DAGScheduler: Got job 42 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,466 INFO scheduler.DAGScheduler: Final stage: ResultStage 62 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,467 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 61)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,467 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,467 INFO scheduler.DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[247] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,469 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 67.7 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,471 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,471 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.2.106.114:37399 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,472 INFO spark.SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,472 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[247] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,473 INFO cluster.YarnScheduler: Adding task set 62.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,474 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 62.0 (TID 48) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,481 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on algo-1:40443 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,486 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,496 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 62.0 (TID 48) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,496 INFO cluster.YarnScheduler: Removed TaskSet 62.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,498 INFO scheduler.DAGScheduler: ResultStage 62 (collect at AnalysisRunner.scala:326) finished in 0.029 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,498 INFO scheduler.DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,498 INFO cluster.YarnScheduler: Killing all running tasks in stage 62: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,499 INFO scheduler.DAGScheduler: Job 42 finished: collect at AnalysisRunner.scala:326, took 0.032853 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,552 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,553 INFO scheduler.DAGScheduler: Registering RDD 255 (countByKey at ColumnProfiler.scala:592) as input to shuffle 20\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,554 INFO scheduler.DAGScheduler: Got job 43 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,554 INFO scheduler.DAGScheduler: Final stage: ResultStage 64 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,554 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 63)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,554 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 63)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,554 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 63 (MapPartitionsRDD[255] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,559 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 41.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,560 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,561 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,561 INFO spark.SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,561 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 63 (MapPartitionsRDD[255] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,561 INFO cluster.YarnScheduler: Adding task set 63.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,562 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 63.0 (TID 49) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,571 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,603 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 63.0 (TID 49) in 41 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,603 INFO cluster.YarnScheduler: Removed TaskSet 63.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,604 INFO scheduler.DAGScheduler: ShuffleMapStage 63 (countByKey at ColumnProfiler.scala:592) finished in 0.049 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,604 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,604 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,604 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 64)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,605 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,605 INFO scheduler.DAGScheduler: Submitting ResultStage 64 (ShuffledRDD[256] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,607 INFO memory.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 5.1 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,610 INFO memory.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,610 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,610 INFO spark.SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,611 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (ShuffledRDD[256] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,611 INFO cluster.YarnScheduler: Adding task set 64.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,612 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 64.0 (TID 50) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,622 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,625 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,638 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 64.0 (TID 50) in 26 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,638 INFO cluster.YarnScheduler: Removed TaskSet 64.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,638 INFO scheduler.DAGScheduler: ResultStage 64 (countByKey at ColumnProfiler.scala:592) finished in 0.032 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,638 INFO scheduler.DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,638 INFO cluster.YarnScheduler: Killing all running tasks in stage 64: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,644 INFO scheduler.DAGScheduler: Job 43 finished: countByKey at ColumnProfiler.scala:592, took 0.091558 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,787 INFO scheduler.DAGScheduler: Registering RDD 261 (collect at AnalysisRunner.scala:326) as input to shuffle 21\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,787 INFO scheduler.DAGScheduler: Got map stage job 44 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,787 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 65 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,787 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,787 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,787 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 65 (MapPartitionsRDD[261] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,792 INFO memory.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 94.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,794 INFO memory.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,794 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.2.106.114:37399 (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,795 INFO spark.SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,795 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 65 (MapPartitionsRDD[261] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,795 INFO cluster.YarnScheduler: Adding task set 65.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,796 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 65.0 (TID 51) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,804 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on algo-1:40443 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,946 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 65.0 (TID 51) in 150 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,946 INFO cluster.YarnScheduler: Removed TaskSet 65.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,947 INFO scheduler.DAGScheduler: ShuffleMapStage 65 (collect at AnalysisRunner.scala:326) finished in 0.159 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,947 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,947 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,947 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,947 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,978 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,979 INFO scheduler.DAGScheduler: Got job 45 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,979 INFO scheduler.DAGScheduler: Final stage: ResultStage 67 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,980 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 66)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,980 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,980 INFO scheduler.DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[264] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,989 INFO memory.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 179.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,991 INFO memory.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,991 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.2.106.114:37399 (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,992 INFO spark.SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,993 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[264] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,993 INFO cluster.YarnScheduler: Adding task set 67.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:26,994 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 67.0 (TID 52) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,001 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on algo-1:40443 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,009 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,091 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 67.0 (TID 52) in 97 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,091 INFO cluster.YarnScheduler: Removed TaskSet 67.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,092 INFO scheduler.DAGScheduler: ResultStage 67 (collect at AnalysisRunner.scala:326) finished in 0.111 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,093 INFO scheduler.DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,093 INFO cluster.YarnScheduler: Killing all running tasks in stage 67: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,094 INFO scheduler.DAGScheduler: Job 45 finished: collect at AnalysisRunner.scala:326, took 0.114906 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,231 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,232 INFO scheduler.DAGScheduler: Got job 46 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,232 INFO scheduler.DAGScheduler: Final stage: ResultStage 68 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,232 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,232 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,233 INFO scheduler.DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[274] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,243 INFO memory.MemoryStore: Block broadcast_55 stored as values in memory (estimated size 48.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,245 INFO memory.MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,245 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.2.106.114:37399 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,246 INFO spark.SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,246 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[274] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,246 INFO cluster.YarnScheduler: Adding task set 68.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,247 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 68.0 (TID 53) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,256 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on algo-1:40443 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,301 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 68.0 (TID 53) in 54 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,301 INFO cluster.YarnScheduler: Removed TaskSet 68.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,302 INFO scheduler.DAGScheduler: ResultStage 68 (treeReduce at KLLRunner.scala:107) finished in 0.068 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,302 INFO scheduler.DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,303 INFO cluster.YarnScheduler: Killing all running tasks in stage 68: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,303 INFO scheduler.DAGScheduler: Job 46 finished: treeReduce at KLLRunner.scala:107, took 0.072131 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,432 INFO scheduler.DAGScheduler: Registering RDD 279 (collect at AnalysisRunner.scala:326) as input to shuffle 22\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,432 INFO scheduler.DAGScheduler: Got map stage job 47 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,432 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 69 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,432 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,432 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,433 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 69 (MapPartitionsRDD[279] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,437 INFO memory.MemoryStore: Block broadcast_56 stored as values in memory (estimated size 87.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,439 INFO memory.MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,440 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.2.106.114:37399 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,440 INFO spark.SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,441 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 69 (MapPartitionsRDD[279] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,441 INFO cluster.YarnScheduler: Adding task set 69.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,442 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 69.0 (TID 54) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,455 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on algo-1:40443 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,492 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 69.0 (TID 54) in 50 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,492 INFO cluster.YarnScheduler: Removed TaskSet 69.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,495 INFO scheduler.DAGScheduler: ShuffleMapStage 69 (collect at AnalysisRunner.scala:326) finished in 0.061 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,495 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,495 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,496 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,496 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,585 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,588 INFO scheduler.DAGScheduler: Got job 48 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,588 INFO scheduler.DAGScheduler: Final stage: ResultStage 71 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,588 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 70)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,589 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,589 INFO scheduler.DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[282] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,591 INFO memory.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 67.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,593 INFO memory.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,594 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.2.106.114:37399 (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,595 INFO spark.SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,596 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[282] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,596 INFO cluster.YarnScheduler: Adding task set 71.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,598 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 71.0 (TID 55) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,607 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on algo-1:40443 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,610 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,617 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 71.0 (TID 55) in 19 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,617 INFO cluster.YarnScheduler: Removed TaskSet 71.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,618 INFO scheduler.DAGScheduler: ResultStage 71 (collect at AnalysisRunner.scala:326) finished in 0.028 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,619 INFO scheduler.DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,619 INFO cluster.YarnScheduler: Killing all running tasks in stage 71: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,619 INFO scheduler.DAGScheduler: Job 48 finished: collect at AnalysisRunner.scala:326, took 0.031699 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,703 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,704 INFO scheduler.DAGScheduler: Registering RDD 290 (countByKey at ColumnProfiler.scala:592) as input to shuffle 23\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,704 INFO scheduler.DAGScheduler: Got job 49 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,704 INFO scheduler.DAGScheduler: Final stage: ResultStage 73 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,705 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 72)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,705 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 72)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,709 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 72 (MapPartitionsRDD[290] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,717 INFO memory.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 41.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,719 INFO memory.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,720 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,723 INFO spark.SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,723 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 72 (MapPartitionsRDD[290] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,724 INFO cluster.YarnScheduler: Adding task set 72.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,725 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 72.0 (TID 56) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,734 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,781 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 72.0 (TID 56) in 56 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,781 INFO cluster.YarnScheduler: Removed TaskSet 72.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,782 INFO scheduler.DAGScheduler: ShuffleMapStage 72 (countByKey at ColumnProfiler.scala:592) finished in 0.073 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,782 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,782 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,783 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 73)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,783 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,783 INFO scheduler.DAGScheduler: Submitting ResultStage 73 (ShuffledRDD[291] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,784 INFO memory.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,786 INFO memory.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,786 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,787 INFO spark.SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,787 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (ShuffledRDD[291] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,787 INFO cluster.YarnScheduler: Adding task set 73.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,788 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 73.0 (TID 57) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,800 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,803 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,811 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 73.0 (TID 57) in 23 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,811 INFO cluster.YarnScheduler: Removed TaskSet 73.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,811 INFO scheduler.DAGScheduler: ResultStage 73 (countByKey at ColumnProfiler.scala:592) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,811 INFO scheduler.DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,812 INFO cluster.YarnScheduler: Killing all running tasks in stage 73: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,812 INFO scheduler.DAGScheduler: Job 49 finished: countByKey at ColumnProfiler.scala:592, took 0.108608 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,885 INFO scheduler.DAGScheduler: Registering RDD 296 (collect at AnalysisRunner.scala:326) as input to shuffle 24\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,885 INFO scheduler.DAGScheduler: Got map stage job 50 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,885 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 74 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,885 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,886 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,886 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 74 (MapPartitionsRDD[296] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,889 INFO memory.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 94.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,891 INFO memory.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,891 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.2.106.114:37399 (size: 30.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,891 INFO spark.SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,892 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 74 (MapPartitionsRDD[296] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,892 INFO cluster.YarnScheduler: Adding task set 74.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,893 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 74.0 (TID 58) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,901 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on algo-1:40443 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,990 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 74.0 (TID 58) in 98 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,990 INFO cluster.YarnScheduler: Removed TaskSet 74.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,991 INFO scheduler.DAGScheduler: ShuffleMapStage 74 (collect at AnalysisRunner.scala:326) finished in 0.103 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,991 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,991 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,991 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:27,991 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,031 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,032 INFO scheduler.DAGScheduler: Got job 51 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,032 INFO scheduler.DAGScheduler: Final stage: ResultStage 76 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,032 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 75)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,032 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,032 INFO scheduler.DAGScheduler: Submitting ResultStage 76 (MapPartitionsRDD[299] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,039 INFO memory.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 179.7 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,041 INFO memory.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 49.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,042 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 10.2.106.114:37399 (size: 49.1 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,042 INFO spark.SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,042 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 76 (MapPartitionsRDD[299] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,042 INFO cluster.YarnScheduler: Adding task set 76.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,043 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 76.0 (TID 59) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,050 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on algo-1:40443 (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,059 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,129 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 76.0 (TID 59) in 86 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,129 INFO cluster.YarnScheduler: Removed TaskSet 76.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,129 INFO scheduler.DAGScheduler: ResultStage 76 (collect at AnalysisRunner.scala:326) finished in 0.096 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,130 INFO scheduler.DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,131 INFO cluster.YarnScheduler: Killing all running tasks in stage 76: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,132 INFO scheduler.DAGScheduler: Job 51 finished: collect at AnalysisRunner.scala:326, took 0.100348 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,233 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,233 INFO scheduler.DAGScheduler: Got job 52 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,233 INFO scheduler.DAGScheduler: Final stage: ResultStage 77 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,233 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,234 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,234 INFO scheduler.DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[309] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,238 INFO memory.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 48.9 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,240 INFO memory.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,240 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 10.2.106.114:37399 (size: 19.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,240 INFO spark.SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,241 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[309] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,241 INFO cluster.YarnScheduler: Adding task set 77.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,242 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 77.0 (TID 60) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,248 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on algo-1:40443 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,295 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 77.0 (TID 60) in 53 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,296 INFO scheduler.DAGScheduler: ResultStage 77 (treeReduce at KLLRunner.scala:107) finished in 0.062 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,296 INFO scheduler.DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,296 INFO cluster.YarnScheduler: Removed TaskSet 77.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,296 INFO cluster.YarnScheduler: Killing all running tasks in stage 77: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,296 INFO scheduler.DAGScheduler: Job 52 finished: treeReduce at KLLRunner.scala:107, took 0.063424 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,349 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on 10.2.106.114:37399 in memory (size: 19.9 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,352 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on algo-1:40443 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,355 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on 10.2.106.114:37399 in memory (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,362 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on algo-1:40443 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,368 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on 10.2.106.114:37399 in memory (size: 19.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,369 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on algo-1:40443 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,371 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 10.2.106.114:37399 in memory (size: 27.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,373 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on algo-1:40443 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,377 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on 10.2.106.114:37399 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,377 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on algo-1:40443 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,382 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 10.2.106.114:37399 in memory (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,383 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on algo-1:40443 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,385 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on 10.2.106.114:37399 in memory (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,386 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on algo-1:40443 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,390 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on 10.2.106.114:37399 in memory (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,391 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on algo-1:40443 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,395 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on 10.2.106.114:37399 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,396 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on algo-1:40443 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,400 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 10.2.106.114:37399 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,400 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on algo-1:40443 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,402 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 10.2.106.114:37399 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,403 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on algo-1:40443 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,405 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on 10.2.106.114:37399 in memory (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,406 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on algo-1:40443 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,408 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on 10.2.106.114:37399 in memory (size: 49.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,409 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on algo-1:40443 in memory (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,411 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 10.2.106.114:37399 in memory (size: 17.4 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,415 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on algo-1:40443 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,419 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on 10.2.106.114:37399 in memory (size: 3.0 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,420 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on algo-1:40443 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,463 INFO scheduler.DAGScheduler: Registering RDD 314 (collect at AnalysisRunner.scala:326) as input to shuffle 25\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,463 INFO scheduler.DAGScheduler: Got map stage job 53 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,463 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 78 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,463 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,464 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,465 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 78 (MapPartitionsRDD[314] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,467 INFO memory.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 87.4 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,468 INFO memory.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,468 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 10.2.106.114:37399 (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,469 INFO spark.SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,469 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 78 (MapPartitionsRDD[314] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,469 INFO cluster.YarnScheduler: Adding task set 78.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,470 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 78.0 (TID 61) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,476 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on algo-1:40443 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,486 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 78.0 (TID 61) in 16 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,486 INFO cluster.YarnScheduler: Removed TaskSet 78.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,487 INFO scheduler.DAGScheduler: ShuffleMapStage 78 (collect at AnalysisRunner.scala:326) finished in 0.022 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,487 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,487 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,487 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,487 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,522 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,522 INFO scheduler.DAGScheduler: Got job 54 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,522 INFO scheduler.DAGScheduler: Final stage: ResultStage 80 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,522 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 79)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,522 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,522 INFO scheduler.DAGScheduler: Submitting ResultStage 80 (MapPartitionsRDD[317] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,524 INFO memory.MemoryStore: Block broadcast_64 stored as values in memory (estimated size 67.7 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,525 INFO memory.MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,525 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 10.2.106.114:37399 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,526 INFO spark.SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,526 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 80 (MapPartitionsRDD[317] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,526 INFO cluster.YarnScheduler: Adding task set 80.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,527 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 80.0 (TID 62) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,533 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on algo-1:40443 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,537 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 25 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,541 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 80.0 (TID 62) in 14 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,541 INFO cluster.YarnScheduler: Removed TaskSet 80.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,541 INFO scheduler.DAGScheduler: ResultStage 80 (collect at AnalysisRunner.scala:326) finished in 0.018 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,542 INFO scheduler.DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,542 INFO cluster.YarnScheduler: Killing all running tasks in stage 80: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,542 INFO scheduler.DAGScheduler: Job 54 finished: collect at AnalysisRunner.scala:326, took 0.020743 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,585 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,586 INFO scheduler.DAGScheduler: Registering RDD 325 (countByKey at ColumnProfiler.scala:592) as input to shuffle 26\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,586 INFO scheduler.DAGScheduler: Got job 55 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,586 INFO scheduler.DAGScheduler: Final stage: ResultStage 82 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,586 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 81)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,586 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 81)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,586 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 81 (MapPartitionsRDD[325] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,590 INFO memory.MemoryStore: Block broadcast_65 stored as values in memory (estimated size 41.9 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,591 INFO memory.MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,591 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,592 INFO spark.SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,592 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 81 (MapPartitionsRDD[325] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,592 INFO cluster.YarnScheduler: Adding task set 81.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,593 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 81.0 (TID 63) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,600 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,621 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 81.0 (TID 63) in 28 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,621 INFO cluster.YarnScheduler: Removed TaskSet 81.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,621 INFO scheduler.DAGScheduler: ShuffleMapStage 81 (countByKey at ColumnProfiler.scala:592) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,621 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,622 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,622 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 82)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,622 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,622 INFO scheduler.DAGScheduler: Submitting ResultStage 82 (ShuffledRDD[326] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,623 INFO memory.MemoryStore: Block broadcast_66 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,624 INFO memory.MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,624 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,625 INFO spark.SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,625 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 82 (ShuffledRDD[326] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,625 INFO cluster.YarnScheduler: Adding task set 82.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,626 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 82.0 (TID 64) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,632 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,635 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,644 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 82.0 (TID 64) in 18 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,645 INFO cluster.YarnScheduler: Removed TaskSet 82.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,645 INFO scheduler.DAGScheduler: ResultStage 82 (countByKey at ColumnProfiler.scala:592) finished in 0.023 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,646 INFO scheduler.DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,646 INFO cluster.YarnScheduler: Killing all running tasks in stage 82: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,646 INFO scheduler.DAGScheduler: Job 55 finished: countByKey at ColumnProfiler.scala:592, took 0.061187 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,778 INFO scheduler.DAGScheduler: Registering RDD 331 (collect at AnalysisRunner.scala:326) as input to shuffle 27\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,779 INFO scheduler.DAGScheduler: Got map stage job 56 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,779 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 83 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,779 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,780 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,780 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 83 (MapPartitionsRDD[331] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,784 INFO memory.MemoryStore: Block broadcast_67 stored as values in memory (estimated size 94.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,785 INFO memory.MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,786 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on 10.2.106.114:37399 (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,786 INFO spark.SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,787 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 83 (MapPartitionsRDD[331] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,787 INFO cluster.YarnScheduler: Adding task set 83.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,788 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 83.0 (TID 65) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:28,798 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on algo-1:40443 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,014 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 83.0 (TID 65) in 226 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,014 INFO cluster.YarnScheduler: Removed TaskSet 83.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,014 INFO scheduler.DAGScheduler: ShuffleMapStage 83 (collect at AnalysisRunner.scala:326) finished in 0.233 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,015 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,015 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,015 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,015 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,048 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,049 INFO scheduler.DAGScheduler: Got job 57 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,049 INFO scheduler.DAGScheduler: Final stage: ResultStage 85 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,049 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 84)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,050 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,050 INFO scheduler.DAGScheduler: Submitting ResultStage 85 (MapPartitionsRDD[334] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,060 INFO memory.MemoryStore: Block broadcast_68 stored as values in memory (estimated size 179.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,062 INFO memory.MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,062 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on 10.2.106.114:37399 (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,063 INFO spark.SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,063 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 85 (MapPartitionsRDD[334] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,064 INFO cluster.YarnScheduler: Adding task set 85.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,065 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 85.0 (TID 66) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,072 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on algo-1:40443 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,094 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 27 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,208 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 85.0 (TID 66) in 143 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,208 INFO cluster.YarnScheduler: Removed TaskSet 85.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,209 INFO scheduler.DAGScheduler: ResultStage 85 (collect at AnalysisRunner.scala:326) finished in 0.157 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,209 INFO scheduler.DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,210 INFO cluster.YarnScheduler: Killing all running tasks in stage 85: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,210 INFO scheduler.DAGScheduler: Job 57 finished: collect at AnalysisRunner.scala:326, took 0.161754 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,310 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,311 INFO scheduler.DAGScheduler: Got job 58 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,311 INFO scheduler.DAGScheduler: Final stage: ResultStage 86 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,311 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,312 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,312 INFO scheduler.DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[344] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,317 INFO memory.MemoryStore: Block broadcast_69 stored as values in memory (estimated size 48.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,318 INFO memory.MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,319 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on 10.2.106.114:37399 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,319 INFO spark.SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,319 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[344] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,319 INFO cluster.YarnScheduler: Adding task set 86.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,320 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 86.0 (TID 67) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,334 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on algo-1:40443 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,365 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 86.0 (TID 67) in 45 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,365 INFO cluster.YarnScheduler: Removed TaskSet 86.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,366 INFO scheduler.DAGScheduler: ResultStage 86 (treeReduce at KLLRunner.scala:107) finished in 0.053 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,367 INFO scheduler.DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,367 INFO cluster.YarnScheduler: Killing all running tasks in stage 86: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,368 INFO scheduler.DAGScheduler: Job 58 finished: treeReduce at KLLRunner.scala:107, took 0.057009 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,492 INFO scheduler.DAGScheduler: Registering RDD 349 (collect at AnalysisRunner.scala:326) as input to shuffle 28\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,492 INFO scheduler.DAGScheduler: Got map stage job 59 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,492 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 87 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,492 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,492 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,492 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 87 (MapPartitionsRDD[349] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,494 INFO memory.MemoryStore: Block broadcast_70 stored as values in memory (estimated size 87.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,496 INFO memory.MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,496 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on 10.2.106.114:37399 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,497 INFO spark.SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,497 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 87 (MapPartitionsRDD[349] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,497 INFO cluster.YarnScheduler: Adding task set 87.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,498 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 87.0 (TID 68) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,505 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on algo-1:40443 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,516 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 87.0 (TID 68) in 18 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,517 INFO cluster.YarnScheduler: Removed TaskSet 87.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,517 INFO scheduler.DAGScheduler: ShuffleMapStage 87 (collect at AnalysisRunner.scala:326) finished in 0.024 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,517 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,518 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,518 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,518 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,554 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,554 INFO scheduler.DAGScheduler: Got job 60 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,554 INFO scheduler.DAGScheduler: Final stage: ResultStage 89 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,554 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 88)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,555 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,555 INFO scheduler.DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[352] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,556 INFO memory.MemoryStore: Block broadcast_71 stored as values in memory (estimated size 67.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,557 INFO memory.MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,558 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on 10.2.106.114:37399 (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,558 INFO spark.SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,558 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[352] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,558 INFO cluster.YarnScheduler: Adding task set 89.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,559 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 89.0 (TID 69) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,566 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on algo-1:40443 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,570 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,574 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 89.0 (TID 69) in 15 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,574 INFO cluster.YarnScheduler: Removed TaskSet 89.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,575 INFO scheduler.DAGScheduler: ResultStage 89 (collect at AnalysisRunner.scala:326) finished in 0.020 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,575 INFO scheduler.DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,575 INFO cluster.YarnScheduler: Killing all running tasks in stage 89: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,575 INFO scheduler.DAGScheduler: Job 60 finished: collect at AnalysisRunner.scala:326, took 0.021525 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,609 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,609 INFO scheduler.DAGScheduler: Registering RDD 360 (countByKey at ColumnProfiler.scala:592) as input to shuffle 29\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,609 INFO scheduler.DAGScheduler: Got job 61 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,609 INFO scheduler.DAGScheduler: Final stage: ResultStage 91 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,610 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 90)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,610 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 90)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,610 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 90 (MapPartitionsRDD[360] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,615 INFO memory.MemoryStore: Block broadcast_72 stored as values in memory (estimated size 41.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,616 INFO memory.MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,616 INFO storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,616 INFO spark.SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,617 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 90 (MapPartitionsRDD[360] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,617 INFO cluster.YarnScheduler: Adding task set 90.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,617 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 90.0 (TID 70) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,624 INFO storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,638 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 90.0 (TID 70) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,639 INFO cluster.YarnScheduler: Removed TaskSet 90.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,639 INFO scheduler.DAGScheduler: ShuffleMapStage 90 (countByKey at ColumnProfiler.scala:592) finished in 0.029 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,639 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,639 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,640 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 91)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,640 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,640 INFO scheduler.DAGScheduler: Submitting ResultStage 91 (ShuffledRDD[361] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,641 INFO memory.MemoryStore: Block broadcast_73 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,642 INFO memory.MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,642 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,642 INFO spark.SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,642 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (ShuffledRDD[361] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,642 INFO cluster.YarnScheduler: Adding task set 91.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,643 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 91.0 (TID 71) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,648 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,650 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 29 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,661 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 91.0 (TID 71) in 18 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,661 INFO cluster.YarnScheduler: Removed TaskSet 91.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,661 INFO scheduler.DAGScheduler: ResultStage 91 (countByKey at ColumnProfiler.scala:592) finished in 0.021 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,662 INFO scheduler.DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,662 INFO cluster.YarnScheduler: Killing all running tasks in stage 91: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,662 INFO scheduler.DAGScheduler: Job 61 finished: countByKey at ColumnProfiler.scala:592, took 0.053175 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,717 INFO scheduler.DAGScheduler: Registering RDD 366 (collect at AnalysisRunner.scala:326) as input to shuffle 30\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,717 INFO scheduler.DAGScheduler: Got map stage job 62 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,717 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 92 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,717 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,717 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,717 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 92 (MapPartitionsRDD[366] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,721 INFO memory.MemoryStore: Block broadcast_74 stored as values in memory (estimated size 94.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,722 INFO memory.MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,722 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on 10.2.106.114:37399 (size: 30.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,723 INFO spark.SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,723 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 92 (MapPartitionsRDD[366] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,723 INFO cluster.YarnScheduler: Adding task set 92.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,724 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 92.0 (TID 72) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,730 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on algo-1:40443 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,840 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 92.0 (TID 72) in 116 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,841 INFO cluster.YarnScheduler: Removed TaskSet 92.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,841 INFO scheduler.DAGScheduler: ShuffleMapStage 92 (collect at AnalysisRunner.scala:326) finished in 0.123 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,843 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,843 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,843 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,844 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,873 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,874 INFO scheduler.DAGScheduler: Got job 63 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,874 INFO scheduler.DAGScheduler: Final stage: ResultStage 94 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,874 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 93)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,874 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,874 INFO scheduler.DAGScheduler: Submitting ResultStage 94 (MapPartitionsRDD[369] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,881 INFO memory.MemoryStore: Block broadcast_75 stored as values in memory (estimated size 179.7 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,882 INFO memory.MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,883 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on 10.2.106.114:37399 (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,883 INFO spark.SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,883 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 94 (MapPartitionsRDD[369] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,883 INFO cluster.YarnScheduler: Adding task set 94.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,884 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 94.0 (TID 73) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,919 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on algo-1:40443 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:29,928 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,035 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 94.0 (TID 73) in 151 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,035 INFO cluster.YarnScheduler: Removed TaskSet 94.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,036 INFO scheduler.DAGScheduler: ResultStage 94 (collect at AnalysisRunner.scala:326) finished in 0.161 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,037 INFO scheduler.DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,038 INFO cluster.YarnScheduler: Killing all running tasks in stage 94: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,038 INFO scheduler.DAGScheduler: Job 63 finished: collect at AnalysisRunner.scala:326, took 0.164666 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,149 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,150 INFO scheduler.DAGScheduler: Got job 64 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,150 INFO scheduler.DAGScheduler: Final stage: ResultStage 95 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,150 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,151 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,151 INFO scheduler.DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[379] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,157 INFO memory.MemoryStore: Block broadcast_76 stored as values in memory (estimated size 48.9 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,158 INFO memory.MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,159 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on 10.2.106.114:37399 (size: 19.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,159 INFO spark.SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,160 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[379] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,160 INFO cluster.YarnScheduler: Adding task set 95.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,161 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 95.0 (TID 74) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,169 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on algo-1:40443 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,210 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 95.0 (TID 74) in 49 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,211 INFO cluster.YarnScheduler: Removed TaskSet 95.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,211 INFO scheduler.DAGScheduler: ResultStage 95 (treeReduce at KLLRunner.scala:107) finished in 0.059 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,212 INFO scheduler.DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,213 INFO cluster.YarnScheduler: Killing all running tasks in stage 95: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,213 INFO scheduler.DAGScheduler: Job 64 finished: treeReduce at KLLRunner.scala:107, took 0.063873 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,357 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on 10.2.106.114:37399 in memory (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,358 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on algo-1:40443 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,364 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on 10.2.106.114:37399 in memory (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,364 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on algo-1:40443 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,368 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on 10.2.106.114:37399 in memory (size: 27.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,368 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on algo-1:40443 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,374 INFO storage.BlockManagerInfo: Removed broadcast_73_piece0 on 10.2.106.114:37399 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,375 INFO storage.BlockManagerInfo: Removed broadcast_73_piece0 on algo-1:40443 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,377 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on 10.2.106.114:37399 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,377 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on algo-1:40443 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,379 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on 10.2.106.114:37399 in memory (size: 27.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,380 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on algo-1:40443 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,384 INFO storage.BlockManagerInfo: Removed broadcast_75_piece0 on 10.2.106.114:37399 in memory (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,385 INFO storage.BlockManagerInfo: Removed broadcast_75_piece0 on algo-1:40443 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,389 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on 10.2.106.114:37399 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,390 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on algo-1:40443 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,392 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on 10.2.106.114:37399 in memory (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,393 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on algo-1:40443 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,395 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on 10.2.106.114:37399 in memory (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,397 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on algo-1:40443 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,398 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on 10.2.106.114:37399 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,399 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on algo-1:40443 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,401 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on 10.2.106.114:37399 in memory (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,402 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on algo-1:40443 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,404 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on 10.2.106.114:37399 in memory (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,405 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on algo-1:40443 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,408 INFO storage.BlockManagerInfo: Removed broadcast_72_piece0 on 10.2.106.114:37399 in memory (size: 17.4 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,409 INFO storage.BlockManagerInfo: Removed broadcast_72_piece0 on algo-1:40443 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,418 INFO scheduler.DAGScheduler: Registering RDD 384 (collect at AnalysisRunner.scala:326) as input to shuffle 31\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,418 INFO scheduler.DAGScheduler: Got map stage job 65 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,419 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 96 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,419 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,419 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,419 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 96 (MapPartitionsRDD[384] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,422 INFO memory.MemoryStore: Block broadcast_77 stored as values in memory (estimated size 87.4 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,424 INFO memory.MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,424 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on 10.2.106.114:37399 (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,425 INFO spark.SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,425 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 96 (MapPartitionsRDD[384] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,425 INFO cluster.YarnScheduler: Adding task set 96.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,426 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 96.0 (TID 75) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,432 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on algo-1:40443 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,445 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 96.0 (TID 75) in 19 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,445 INFO cluster.YarnScheduler: Removed TaskSet 96.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,446 INFO scheduler.DAGScheduler: ShuffleMapStage 96 (collect at AnalysisRunner.scala:326) finished in 0.026 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,446 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,447 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,447 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,447 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,517 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,518 INFO scheduler.DAGScheduler: Got job 66 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,518 INFO scheduler.DAGScheduler: Final stage: ResultStage 98 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,518 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 97)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,518 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,519 INFO scheduler.DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[387] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,521 INFO memory.MemoryStore: Block broadcast_78 stored as values in memory (estimated size 67.7 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,523 INFO memory.MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,523 INFO storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on 10.2.106.114:37399 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,523 INFO spark.SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,524 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 98 (MapPartitionsRDD[387] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,524 INFO cluster.YarnScheduler: Adding task set 98.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,525 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 98.0 (TID 76) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,532 INFO storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on algo-1:40443 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,536 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,540 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 98.0 (TID 76) in 15 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,540 INFO cluster.YarnScheduler: Removed TaskSet 98.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,541 INFO scheduler.DAGScheduler: ResultStage 98 (collect at AnalysisRunner.scala:326) finished in 0.021 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,542 INFO scheduler.DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,542 INFO cluster.YarnScheduler: Killing all running tasks in stage 98: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,542 INFO scheduler.DAGScheduler: Job 66 finished: collect at AnalysisRunner.scala:326, took 0.025184 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,592 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,593 INFO scheduler.DAGScheduler: Registering RDD 395 (countByKey at ColumnProfiler.scala:592) as input to shuffle 32\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,593 INFO scheduler.DAGScheduler: Got job 67 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,594 INFO scheduler.DAGScheduler: Final stage: ResultStage 100 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,594 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 99)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,594 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 99)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,595 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 99 (MapPartitionsRDD[395] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,598 INFO memory.MemoryStore: Block broadcast_79 stored as values in memory (estimated size 41.9 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,600 INFO memory.MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,600 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,600 INFO spark.SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,600 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 99 (MapPartitionsRDD[395] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,600 INFO cluster.YarnScheduler: Adding task set 99.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,601 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 99.0 (TID 77) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,608 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,646 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 99.0 (TID 77) in 45 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,646 INFO cluster.YarnScheduler: Removed TaskSet 99.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,647 INFO scheduler.DAGScheduler: ShuffleMapStage 99 (countByKey at ColumnProfiler.scala:592) finished in 0.051 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,647 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,647 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,647 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 100)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,647 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,648 INFO scheduler.DAGScheduler: Submitting ResultStage 100 (ShuffledRDD[396] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,649 INFO memory.MemoryStore: Block broadcast_80 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,650 INFO memory.MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,651 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,651 INFO spark.SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,652 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 100 (ShuffledRDD[396] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,652 INFO cluster.YarnScheduler: Adding task set 100.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,653 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 100.0 (TID 78) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,664 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,667 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 32 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,674 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 100.0 (TID 78) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,674 INFO cluster.YarnScheduler: Removed TaskSet 100.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,674 INFO scheduler.DAGScheduler: ResultStage 100 (countByKey at ColumnProfiler.scala:592) finished in 0.026 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,675 INFO scheduler.DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,675 INFO cluster.YarnScheduler: Killing all running tasks in stage 100: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,675 INFO scheduler.DAGScheduler: Job 67 finished: countByKey at ColumnProfiler.scala:592, took 0.082711 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,834 INFO scheduler.DAGScheduler: Registering RDD 401 (collect at AnalysisRunner.scala:326) as input to shuffle 33\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,834 INFO scheduler.DAGScheduler: Got map stage job 68 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,834 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 101 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,834 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,834 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,834 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 101 (MapPartitionsRDD[401] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,838 INFO memory.MemoryStore: Block broadcast_81 stored as values in memory (estimated size 94.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,840 INFO memory.MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,841 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on 10.2.106.114:37399 (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,841 INFO spark.SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,841 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 101 (MapPartitionsRDD[401] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,841 INFO cluster.YarnScheduler: Adding task set 101.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,842 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 101.0 (TID 79) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,848 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on algo-1:40443 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,920 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 101.0 (TID 79) in 77 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,920 INFO cluster.YarnScheduler: Removed TaskSet 101.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,920 INFO scheduler.DAGScheduler: ShuffleMapStage 101 (collect at AnalysisRunner.scala:326) finished in 0.085 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,920 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,920 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,920 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,920 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,946 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,947 INFO scheduler.DAGScheduler: Got job 69 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,947 INFO scheduler.DAGScheduler: Final stage: ResultStage 103 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,947 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 102)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,947 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,947 INFO scheduler.DAGScheduler: Submitting ResultStage 103 (MapPartitionsRDD[404] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,952 INFO memory.MemoryStore: Block broadcast_82 stored as values in memory (estimated size 179.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,954 INFO memory.MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 49.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,954 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on 10.2.106.114:37399 (size: 49.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,955 INFO spark.SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,955 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 103 (MapPartitionsRDD[404] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,955 INFO cluster.YarnScheduler: Adding task set 103.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,956 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 103.0 (TID 80) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,963 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on algo-1:40443 (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:30,971 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 33 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,044 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 103.0 (TID 80) in 88 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,044 INFO cluster.YarnScheduler: Removed TaskSet 103.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,045 INFO scheduler.DAGScheduler: ResultStage 103 (collect at AnalysisRunner.scala:326) finished in 0.097 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,045 INFO scheduler.DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,045 INFO cluster.YarnScheduler: Killing all running tasks in stage 103: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,046 INFO scheduler.DAGScheduler: Job 69 finished: collect at AnalysisRunner.scala:326, took 0.099385 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,131 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,132 INFO scheduler.DAGScheduler: Got job 70 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,132 INFO scheduler.DAGScheduler: Final stage: ResultStage 104 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,132 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,132 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,132 INFO scheduler.DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[414] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,137 INFO memory.MemoryStore: Block broadcast_83 stored as values in memory (estimated size 48.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,138 INFO memory.MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,139 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on 10.2.106.114:37399 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,139 INFO spark.SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,139 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 104 (MapPartitionsRDD[414] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,140 INFO cluster.YarnScheduler: Adding task set 104.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,141 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 104.0 (TID 81) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,151 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on algo-1:40443 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,199 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 104.0 (TID 81) in 58 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,199 INFO cluster.YarnScheduler: Removed TaskSet 104.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,200 INFO scheduler.DAGScheduler: ResultStage 104 (treeReduce at KLLRunner.scala:107) finished in 0.067 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,200 INFO scheduler.DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,201 INFO cluster.YarnScheduler: Killing all running tasks in stage 104: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,201 INFO scheduler.DAGScheduler: Job 70 finished: treeReduce at KLLRunner.scala:107, took 0.069853 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,328 INFO scheduler.DAGScheduler: Registering RDD 419 (collect at AnalysisRunner.scala:326) as input to shuffle 34\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,328 INFO scheduler.DAGScheduler: Got map stage job 71 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,328 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 105 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,328 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,328 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,328 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 105 (MapPartitionsRDD[419] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,331 INFO memory.MemoryStore: Block broadcast_84 stored as values in memory (estimated size 87.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,332 INFO memory.MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,333 INFO storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on 10.2.106.114:37399 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,333 INFO spark.SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,333 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 105 (MapPartitionsRDD[419] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,334 INFO cluster.YarnScheduler: Adding task set 105.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,335 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 105.0 (TID 82) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,341 INFO storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on algo-1:40443 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,352 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 105.0 (TID 82) in 18 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,352 INFO cluster.YarnScheduler: Removed TaskSet 105.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,353 INFO scheduler.DAGScheduler: ShuffleMapStage 105 (collect at AnalysisRunner.scala:326) finished in 0.024 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,353 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,353 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,353 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,353 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,395 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,395 INFO scheduler.DAGScheduler: Got job 72 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,395 INFO scheduler.DAGScheduler: Final stage: ResultStage 107 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,395 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 106)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,395 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,396 INFO scheduler.DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[422] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,397 INFO memory.MemoryStore: Block broadcast_85 stored as values in memory (estimated size 67.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,399 INFO memory.MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,399 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on 10.2.106.114:37399 (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,399 INFO spark.SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,400 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 107 (MapPartitionsRDD[422] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,400 INFO cluster.YarnScheduler: Adding task set 107.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,401 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 107.0 (TID 83) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,410 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on algo-1:40443 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,414 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,419 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 107.0 (TID 83) in 18 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,419 INFO cluster.YarnScheduler: Removed TaskSet 107.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,420 INFO scheduler.DAGScheduler: ResultStage 107 (collect at AnalysisRunner.scala:326) finished in 0.024 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,420 INFO scheduler.DAGScheduler: Job 72 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,420 INFO cluster.YarnScheduler: Killing all running tasks in stage 107: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,420 INFO scheduler.DAGScheduler: Job 72 finished: collect at AnalysisRunner.scala:326, took 0.025123 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,456 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,457 INFO scheduler.DAGScheduler: Registering RDD 430 (countByKey at ColumnProfiler.scala:592) as input to shuffle 35\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,457 INFO scheduler.DAGScheduler: Got job 73 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,457 INFO scheduler.DAGScheduler: Final stage: ResultStage 109 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,457 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 108)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,457 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 108)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,458 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 108 (MapPartitionsRDD[430] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,461 INFO memory.MemoryStore: Block broadcast_86 stored as values in memory (estimated size 41.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,462 INFO memory.MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,462 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,463 INFO spark.SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,463 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 108 (MapPartitionsRDD[430] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,463 INFO cluster.YarnScheduler: Adding task set 108.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,464 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 108.0 (TID 84) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,471 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,504 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 108.0 (TID 84) in 40 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,505 INFO scheduler.DAGScheduler: ShuffleMapStage 108 (countByKey at ColumnProfiler.scala:592) finished in 0.047 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,505 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,505 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,505 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 109)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,506 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,509 INFO cluster.YarnScheduler: Removed TaskSet 108.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,509 INFO scheduler.DAGScheduler: Submitting ResultStage 109 (ShuffledRDD[431] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,510 INFO memory.MemoryStore: Block broadcast_87 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,511 INFO memory.MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,512 INFO storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,512 INFO spark.SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,512 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 109 (ShuffledRDD[431] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,512 INFO cluster.YarnScheduler: Adding task set 109.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,513 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 109.0 (TID 85) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,518 INFO storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,520 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 35 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,526 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 109.0 (TID 85) in 13 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,526 INFO cluster.YarnScheduler: Removed TaskSet 109.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,527 INFO scheduler.DAGScheduler: ResultStage 109 (countByKey at ColumnProfiler.scala:592) finished in 0.018 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,527 INFO scheduler.DAGScheduler: Job 73 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,527 INFO cluster.YarnScheduler: Killing all running tasks in stage 109: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,527 INFO scheduler.DAGScheduler: Job 73 finished: countByKey at ColumnProfiler.scala:592, took 0.070700 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,597 INFO scheduler.DAGScheduler: Registering RDD 436 (collect at AnalysisRunner.scala:326) as input to shuffle 36\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,598 INFO scheduler.DAGScheduler: Got map stage job 74 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,598 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 110 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,598 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,598 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,599 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 110 (MapPartitionsRDD[436] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,602 INFO memory.MemoryStore: Block broadcast_88 stored as values in memory (estimated size 94.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,605 INFO memory.MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,605 INFO storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on 10.2.106.114:37399 (size: 30.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,606 INFO spark.SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,606 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 110 (MapPartitionsRDD[436] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,606 INFO cluster.YarnScheduler: Adding task set 110.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,608 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 110.0 (TID 86) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,614 INFO storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on algo-1:40443 (size: 30.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,728 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 110.0 (TID 86) in 121 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,728 INFO cluster.YarnScheduler: Removed TaskSet 110.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,729 INFO scheduler.DAGScheduler: ShuffleMapStage 110 (collect at AnalysisRunner.scala:326) finished in 0.129 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,729 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,729 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,729 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,729 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,763 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,764 INFO scheduler.DAGScheduler: Got job 75 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,764 INFO scheduler.DAGScheduler: Final stage: ResultStage 112 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,764 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 111)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,764 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,764 INFO scheduler.DAGScheduler: Submitting ResultStage 112 (MapPartitionsRDD[439] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,770 INFO memory.MemoryStore: Block broadcast_89 stored as values in memory (estimated size 179.8 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,772 INFO memory.MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,773 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on 10.2.106.114:37399 (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,773 INFO spark.SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,773 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 112 (MapPartitionsRDD[439] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,773 INFO cluster.YarnScheduler: Adding task set 112.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,777 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 112.0 (TID 87) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,790 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on algo-1:40443 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,797 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,849 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 112.0 (TID 87) in 72 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,849 INFO cluster.YarnScheduler: Removed TaskSet 112.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,850 INFO scheduler.DAGScheduler: ResultStage 112 (collect at AnalysisRunner.scala:326) finished in 0.086 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,850 INFO scheduler.DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,850 INFO cluster.YarnScheduler: Killing all running tasks in stage 112: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,850 INFO scheduler.DAGScheduler: Job 75 finished: collect at AnalysisRunner.scala:326, took 0.087188 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,920 INFO codegen.CodeGenerator: Code generated in 12.570471 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,947 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,947 INFO scheduler.DAGScheduler: Got job 76 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,948 INFO scheduler.DAGScheduler: Final stage: ResultStage 113 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,948 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,948 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,948 INFO scheduler.DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[449] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,956 INFO memory.MemoryStore: Block broadcast_90 stored as values in memory (estimated size 48.9 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,957 INFO memory.MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,958 INFO storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on 10.2.106.114:37399 (size: 19.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,958 INFO spark.SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,959 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 113 (MapPartitionsRDD[449] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,959 INFO cluster.YarnScheduler: Adding task set 113.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,960 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 113.0 (TID 88) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:31,967 INFO storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on algo-1:40443 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,013 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 113.0 (TID 88) in 53 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,013 INFO cluster.YarnScheduler: Removed TaskSet 113.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,014 INFO scheduler.DAGScheduler: ResultStage 113 (treeReduce at KLLRunner.scala:107) finished in 0.065 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,015 INFO scheduler.DAGScheduler: Job 76 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,015 INFO cluster.YarnScheduler: Killing all running tasks in stage 113: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,016 INFO scheduler.DAGScheduler: Job 76 finished: treeReduce at KLLRunner.scala:107, took 0.068893 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,143 INFO codegen.CodeGenerator: Code generated in 30.183808 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,149 INFO scheduler.DAGScheduler: Registering RDD 454 (collect at AnalysisRunner.scala:326) as input to shuffle 37\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,149 INFO scheduler.DAGScheduler: Got map stage job 77 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,149 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 114 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,149 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,150 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,150 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 114 (MapPartitionsRDD[454] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,154 INFO memory.MemoryStore: Block broadcast_91 stored as values in memory (estimated size 87.4 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,157 INFO memory.MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,157 INFO storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on 10.2.106.114:37399 (size: 27.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,158 INFO spark.SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,158 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 114 (MapPartitionsRDD[454] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,158 INFO cluster.YarnScheduler: Adding task set 114.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,160 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 114.0 (TID 89) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,167 INFO storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on algo-1:40443 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,269 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 114.0 (TID 89) in 110 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,270 INFO cluster.YarnScheduler: Removed TaskSet 114.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,270 INFO scheduler.DAGScheduler: ShuffleMapStage 114 (collect at AnalysisRunner.scala:326) finished in 0.118 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,270 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,270 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,271 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,271 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,322 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,323 INFO scheduler.DAGScheduler: Got job 78 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,323 INFO scheduler.DAGScheduler: Final stage: ResultStage 116 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,323 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 115)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,323 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,323 INFO scheduler.DAGScheduler: Submitting ResultStage 116 (MapPartitionsRDD[457] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,325 INFO memory.MemoryStore: Block broadcast_92 stored as values in memory (estimated size 67.7 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,329 INFO memory.MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,329 INFO storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on 10.2.106.114:37399 (size: 19.9 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,330 INFO spark.SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,330 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 116 (MapPartitionsRDD[457] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,331 INFO cluster.YarnScheduler: Adding task set 116.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,332 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 116.0 (TID 90) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,342 INFO storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on algo-1:40443 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,344 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,349 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 116.0 (TID 90) in 17 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,349 INFO cluster.YarnScheduler: Removed TaskSet 116.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,349 INFO scheduler.DAGScheduler: ResultStage 116 (collect at AnalysisRunner.scala:326) finished in 0.025 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,350 INFO scheduler.DAGScheduler: Job 78 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,350 INFO cluster.YarnScheduler: Killing all running tasks in stage 116: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,350 INFO scheduler.DAGScheduler: Job 78 finished: collect at AnalysisRunner.scala:326, took 0.027799 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,418 INFO storage.BlockManagerInfo: Removed broadcast_91_piece0 on 10.2.106.114:37399 in memory (size: 27.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,419 INFO storage.BlockManagerInfo: Removed broadcast_91_piece0 on algo-1:40443 in memory (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,422 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,423 INFO scheduler.DAGScheduler: Registering RDD 465 (countByKey at ColumnProfiler.scala:592) as input to shuffle 38\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,423 INFO scheduler.DAGScheduler: Got job 79 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,423 INFO scheduler.DAGScheduler: Final stage: ResultStage 118 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,423 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 117)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,424 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 117)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,424 INFO storage.BlockManagerInfo: Removed broadcast_87_piece0 on 10.2.106.114:37399 in memory (size: 3.0 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,425 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 117 (MapPartitionsRDD[465] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,426 INFO storage.BlockManagerInfo: Removed broadcast_87_piece0 on algo-1:40443 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,428 INFO storage.BlockManagerInfo: Removed broadcast_84_piece0 on 10.2.106.114:37399 in memory (size: 27.3 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,429 INFO memory.MemoryStore: Block broadcast_93 stored as values in memory (estimated size 41.9 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,430 INFO storage.BlockManagerInfo: Removed broadcast_84_piece0 on algo-1:40443 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,430 INFO memory.MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,431 INFO storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,431 INFO storage.BlockManagerInfo: Removed broadcast_89_piece0 on 10.2.106.114:37399 in memory (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,431 INFO spark.SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,432 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 117 (MapPartitionsRDD[465] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,432 INFO cluster.YarnScheduler: Adding task set 117.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,432 INFO storage.BlockManagerInfo: Removed broadcast_89_piece0 on algo-1:40443 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,433 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 117.0 (TID 91) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,437 INFO storage.BlockManagerInfo: Removed broadcast_81_piece0 on 10.2.106.114:37399 in memory (size: 30.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,438 INFO storage.BlockManagerInfo: Removed broadcast_81_piece0 on algo-1:40443 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,441 INFO storage.BlockManagerInfo: Removed broadcast_79_piece0 on 10.2.106.114:37399 in memory (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,442 INFO storage.BlockManagerInfo: Removed broadcast_79_piece0 on algo-1:40443 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,444 INFO storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,447 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on 10.2.106.114:37399 in memory (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,458 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on algo-1:40443 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,459 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on 10.2.106.114:37399 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,460 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on algo-1:40443 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,466 INFO storage.BlockManagerInfo: Removed broadcast_78_piece0 on 10.2.106.114:37399 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,467 INFO storage.BlockManagerInfo: Removed broadcast_78_piece0 on algo-1:40443 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,469 INFO storage.BlockManagerInfo: Removed broadcast_85_piece0 on 10.2.106.114:37399 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,470 INFO storage.BlockManagerInfo: Removed broadcast_85_piece0 on algo-1:40443 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,473 INFO storage.BlockManagerInfo: Removed broadcast_88_piece0 on algo-1:40443 in memory (size: 30.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,475 INFO storage.BlockManagerInfo: Removed broadcast_88_piece0 on 10.2.106.114:37399 in memory (size: 30.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,476 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on 10.2.106.114:37399 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,477 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on algo-1:40443 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,479 INFO storage.BlockManagerInfo: Removed broadcast_86_piece0 on 10.2.106.114:37399 in memory (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,479 INFO storage.BlockManagerInfo: Removed broadcast_86_piece0 on algo-1:40443 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,481 INFO storage.BlockManagerInfo: Removed broadcast_90_piece0 on 10.2.106.114:37399 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,481 INFO storage.BlockManagerInfo: Removed broadcast_90_piece0 on algo-1:40443 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,483 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on 10.2.106.114:37399 in memory (size: 49.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,484 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on algo-1:40443 in memory (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,488 INFO storage.BlockManagerInfo: Removed broadcast_92_piece0 on 10.2.106.114:37399 in memory (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,490 INFO storage.BlockManagerInfo: Removed broadcast_92_piece0 on algo-1:40443 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,493 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 117.0 (TID 91) in 60 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,493 INFO cluster.YarnScheduler: Removed TaskSet 117.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,494 INFO scheduler.DAGScheduler: ShuffleMapStage 117 (countByKey at ColumnProfiler.scala:592) finished in 0.069 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,494 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,494 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,494 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 118)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,494 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,494 INFO scheduler.DAGScheduler: Submitting ResultStage 118 (ShuffledRDD[466] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,495 INFO memory.MemoryStore: Block broadcast_94 stored as values in memory (estimated size 5.1 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,496 INFO memory.MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,496 INFO storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,497 INFO spark.SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,497 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 118 (ShuffledRDD[466] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,498 INFO cluster.YarnScheduler: Adding task set 118.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,499 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 118.0 (TID 92) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,504 INFO storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,507 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 38 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,515 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 118.0 (TID 92) in 17 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,515 INFO cluster.YarnScheduler: Removed TaskSet 118.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,516 INFO scheduler.DAGScheduler: ResultStage 118 (countByKey at ColumnProfiler.scala:592) finished in 0.021 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,516 INFO scheduler.DAGScheduler: Job 79 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,516 INFO cluster.YarnScheduler: Killing all running tasks in stage 118: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,516 INFO scheduler.DAGScheduler: Job 79 finished: countByKey at ColumnProfiler.scala:592, took 0.094027 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,629 INFO scheduler.DAGScheduler: Registering RDD 471 (collect at AnalysisRunner.scala:326) as input to shuffle 39\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,629 INFO scheduler.DAGScheduler: Got map stage job 80 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,630 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 119 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,630 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,630 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,630 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 119 (MapPartitionsRDD[471] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,634 INFO memory.MemoryStore: Block broadcast_95 stored as values in memory (estimated size 94.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,635 INFO memory.MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,636 INFO storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on 10.2.106.114:37399 (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,636 INFO spark.SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,637 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 119 (MapPartitionsRDD[471] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,637 INFO cluster.YarnScheduler: Adding task set 119.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,638 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 119.0 (TID 93) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,645 INFO storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on algo-1:40443 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,730 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 119.0 (TID 93) in 92 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,731 INFO cluster.YarnScheduler: Removed TaskSet 119.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,731 INFO scheduler.DAGScheduler: ShuffleMapStage 119 (collect at AnalysisRunner.scala:326) finished in 0.100 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,731 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,731 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,731 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,731 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,757 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,758 INFO scheduler.DAGScheduler: Got job 81 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,758 INFO scheduler.DAGScheduler: Final stage: ResultStage 121 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,758 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 120)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,758 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,758 INFO scheduler.DAGScheduler: Submitting ResultStage 121 (MapPartitionsRDD[474] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,762 INFO memory.MemoryStore: Block broadcast_96 stored as values in memory (estimated size 179.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,764 INFO memory.MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,764 INFO storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on 10.2.106.114:37399 (size: 49.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,764 INFO spark.SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,764 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 121 (MapPartitionsRDD[474] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,764 INFO cluster.YarnScheduler: Adding task set 121.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,765 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 121.0 (TID 94) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,771 INFO storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on algo-1:40443 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,780 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 39 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,826 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 121.0 (TID 94) in 61 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,826 INFO cluster.YarnScheduler: Removed TaskSet 121.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,826 INFO scheduler.DAGScheduler: ResultStage 121 (collect at AnalysisRunner.scala:326) finished in 0.067 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,827 INFO scheduler.DAGScheduler: Job 81 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,827 INFO cluster.YarnScheduler: Killing all running tasks in stage 121: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,827 INFO scheduler.DAGScheduler: Job 81 finished: collect at AnalysisRunner.scala:326, took 0.069590 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,894 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,895 INFO scheduler.DAGScheduler: Got job 82 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,895 INFO scheduler.DAGScheduler: Final stage: ResultStage 122 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,895 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,895 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,895 INFO scheduler.DAGScheduler: Submitting ResultStage 122 (MapPartitionsRDD[484] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,899 INFO memory.MemoryStore: Block broadcast_97 stored as values in memory (estimated size 48.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,900 INFO memory.MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,900 INFO storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on 10.2.106.114:37399 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,900 INFO spark.SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,901 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 122 (MapPartitionsRDD[484] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,901 INFO cluster.YarnScheduler: Adding task set 122.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,902 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 122.0 (TID 95) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,908 INFO storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on algo-1:40443 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,978 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 122.0 (TID 95) in 76 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,978 INFO cluster.YarnScheduler: Removed TaskSet 122.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,979 INFO scheduler.DAGScheduler: ResultStage 122 (treeReduce at KLLRunner.scala:107) finished in 0.083 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,979 INFO scheduler.DAGScheduler: Job 82 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,979 INFO cluster.YarnScheduler: Killing all running tasks in stage 122: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:32,979 INFO scheduler.DAGScheduler: Job 82 finished: treeReduce at KLLRunner.scala:107, took 0.084749 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,055 INFO scheduler.DAGScheduler: Registering RDD 489 (collect at AnalysisRunner.scala:326) as input to shuffle 40\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,056 INFO scheduler.DAGScheduler: Got map stage job 83 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,056 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 123 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,056 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,057 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,057 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 123 (MapPartitionsRDD[489] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,059 INFO memory.MemoryStore: Block broadcast_98 stored as values in memory (estimated size 87.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,060 INFO memory.MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,060 INFO storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on 10.2.106.114:37399 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,061 INFO spark.SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,061 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 123 (MapPartitionsRDD[489] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,061 INFO cluster.YarnScheduler: Adding task set 123.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,062 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 123.0 (TID 96) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,067 INFO storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on algo-1:40443 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,076 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 123.0 (TID 96) in 15 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,076 INFO cluster.YarnScheduler: Removed TaskSet 123.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,077 INFO scheduler.DAGScheduler: ShuffleMapStage 123 (collect at AnalysisRunner.scala:326) finished in 0.020 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,077 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,077 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,077 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,077 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,102 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,103 INFO scheduler.DAGScheduler: Got job 84 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,103 INFO scheduler.DAGScheduler: Final stage: ResultStage 125 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,103 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 124)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,103 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,103 INFO scheduler.DAGScheduler: Submitting ResultStage 125 (MapPartitionsRDD[492] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,104 INFO memory.MemoryStore: Block broadcast_99 stored as values in memory (estimated size 67.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,106 INFO memory.MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,106 INFO storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on 10.2.106.114:37399 (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,106 INFO spark.SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,106 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 125 (MapPartitionsRDD[492] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,106 INFO cluster.YarnScheduler: Adding task set 125.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,107 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 125.0 (TID 97) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,114 INFO storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on algo-1:40443 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,116 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 40 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,121 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 125.0 (TID 97) in 14 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,121 INFO cluster.YarnScheduler: Removed TaskSet 125.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,122 INFO scheduler.DAGScheduler: ResultStage 125 (collect at AnalysisRunner.scala:326) finished in 0.019 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,122 INFO scheduler.DAGScheduler: Job 84 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,122 INFO cluster.YarnScheduler: Killing all running tasks in stage 125: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,122 INFO scheduler.DAGScheduler: Job 84 finished: collect at AnalysisRunner.scala:326, took 0.019735 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,159 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,159 INFO scheduler.DAGScheduler: Registering RDD 500 (countByKey at ColumnProfiler.scala:592) as input to shuffle 41\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,159 INFO scheduler.DAGScheduler: Got job 85 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,160 INFO scheduler.DAGScheduler: Final stage: ResultStage 127 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,160 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 126)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,160 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 126)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,160 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 126 (MapPartitionsRDD[500] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,163 INFO memory.MemoryStore: Block broadcast_100 stored as values in memory (estimated size 41.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,164 INFO memory.MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,164 INFO storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on 10.2.106.114:37399 (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,165 INFO spark.SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,165 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 126 (MapPartitionsRDD[500] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,165 INFO cluster.YarnScheduler: Adding task set 126.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,166 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 126.0 (TID 98) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,170 INFO storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on algo-1:40443 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,187 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 126.0 (TID 98) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,187 INFO cluster.YarnScheduler: Removed TaskSet 126.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,188 INFO scheduler.DAGScheduler: ShuffleMapStage 126 (countByKey at ColumnProfiler.scala:592) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,188 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,188 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,188 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 127)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,188 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,188 INFO scheduler.DAGScheduler: Submitting ResultStage 127 (ShuffledRDD[501] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,189 INFO memory.MemoryStore: Block broadcast_101 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,190 INFO memory.MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,190 INFO storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on 10.2.106.114:37399 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,191 INFO spark.SparkContext: Created broadcast 101 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,191 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 127 (ShuffledRDD[501] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,191 INFO cluster.YarnScheduler: Adding task set 127.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,192 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 127.0 (TID 99) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,196 INFO storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on algo-1:40443 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,198 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 41 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,203 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 127.0 (TID 99) in 12 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,203 INFO cluster.YarnScheduler: Removed TaskSet 127.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,203 INFO scheduler.DAGScheduler: ResultStage 127 (countByKey at ColumnProfiler.scala:592) finished in 0.014 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,204 INFO scheduler.DAGScheduler: Job 85 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,204 INFO cluster.YarnScheduler: Killing all running tasks in stage 127: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,204 INFO scheduler.DAGScheduler: Job 85 finished: countByKey at ColumnProfiler.scala:592, took 0.044875 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,410 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,436 INFO codegen.CodeGenerator: Code generated in 6.61213 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,440 INFO scheduler.DAGScheduler: Registering RDD 506 (count at StatsGenerator.scala:66) as input to shuffle 42\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,440 INFO scheduler.DAGScheduler: Got map stage job 86 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,440 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 128 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,440 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,441 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,441 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 128 (MapPartitionsRDD[506] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,443 INFO memory.MemoryStore: Block broadcast_102 stored as values in memory (estimated size 34.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,444 INFO memory.MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 13.7 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,445 INFO storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on 10.2.106.114:37399 (size: 13.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,445 INFO spark.SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,446 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 128 (MapPartitionsRDD[506] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,446 INFO cluster.YarnScheduler: Adding task set 128.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,447 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 128.0 (TID 100) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,453 INFO storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on algo-1:40443 (size: 13.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,477 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 128.0 (TID 100) in 30 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,477 INFO cluster.YarnScheduler: Removed TaskSet 128.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,478 INFO scheduler.DAGScheduler: ShuffleMapStage 128 (count at StatsGenerator.scala:66) finished in 0.036 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,478 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,479 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,479 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,479 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,502 INFO codegen.CodeGenerator: Code generated in 18.659373 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,517 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,518 INFO scheduler.DAGScheduler: Got job 87 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,518 INFO scheduler.DAGScheduler: Final stage: ResultStage 130 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,518 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 129)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,519 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,519 INFO scheduler.DAGScheduler: Submitting ResultStage 130 (MapPartitionsRDD[509] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,520 INFO memory.MemoryStore: Block broadcast_103 stored as values in memory (estimated size 11.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,521 INFO memory.MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,522 INFO storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on 10.2.106.114:37399 (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,522 INFO spark.SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,523 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 130 (MapPartitionsRDD[509] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,523 INFO cluster.YarnScheduler: Adding task set 130.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,524 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 130.0 (TID 101) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,533 INFO storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on algo-1:40443 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,536 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 42 to 10.2.106.114:48140\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,546 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 130.0 (TID 101) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,546 INFO cluster.YarnScheduler: Removed TaskSet 130.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,547 INFO scheduler.DAGScheduler: ResultStage 130 (count at StatsGenerator.scala:66) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,547 INFO scheduler.DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,548 INFO cluster.YarnScheduler: Killing all running tasks in stage 130: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:33,548 INFO scheduler.DAGScheduler: Job 87 finished: count at StatsGenerator.scala:66, took 0.030174 s\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,006 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,020 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,098 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,099 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,109 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,125 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,236 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,236 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,245 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,249 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,333 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,334 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,334 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,390 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,391 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ece4696b-ab96-408a-9fa4-3c1d1cb9098c\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,407 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-66f4821c-8cba-4e74-b5a4-a8728e27dec0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,499 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2023-05-14 04:43:34,500 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cell 12\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor_baseline = my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri+'/training-dataset-with-header.csv',\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Files:\n",
      "sagemaker/DEMO-ModelMonitor/baselining/results/constraints.json\n",
      " sagemaker/DEMO-ModelMonitor/baselining/results/statistics.json\n"
     ]
    }
   ],
   "source": [
    "# cell 13\n",
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>numerical_statistics.common.num_present</th>\n",
       "      <th>numerical_statistics.common.num_missing</th>\n",
       "      <th>numerical_statistics.mean</th>\n",
       "      <th>numerical_statistics.sum</th>\n",
       "      <th>numerical_statistics.std_dev</th>\n",
       "      <th>numerical_statistics.min</th>\n",
       "      <th>numerical_statistics.max</th>\n",
       "      <th>numerical_statistics.distribution.kll.buckets</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.c</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.k</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Churn</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.139306</td>\n",
       "      <td>325.0</td>\n",
       "      <td>0.346265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Account Length</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>101.276897</td>\n",
       "      <td>236279.0</td>\n",
       "      <td>39.552442</td>\n",
       "      <td>1.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>[{'lower_bound': 1.0, 'upper_bound': 25.2, 'co...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[119.0, 100.0, 111.0, 181.0, 95.0, 104.0, 70....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VMail Message</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>8.214316</td>\n",
       "      <td>19164.0</td>\n",
       "      <td>13.776908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 5.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[19.0, 0.0, 0.0, 40.0, 36.0, 0.0, 0.0, 24.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Day Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>180.226489</td>\n",
       "      <td>420468.4</td>\n",
       "      <td>53.987179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>350.8</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 35.08, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[178.1, 160.3, 197.1, 105.2, 283.1, 113.6, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Day Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>100.259323</td>\n",
       "      <td>233905.0</td>\n",
       "      <td>20.165008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 16.5, 'co...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[110.0, 138.0, 117.0, 61.0, 112.0, 87.0, 122....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Eve Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>200.050107</td>\n",
       "      <td>466716.9</td>\n",
       "      <td>50.015928</td>\n",
       "      <td>31.2</td>\n",
       "      <td>361.8</td>\n",
       "      <td>[{'lower_bound': 31.2, 'upper_bound': 64.26, '...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[212.8, 221.3, 227.8, 341.3, 286.2, 158.6, 29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eve Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>99.573939</td>\n",
       "      <td>232306.0</td>\n",
       "      <td>19.675578</td>\n",
       "      <td>12.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>[{'lower_bound': 12.0, 'upper_bound': 27.8, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[100.0, 92.0, 128.0, 79.0, 86.0, 98.0, 112.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Night Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>201.388598</td>\n",
       "      <td>469839.6</td>\n",
       "      <td>50.627961</td>\n",
       "      <td>23.2</td>\n",
       "      <td>395.0</td>\n",
       "      <td>[{'lower_bound': 23.2, 'upper_bound': 60.37999...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[226.3, 150.4, 214.0, 165.7, 261.7, 187.7, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Night Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>100.227175</td>\n",
       "      <td>233830.0</td>\n",
       "      <td>19.282029</td>\n",
       "      <td>42.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>[{'lower_bound': 42.0, 'upper_bound': 55.3, 'c...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[123.0, 120.0, 101.0, 97.0, 129.0, 87.0, 112....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Intl Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>10.253065</td>\n",
       "      <td>23920.4</td>\n",
       "      <td>2.778766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 1.8399999...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[10.0, 11.2, 9.3, 6.3, 11.3, 10.5, 0.0, 9.7, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name inferred_type  numerical_statistics.common.num_present  \\\n",
       "0           Churn      Integral                                     2333   \n",
       "1  Account Length      Integral                                     2333   \n",
       "2   VMail Message      Integral                                     2333   \n",
       "3        Day Mins    Fractional                                     2333   \n",
       "4       Day Calls      Integral                                     2333   \n",
       "5        Eve Mins    Fractional                                     2333   \n",
       "6       Eve Calls      Integral                                     2333   \n",
       "7      Night Mins    Fractional                                     2333   \n",
       "8     Night Calls      Integral                                     2333   \n",
       "9       Intl Mins    Fractional                                     2333   \n",
       "\n",
       "   numerical_statistics.common.num_missing  numerical_statistics.mean  \\\n",
       "0                                        0                   0.139306   \n",
       "1                                        0                 101.276897   \n",
       "2                                        0                   8.214316   \n",
       "3                                        0                 180.226489   \n",
       "4                                        0                 100.259323   \n",
       "5                                        0                 200.050107   \n",
       "6                                        0                  99.573939   \n",
       "7                                        0                 201.388598   \n",
       "8                                        0                 100.227175   \n",
       "9                                        0                  10.253065   \n",
       "\n",
       "   numerical_statistics.sum  numerical_statistics.std_dev  \\\n",
       "0                     325.0                      0.346265   \n",
       "1                  236279.0                     39.552442   \n",
       "2                   19164.0                     13.776908   \n",
       "3                  420468.4                     53.987179   \n",
       "4                  233905.0                     20.165008   \n",
       "5                  466716.9                     50.015928   \n",
       "6                  232306.0                     19.675578   \n",
       "7                  469839.6                     50.627961   \n",
       "8                  233830.0                     19.282029   \n",
       "9                   23920.4                      2.778766   \n",
       "\n",
       "   numerical_statistics.min  numerical_statistics.max  \\\n",
       "0                       0.0                       1.0   \n",
       "1                       1.0                     243.0   \n",
       "2                       0.0                      51.0   \n",
       "3                       0.0                     350.8   \n",
       "4                       0.0                     165.0   \n",
       "5                      31.2                     361.8   \n",
       "6                      12.0                     170.0   \n",
       "7                      23.2                     395.0   \n",
       "8                      42.0                     175.0   \n",
       "9                       0.0                      18.4   \n",
       "\n",
       "       numerical_statistics.distribution.kll.buckets  \\\n",
       "0  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "1  [{'lower_bound': 1.0, 'upper_bound': 25.2, 'co...   \n",
       "2  [{'lower_bound': 0.0, 'upper_bound': 5.1, 'cou...   \n",
       "3  [{'lower_bound': 0.0, 'upper_bound': 35.08, 'c...   \n",
       "4  [{'lower_bound': 0.0, 'upper_bound': 16.5, 'co...   \n",
       "5  [{'lower_bound': 31.2, 'upper_bound': 64.26, '...   \n",
       "6  [{'lower_bound': 12.0, 'upper_bound': 27.8, 'c...   \n",
       "7  [{'lower_bound': 23.2, 'upper_bound': 60.37999...   \n",
       "8  [{'lower_bound': 42.0, 'upper_bound': 55.3, 'c...   \n",
       "9  [{'lower_bound': 0.0, 'upper_bound': 1.8399999...   \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.c  \\\n",
       "0                                               0.64           \n",
       "1                                               0.64           \n",
       "2                                               0.64           \n",
       "3                                               0.64           \n",
       "4                                               0.64           \n",
       "5                                               0.64           \n",
       "6                                               0.64           \n",
       "7                                               0.64           \n",
       "8                                               0.64           \n",
       "9                                               0.64           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.k  \\\n",
       "0                                             2048.0           \n",
       "1                                             2048.0           \n",
       "2                                             2048.0           \n",
       "3                                             2048.0           \n",
       "4                                             2048.0           \n",
       "5                                             2048.0           \n",
       "6                                             2048.0           \n",
       "7                                             2048.0           \n",
       "8                                             2048.0           \n",
       "9                                             2048.0           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.data  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...  \n",
       "1  [[119.0, 100.0, 111.0, 181.0, 95.0, 104.0, 70....  \n",
       "2  [[19.0, 0.0, 0.0, 40.0, 36.0, 0.0, 0.0, 24.0, ...  \n",
       "3  [[178.1, 160.3, 197.1, 105.2, 283.1, 113.6, 23...  \n",
       "4  [[110.0, 138.0, 117.0, 61.0, 112.0, 87.0, 122....  \n",
       "5  [[212.8, 221.3, 227.8, 341.3, 286.2, 158.6, 29...  \n",
       "6  [[100.0, 92.0, 128.0, 79.0, 86.0, 98.0, 112.0,...  \n",
       "7  [[226.3, 150.4, 214.0, 165.7, 261.7, 187.7, 20...  \n",
       "8  [[123.0, 120.0, 101.0, 97.0, 129.0, 87.0, 112....  \n",
       "9  [[10.0, 11.2, 9.3, 6.3, 11.3, 10.5, 0.0, 9.7, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 14\n",
    "import pandas as pd\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>completeness</th>\n",
       "      <th>num_constraints.is_non_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Churn</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Account Length</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VMail Message</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Day Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Day Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Eve Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eve Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Night Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Night Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Intl Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name inferred_type  completeness  num_constraints.is_non_negative\n",
       "0           Churn      Integral           1.0                             True\n",
       "1  Account Length      Integral           1.0                             True\n",
       "2   VMail Message      Integral           1.0                             True\n",
       "3        Day Mins    Fractional           1.0                             True\n",
       "4       Day Calls      Integral           1.0                             True\n",
       "5        Eve Mins    Fractional           1.0                             True\n",
       "6       Eve Calls      Integral           1.0                             True\n",
       "7      Night Mins    Fractional           1.0                             True\n",
       "8     Night Calls      Integral           1.0                             True\n",
       "9       Intl Mins    Fractional           1.0                             True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 15\n",
    "constraints_df = pd.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Analyzing collected data for data quality issues\n",
    "\n",
    "When you have collected the data above, analyze and monitor the data with Monitoring Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 16\n",
    "# First, copy over some test scripts to the S3 bucket so that they can be used for pre and post processing\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(code_prefix+\"/preprocessor.py\").upload_file('preprocessor.py')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(code_prefix+\"/postprocessor.py\").upload_file('postprocessor.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a model monitoring schedule for the endpoint created earlier. Use the baseline resources (constraints and statistics) to compare against the realtime traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n"
     ]
    }
   ],
   "source": [
    "# cell 17\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "mon_schedule_name = 'DEMO-xgb-churn-pred-model-monitor-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    #record_preprocessor_script=pre_processor_script,\n",
    "    post_analytics_processor_script=s3_code_postprocessor_uri,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start generating some artificial traffic\n",
    "The cell below starts a thread to send some traffic to the endpoint. Note that you need to stop the kernel to terminate this thread. If there is no traffic, the monitoring jobs are marked as `Failed` since there is no data to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 18\n",
    "from threading import Thread\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "endpoint_name=predictor.endpoint_name\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# (just repeating code from above for convenience/ able to run this section independently)\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    with open(file_name, 'r') as f:\n",
    "        for row in f:\n",
    "            payload = row.rstrip('\\n')\n",
    "            response = runtime_client.invoke_endpoint(EndpointName=ep_name,\n",
    "                                          ContentType='text/csv', \n",
    "                                          Body=payload)\n",
    "            response['Body'].read()\n",
    "            time.sleep(1)\n",
    "            \n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        invoke_endpoint(endpoint_name, 'test_data/test-dataset-input-cols.csv', runtime_client)\n",
    "        \n",
    "thread = Thread(target = invoke_endpoint_forever)\n",
    "thread.start()\n",
    "\n",
    "# Note that you need to stop the kernel to stop the invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe and inspect the schedule\n",
    "Once you describe, observe that the MonitoringScheduleStatus changes to Scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule status: Pending\n"
     ]
    }
   ],
   "source": [
    "# cell 19\n",
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List executions\n",
    "The schedule starts jobs at the previously specified intervals. Here, you list the latest five executions. Note that if you are kicking this off after creating the hourly schedule, you might find the executions empty. You might have to wait until you cross the hour boundary (in UTC) to see executions kick off. The code below has the logic for waiting.\n",
    "\n",
    "Note: Even for an hourly schedule, Amazon SageMaker has a buffer period of 20 minutes to schedule your execution. You might see your execution start in anywhere from zero to ~20 minutes from the hour boundary. This is expected and done for load balancing in the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "We created a hourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer.\n",
      "We will have to wait till we hit the hour...\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n"
     ]
    }
   ],
   "source": [
    "# cell 20\n",
    "mon_executions = my_default_monitor.list_executions()\n",
    "print(\"We created a hourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer.\\nWe will have to wait till we hit the hour...\")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = my_default_monitor.list_executions()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect a specific execution (latest execution)\n",
    "In the previous cell, you picked up the latest completed or failed scheduled execution. Here are the possible terminal states and what each of them mean: \n",
    "* Completed - This means the monitoring execution completed and no issues were found in the violations report.\n",
    "* CompletedWithViolations - This means the execution completed, but constraint violations were detected.\n",
    "* Failed - The monitoring execution failed, maybe due to client error (perhaps incorrect role premissions) or infrastructure issues. Further examination of FailureReason and ExitMessage is necessary to identify what exactly happened.\n",
    "* Stopped - job exceeded max runtime or was manually stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................................................!Latest execution status: Completed\n",
      "Latest execution result: CompletedWithViolations: Job completed successfully with 60 violations.\n"
     ]
    }
   ],
   "source": [
    "# cell 21\n",
    "latest_execution = mon_executions[-1] # latest execution's index is -1, second to last is -2 and so on..\n",
    "time.sleep(60)\n",
    "latest_execution.wait(logs=False)\n",
    "\n",
    "print(\"Latest execution status: {}\".format(latest_execution.describe()['ProcessingJobStatus']))\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()['ExitMessage']))\n",
    "\n",
    "latest_job = latest_execution.describe()\n",
    "if (latest_job['ProcessingJobStatus'] != 'Completed'):\n",
    "        print(\"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Uri: s3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/reports/DEMO-xgb-churn-pred-model-monitor-2023-05-14-04-01-38/DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15/2023/05/14/05\n"
     ]
    }
   ],
   "source": [
    "# cell 22\n",
    "report_uri=latest_execution.output.destination\n",
    "print('Report Uri: {}'.format(report_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List the generated reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report bucket: sagemaker-us-east-1-369074678854\n",
      "Report key: sagemaker/DEMO-ModelMonitor/reports/DEMO-xgb-churn-pred-model-monitor-2023-05-14-04-01-38/DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15/2023/05/14/05\n",
      "Found Report Files:\n",
      "sagemaker/DEMO-ModelMonitor/reports/DEMO-xgb-churn-pred-model-monitor-2023-05-14-04-01-38/DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15/2023/05/14/05/constraint_violations.json\n",
      " sagemaker/DEMO-ModelMonitor/reports/DEMO-xgb-churn-pred-model-monitor-2023-05-14-04-01-38/DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15/2023/05/14/05/constraints.json\n",
      " sagemaker/DEMO-ModelMonitor/reports/DEMO-xgb-churn-pred-model-monitor-2023-05-14-04-01-38/DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15/2023/05/14/05/statistics.json\n"
     ]
    }
   ],
   "source": [
    "# cell 23\n",
    "from urllib.parse import urlparse\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip('/')\n",
    "print('Report bucket: {}'.format(report_bucket))\n",
    "print('Report key: {}'.format(report_key))\n",
    "\n",
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violations report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any violations compared to the baseline, they will be listed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>constraint_check_type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VMail Plan_yes</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>State_ME</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>State_FL</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>State_VA</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>State_NE</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>State_WA</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>State_MT</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>State_RI</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>State_HI</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Churn</td>\n",
       "      <td>data_type_check</td>\n",
       "      <td>Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 0.0% of data is Integral.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_name constraint_check_type  \\\n",
       "0  VMail Plan_yes       data_type_check   \n",
       "1        State_ME       data_type_check   \n",
       "2        State_FL       data_type_check   \n",
       "3        State_VA       data_type_check   \n",
       "4        State_NE       data_type_check   \n",
       "5        State_WA       data_type_check   \n",
       "6        State_MT       data_type_check   \n",
       "7        State_RI       data_type_check   \n",
       "8        State_HI       data_type_check   \n",
       "9           Churn       data_type_check   \n",
       "\n",
       "                                                                                                                                            description  \n",
       "0  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.  \n",
       "1  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.  \n",
       "2  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.  \n",
       "3  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.  \n",
       "4  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.  \n",
       "5  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.  \n",
       "6  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.  \n",
       "7  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.  \n",
       "8  Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 99.62962962962963% of data is Integral.  \n",
       "9                Data type match requirement is not met. Expected data type: Integral, Expected match: 100.0%. Observed: Only 0.0% of data is Integral.  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 24\n",
    "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "constraints_df = pd.json_normalize(violations.body_dict[\"violations\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other commands\n",
    "We can also start and stop the monitoring schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 25\n",
    "#my_default_monitor.stop_monitoring_schedule()\n",
    "#my_default_monitor.start_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the resources\n",
    "\n",
    "You can keep your endpoint running to continue capturing data. If you do not plan to collect more data or use this endpoint further, you should delete the endpoint to avoid incurring additional charges. Note that deleting your endpoint does not delete the data that was captured during the model invocations. That data persists in Amazon S3 until you delete it yourself.\n",
    "\n",
    "But before that, you need to delete the schedule first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting Monitoring Schedule with name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-04-44-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Deleting Data Quality Job Definition with name: data-quality-job-definition-2023-05-14-04-44-15-747\n"
     ]
    }
   ],
   "source": [
    "# cell 26\n",
    "my_default_monitor.delete_monitoring_schedule()\n",
    "time.sleep(60) # actually wait for the deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: DEMO-xgb-churn-pred-model-monitor-2023-05-14-04-01-38\n",
      "INFO:sagemaker:Deleting endpoint with name: DEMO-xgb-churn-pred-model-monitor-2023-05-14-04-01-38\n"
     ]
    }
   ],
   "source": [
    "# cell 27\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DescribeEndpointConfig operation: Could not find endpoint configuration \"arn:aws:sagemaker:us-east-1:369074678854:endpoint-config/demo-xgb-churn-pred-model-monitor-2023-05-14-04-01-38\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ec7c5764f1c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# cell 28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mdelete_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mrequest_failed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mfailed_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mcurrent_model_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent_model_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36m_get_model_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0mcurrent_endpoint_config_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_endpoint_config_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         endpoint_config = self.sagemaker_session.sagemaker_client.describe_endpoint_config(\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_endpoint_config_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         )\n\u001b[1;32m    533\u001b[0m         \u001b[0mproduction_variants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendpoint_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ProductionVariants\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 )\n\u001b[1;32m    529\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DescribeEndpointConfig operation: Could not find endpoint configuration \"arn:aws:sagemaker:us-east-1:369074678854:endpoint-config/demo-xgb-churn-pred-model-monitor-2023-05-14-04-01-38\"."
     ]
    }
   ],
   "source": [
    "# cell 28\n",
    "predictor.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Option 2: Model monitoring with Batch transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PART A: Capturing data from Batch Transform jobs\n",
    "Create a Batch transform job to showcase the data capture capability in action.\n",
    "\n",
    "### 1) Upload the pre-trained model to Amazon S3\n",
    "This code uploads a pre-trained XGBoost model that is ready for you to deploy. This model was trained using the XGB Churn Prediction Notebook in SageMaker. You can also use your own pre-trained model in this step. If you already have a pretrained model in Amazon S3, you can add it instead by specifying the s3_key.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_file = open(\"model/xgb-churn-prediction-model.tar.gz\", \"rb\")\n",
    "s3_key = os.path.join(prefix, \"xgb-churn-prediction-model.tar.gz\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(s3_key).upload_fileobj(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "model_name = \"DEMO-xgb-churn-pred-model-monitor-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = \"https://{}.s3-{}.amazonaws.com/{}/xgb-churn-prediction-model.tar.gz\".format(\n",
    "    bucket, region, prefix\n",
    ")\n",
    "\n",
    "image_uri = retrieve(\"xgboost\", boto3.Session().region_name, \"0.90-1\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=model_url, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Upload test data for batch inference that will be used as input for a Batch Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: test_data/test-dataset-input-cols.csv to s3://sagemaker-us-east-1-369074678854/transform-input/test-dataset-input-cols.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp test_data/test-dataset-input-cols.csv s3://{bucket}/transform-input/test-dataset-input-cols.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Create the Batch Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import BatchDataCaptureConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2023-05-14-05-34-04-762\n",
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2023-05-14-05-34-05-441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................\u001b[34m[2023-05-14 05:39:19 +0000] [14] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2023-05-14 05:39:19 +0000] [14] [INFO] Listening at: unix:/tmp/gunicorn.sock (14)\u001b[0m\n",
      "\u001b[34m[2023-05-14 05:39:19 +0000] [14] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2023-05-14 05:39:19 +0000] [21] [INFO] Booting worker with pid: 21\u001b[0m\n",
      "\u001b[34m[2023-05-14 05:39:19 +0000] [22] [INFO] Booting worker with pid: 22\u001b[0m\n",
      "\u001b[34m[2023-05-14 05:39:19 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m[2023-05-14 05:39:19 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[35m[2023-05-14 05:39:19 +0000] [14] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[35m[2023-05-14 05:39:19 +0000] [14] [INFO] Listening at: unix:/tmp/gunicorn.sock (14)\u001b[0m\n",
      "\u001b[35m[2023-05-14 05:39:19 +0000] [14] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2023-05-14 05:39:19 +0000] [21] [INFO] Booting worker with pid: 21\u001b[0m\n",
      "\u001b[35m[2023-05-14 05:39:19 +0000] [22] [INFO] Booting worker with pid: 22\u001b[0m\n",
      "\u001b[35m[2023-05-14 05:39:19 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[35m[2023-05-14 05:39:19 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m[2023-05-14:05:39:25:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [14/May/2023:05:39:25 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2023-05-14:05:39:25:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-05-14:05:39:25:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [14/May/2023:05:39:25 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2023-05-14:05:39:25:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [14/May/2023:05:39:25 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2023-05-14:05:39:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [14/May/2023:05:39:26 +0000] \"POST /invocations HTTP/1.1\" 200 6762 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [14/May/2023:05:39:25 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2023-05-14:05:39:26:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [14/May/2023:05:39:26 +0000] \"POST /invocations HTTP/1.1\" 200 6762 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2023-05-14T05:39:26.005:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transfomer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    accept=\"text/csv\",\n",
    "    assemble_with=\"Line\",\n",
    ")\n",
    "\n",
    "transfomer.transform(\n",
    "    \"s3://{}/transform-input\".format(bucket),\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\",\n",
    "    # configure the data capturing\n",
    "    batch_data_capture_config=BatchDataCaptureConfig(\n",
    "        destination_s3_uri=s3_capture_upload_path,\n",
    "    ),\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Examine the Batch Transform Captured Data\n",
    "\n",
    "There are two directory under `s3_capture_upload_path`, one is the `/input`, another is the `/output`. Under the `/input` is the captured data file for transform input, whereas, the under the `/output` is the captured data file for transform output. Note that, batch transform data capture is unlike Endpoint data capture, it does not capture the data and log to s3 as this will create tremendous amount of duplications. Instead, batch transform captures data in manifests. The manifests contain the source transform input or output s3 locations.\n",
    "\n",
    "Lets take a look at the captured data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-14 01:39:10         99 sagemaker/DEMO-ModelMonitor/datacapture/input/2023/05/14/01/f217600e-cad0-4cb1-94ee-6c8bef3cfbac.json\n",
      "2023-05-14 05:39:27         99 sagemaker/DEMO-ModelMonitor/datacapture/input/2023/05/14/05/b736e403-8f8b-4ded-ac23-dd8d765fe525.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {s3_capture_upload_path}/input/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "captured_input_s3_key = [\n",
    "    k[\"Key\"]\n",
    "    for k in s3.list_objects_v2(Bucket=bucket, Prefix=f\"{data_capture_prefix}/input/\")[\"Contents\"]\n",
    "]\n",
    "assert len(captured_input_s3_key) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_input_body = s3.get_object(Bucket=bucket, Key=captured_input_s3_key[0])[\"Body\"]\n",
    "sample_input_content = json.loads(sample_input_body.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prefix': 's3://sagemaker-us-east-1-369074678854/transform-input'},\n",
       " '/test-dataset-input-cols.csv']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid data duplication, the captured data are manifest files. Each manifest is a JSONL file that contains the Amazon S3 locations of the source objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-14 01:39:10        129 sagemaker/DEMO-ModelMonitor/datacapture/output/2023/05/14/01/f6830e87-5b81-4328-8e8d-b4d4d011501a.json\n",
      "2023-05-14 05:39:27        129 sagemaker/DEMO-ModelMonitor/datacapture/output/2023/05/14/05/455ae827-2c23-495b-a11a-47dcb2ee4110.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {s3_capture_upload_path}/output/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "captured_input_s3_key = [\n",
    "    k[\"Key\"]\n",
    "    for k in s3.list_objects_v2(Bucket=bucket, Prefix=f\"{data_capture_prefix}/output/\")[\"Contents\"]\n",
    "]\n",
    "assert len(captured_input_s3_key) > 0\n",
    "sample_output_body = s3.get_object(Bucket=bucket, Key=captured_input_s3_key[0])[\"Body\"]\n",
    "sample_output_content = json.loads(sample_output_body.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prefix': 's3://sagemaker-us-east-1-369074678854/sagemaker-xgboost-2023-05-14-01-33-58-972/'},\n",
       " 'test-dataset-input-cols.csv.out']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, you observed how you can enable capturing the input or output payloads of your batch transform job with a new parameter. You have also observed what the captured format looks like in Amazon S3. Next, continue to explore how Amazon SageMaker helps with monitoring the data collected in Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PART B: Model Monitor - Baseling and continuous monitoring\n",
    "\n",
    "### 5) Create a Baseline that will be used by Model Monitor\n",
    "\n",
    "In addition to collecting the data, Amazon SageMaker provides the capability for you to monitor and evaluate the data observed by Batch transform. For this:\n",
    "1. Create a baseline with which you compare the realtime traffic. \n",
    "1. Once a baseline is ready, setup a schedule to continously evaluate and compare against the baseline.\n",
    "\n",
    "In general this can be done parallel to the Transform Job\n",
    "\n",
    "The training dataset with which you trained the model is usually a good baseline dataset. Note that the training dataset data schema and the inference dataset schema should exactly match (i.e. the number and order of the features).\n",
    "\n",
    "From the training dataset you can ask Amazon SageMaker to suggest a set of baseline `constraints` and generate descriptive `statistics` to explore the data. For this example, upload the training dataset that was used to train the pre-trained model included in this example. If you already have it in Amazon S3, you can directly point to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/baselining/data\n",
      "Baseline results uri: s3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/baselining/results\n"
     ]
    }
   ],
   "source": [
    "# copy over the training dataset to Amazon S3 (if you already have it in Amazon S3, you could reuse it)\n",
    "baseline_prefix = prefix + \"/baselining\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = \"s3://{}/{}\".format(bucket, baseline_data_prefix)\n",
    "baseline_results_uri = \"s3://{}/{}\".format(bucket, baseline_results_prefix)\n",
    "print(\"Baseline data uri: {}\".format(baseline_data_uri))\n",
    "print(\"Baseline results uri: {}\".format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data_file = open(\"test_data/training-dataset-with-header.csv\", \"rb\")\n",
    "s3_key = os.path.join(baseline_prefix, \"data\", \"training-dataset-with-header.csv\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(s3_key).upload_fileobj(training_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the training data ready in Amazon S3, start a job to `suggest` constraints. `DefaultModelMonitor.suggest_baseline(..)` starts a `ProcessingJob` using an Amazon SageMaker provided Model Monitor container to generate the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2023-05-14-05-40-04-188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................\u001b[34m2023-05-14 05:44:25,444 - matplotlib.font_manager - INFO - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:25.983571: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:25.983599: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:27.516466: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:27.516493: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:27.516514: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-119-4.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:27.516774: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,067 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:369074678854:processing-job/baseline-suggestion-job-2023-05-14-05-40-04-188', 'ProcessingJobName': 'baseline-suggestion-job-2023-05-14-05-40-04-188', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/baselining/data/training-dataset-with-header.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/baselining/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::369074678854:role/sagemaker-immersion-day-SageMakerExecutionRole-2UAKB1ISM2UJ', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,067 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,067 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,068 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,068 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,125 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,125 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,126 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,133 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,133 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,133 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,585 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.119.4\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoo\u001b[0m\n",
      "\u001b[34mp-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_362\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,595 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:29,599 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-b6f04f0d-ddb3-4353-9af1-a06797bd5738\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,074 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,085 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,086 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,089 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,093 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,093 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,093 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,093 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,123 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,133 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,133 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,137 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,141 INFO blockmanagement.BlockManager: The block deletion will start around 2023 May 14 05:44:30\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,142 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,142 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,143 INFO util.GSet: 2.0% max memory 3.1 GB = 63.9 MB\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,143 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,220 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,223 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,223 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,223 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,224 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,224 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,224 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,224 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,224 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,224 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,224 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,224 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,249 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,249 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,249 INFO util.GSet: 1.0% max memory 3.1 GB = 31.9 MB\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,249 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,251 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,251 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,251 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,251 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,255 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,259 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,259 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,259 INFO util.GSet: 0.25% max memory 3.1 GB = 8.0 MB\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,259 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,265 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,265 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,265 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,269 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,269 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,270 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,270 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,271 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 980.9 KB\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,271 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,293 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1195878688-10.0.119.4-1684043070285\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,307 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,314 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,391 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,403 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,407 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.119.4\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:30,416 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:32,472 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:32,473 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:34,549 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:34,550 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:36,614 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:36,615 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:38,720 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:38,721 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:40,892 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:40,892 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:50,900 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:52,485 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:52,908 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:52,943 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:52,954 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,361 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,385 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,385 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,385 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,386 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,408 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11536, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,421 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,423 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,469 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,469 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,470 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,470 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,470 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,796 INFO util.Utils: Successfully started service 'sparkDriver' on port 33139.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,828 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,867 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,886 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,887 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,927 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,953 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-340d70ee-759e-443b-9031-df90121f52a6\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:53,973 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:54,023 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:54,066 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.119.4:33139/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1684043093357\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:54,546 INFO client.RMProxy: Connecting to ResourceManager at /10.0.119.4:8032\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:55,380 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:55,380 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:55,389 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15731 MB per container)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:55,389 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:55,389 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:55,390 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:55,395 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:55,470 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:57,275 INFO yarn.Client: Uploading resource file:/tmp/spark-2451e5b9-a4f1-4e9e-bb1e-3af5ee5cc68e/__spark_libs__8707637077512337478.zip -> hdfs://10.0.119.4/user/root/.sparkStaging/application_1684043076084_0001/__spark_libs__8707637077512337478.zip\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:58,960 INFO yarn.Client: Uploading resource file:/tmp/spark-2451e5b9-a4f1-4e9e-bb1e-3af5ee5cc68e/__spark_conf__4216909768846432646.zip -> hdfs://10.0.119.4/user/root/.sparkStaging/application_1684043076084_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:59,022 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:59,022 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:59,022 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:59,022 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:59,022 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:59,049 INFO yarn.Client: Submitting application application_1684043076084_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2023-05-14 05:44:59,259 INFO impl.YarnClientImpl: Submitted application application_1684043076084_0001\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:00,265 INFO yarn.Client: Application report for application_1684043076084_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:00,269 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1684043099164\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1684043076084_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:01,272 INFO yarn.Client: Application report for application_1684043076084_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:02,275 INFO yarn.Client: Application report for application_1684043076084_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:03,283 INFO yarn.Client: Application report for application_1684043076084_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:04,110 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1684043076084_0001), /proxy/application_1684043076084_0001\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:04,287 INFO yarn.Client: Application report for application_1684043076084_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:04,288 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.119.4\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1684043099164\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1684043076084_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:04,289 INFO cluster.YarnClientSchedulerBackend: Application application_1684043076084_0001 has started running.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:04,301 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35659.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:04,301 INFO netty.NettyBlockTransferService: Server created on 10.0.119.4:35659\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:04,304 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:04,322 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.119.4, 35659, None)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:04,328 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.119.4:35659 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.119.4, 35659, None)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:04,332 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.119.4, 35659, None)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:04,334 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.119.4, 35659, None)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:04,479 INFO util.log: Logging initialized @13372ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:05,690 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:09,284 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.119.4:46910) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:09,473 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:39795 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 39795, None)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:24,479 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:24,669 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:24,723 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:24,729 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:25,767 INFO datasources.InMemoryFileIndex: It took 40 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:25,987 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,278 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.3 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,281 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.119.4:35659 (size: 39.3 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,286 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,616 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,619 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,622 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 375873\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,679 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,702 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,702 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,703 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,705 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,711 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,761 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,765 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,766 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.119.4:35659 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,767 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,783 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,784 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:26,826 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4641 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:27,097 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:39795 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:27,921 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:39795 (size: 39.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:28,259 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1445 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:28,272 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.535 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:28,277 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:28,279 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:28,283 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:28,285 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.605510 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:28,463 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.119.4:35659 in memory (size: 39.3 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:28,476 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:39795 in memory (size: 39.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:28,508 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.119.4:35659 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:28,509 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:39795 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,490 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,492 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,495 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Churn: string, Account Length: string, VMail Message: string, Day Mins: string, Day Calls: string ... 68 more fields>\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,537 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,724 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,741 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,742 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.119.4:35659 (size: 39.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,743 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,758 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,828 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,830 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:100) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,830 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:100)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,830 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,832 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,840 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,892 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 29.8 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,894 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,895 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.119.4:35659 (size: 11.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,895 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,896 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,896 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,900 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:30,935 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:39795 (size: 11.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:31,899 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:39795 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:32,272 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:39795 (size: 188.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:32,493 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1596 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:32,494 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:100) finished in 1.648 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:32,494 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:32,495 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:32,495 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:32,496 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:100, took 1.667663 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:32,970 INFO codegen.CodeGenerator: Code generated in 298.035419 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,569 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,572 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,572 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,572 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,574 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,576 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,596 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 125.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,599 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 37.6 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,599 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.119.4:35659 (size: 37.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,600 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,602 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,602 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,609 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:33,635 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:39795 (size: 37.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,634 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1026 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,634 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,637 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.058 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,638 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,638 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,639 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,639 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,716 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,718 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,719 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,719 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,719 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,720 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,730 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 178.7 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,732 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,733 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.119.4:35659 (size: 49.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,733 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,734 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,734 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,736 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,752 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:39795 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:34,791 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,161 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 426 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,161 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,163 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.437 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,163 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,163 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,164 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.447659 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,239 INFO codegen.CodeGenerator: Code generated in 65.057122 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,565 INFO codegen.CodeGenerator: Code generated in 33.670925 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,650 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,651 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,651 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,651 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,653 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,654 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,680 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 49.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,682 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,683 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.119.4:35659 (size: 19.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,686 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,686 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,686 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,688 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:35,715 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:39795 (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,128 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 440 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,128 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,130 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 0.475 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,131 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,131 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,131 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 0.481288 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,618 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:39795 in memory (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,623 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.119.4:35659 in memory (size: 19.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,667 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.119.4:35659 in memory (size: 37.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,680 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:39795 in memory (size: 37.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,701 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.119.4:35659 in memory (size: 49.3 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,706 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:39795 in memory (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,756 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.119.4:35659 in memory (size: 11.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,757 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:39795 in memory (size: 11.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,776 INFO codegen.CodeGenerator: Code generated in 140.063291 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,785 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,785 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,785 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,785 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,786 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,787 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,795 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 87.0 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,797 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,797 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.119.4:35659 (size: 27.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,798 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,798 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,798 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,799 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,817 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:39795 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,934 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 135 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,934 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,935 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.147 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,936 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,936 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,936 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:36,936 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,109 INFO codegen.CodeGenerator: Code generated in 79.288204 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,123 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,124 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,124 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,125 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,125 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,127 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,134 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 67.4 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,136 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,136 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.119.4:35659 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,137 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,138 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,138 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,140 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,153 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:39795 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,158 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,236 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 97 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,236 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,237 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.109 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,237 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,239 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,239 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.116215 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,349 INFO codegen.CodeGenerator: Code generated in 64.722368 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,457 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,461 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,461 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,461 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,461 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,461 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,463 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,474 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 41.9 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,476 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,480 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,481 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,482 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,482 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,484 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:37,498 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,783 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1299 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,784 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,785 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.320 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,786 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,787 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,787 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,787 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,788 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,790 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,792 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,794 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,795 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,796 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,796 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,798 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,815 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,822 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,867 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 70 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,867 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,868 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.079 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,869 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,869 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:38,869 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.412079 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,094 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,094 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,094 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,095 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,096 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,097 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,104 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 94.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,106 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,107 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.119.4:35659 (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,108 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,108 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,108 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,110 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,121 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:39795 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,299 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 189 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,299 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,300 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.201 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,300 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,300 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,300 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,301 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,380 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,381 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,381 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,381 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,382 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,382 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,392 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 179.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,395 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,398 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.119.4:35659 (size: 49.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,399 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,399 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,399 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,401 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,413 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:39795 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,428 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,579 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 178 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,580 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,582 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.197 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,585 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,585 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,586 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.205872 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,757 INFO codegen.CodeGenerator: Code generated in 13.082011 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,781 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,782 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,783 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,783 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,784 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,785 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,791 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 49.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,792 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,793 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.119.4:35659 (size: 19.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,793 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,793 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,793 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,795 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,804 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:39795 (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,901 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 107 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,901 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,902 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.116 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,902 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,902 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:39,902 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.120715 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,120 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.119.4:35659 in memory (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,121 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:39795 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,156 INFO codegen.CodeGenerator: Code generated in 81.475279 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,156 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:39795 in memory (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,159 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.119.4:35659 in memory (size: 19.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,165 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,165 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,166 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,166 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,166 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,167 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,171 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 86.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,173 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,174 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.119.4:35659 (size: 27.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,174 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,174 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:39795 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,176 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,177 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,179 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,186 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.119.4:35659 in memory (size: 30.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,194 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:39795 (size: 27.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,241 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.119.4:35659 in memory (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,249 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:39795 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,306 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.119.4:35659 in memory (size: 27.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,312 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:39795 in memory (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,313 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 134 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,314 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.146 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,315 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,315 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,314 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,315 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,315 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,360 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.119.4:35659 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,365 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:39795 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,439 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.119.4:35659 in memory (size: 49.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,440 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:39795 in memory (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,521 INFO codegen.CodeGenerator: Code generated in 132.049344 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,547 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,549 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,550 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,550 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,550 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,552 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,554 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.8 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,561 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,561 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.119.4:35659 (size: 19.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,562 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,563 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,563 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,564 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,580 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:39795 (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,584 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,651 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 87 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,651 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,652 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.099 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,654 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,656 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,656 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.108454 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,770 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,771 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,772 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,772 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,772 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,772 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,773 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,779 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 41.9 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,787 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,787 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,788 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,788 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,789 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,790 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,799 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,848 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 58 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,849 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,850 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.075 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,850 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,850 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,850 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,850 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,851 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,852 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,854 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,855 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,855 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,856 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,856 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,858 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,867 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,879 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,901 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 43 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,902 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,902 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.051 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,903 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,903 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:40,903 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.132838 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,120 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,120 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,120 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,121 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,121 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,122 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,126 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 94.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,128 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,128 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.119.4:35659 (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,129 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,130 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,130 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,131 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,143 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:39795 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,289 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 158 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,289 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,290 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.167 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,291 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,292 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,292 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,292 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,336 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,338 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,340 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,340 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,341 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,341 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,349 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 179.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,354 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,355 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.119.4:35659 (size: 49.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,356 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,357 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,357 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,358 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,419 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:39795 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,503 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,612 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 254 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,613 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,614 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.272 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,614 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,615 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,615 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.278373 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,781 INFO codegen.CodeGenerator: Code generated in 15.609398 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,836 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,838 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,839 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,839 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,839 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,841 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,847 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 48.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,849 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,850 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.119.4:35659 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,851 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,851 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,852 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,853 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:41,867 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:39795 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,113 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 259 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,113 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,114 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.273 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,114 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,114 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,115 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.278634 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,343 INFO codegen.CodeGenerator: Code generated in 71.5246 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,357 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,357 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,357 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,357 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,358 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,358 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,368 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 87.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,371 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,372 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.119.4:35659 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,373 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,375 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,376 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,378 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,391 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:39795 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,491 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 114 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,491 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,492 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.133 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,493 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,493 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,493 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,493 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,665 INFO codegen.CodeGenerator: Code generated in 65.312116 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,680 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,682 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,682 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,682 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,682 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,683 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,685 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 67.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,691 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,692 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.119.4:35659 (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,692 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,694 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,694 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,696 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,708 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:39795 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,714 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,777 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 81 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,778 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,778 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.094 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,779 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,779 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,781 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.100004 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,931 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,932 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,933 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,933 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,933 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,933 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,937 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,968 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:39795 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,971 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.119.4:35659 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,982 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 41.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,983 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,985 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,986 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,987 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,987 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:42,988 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,015 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,048 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.119.4:35659 in memory (size: 19.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,071 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:39795 in memory (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,106 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.119.4:35659 in memory (size: 49.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,107 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:39795 in memory (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,108 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 120 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,108 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,108 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.170 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,109 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,109 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,109 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,109 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,109 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,111 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,113 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,114 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,116 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,125 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,126 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,127 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,138 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,142 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,156 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 29 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,156 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,156 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.046 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,157 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,157 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,157 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.225546 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,161 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:39795 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,163 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.119.4:35659 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,212 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:39795 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,222 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.119.4:35659 in memory (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,336 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:39795 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,342 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.119.4:35659 in memory (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,383 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:39795 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,390 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.119.4:35659 in memory (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,413 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,413 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,413 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,413 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,414 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.119.4:35659 in memory (size: 27.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,414 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:39795 in memory (size: 27.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,415 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,415 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,420 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 94.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,422 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,423 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.119.4:35659 (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,423 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.119.4:35659 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,423 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,424 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,424 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,425 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,426 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:39795 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,439 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:39795 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,609 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 184 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,609 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,610 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.194 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,614 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,614 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,614 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,615 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,649 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,650 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,650 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,651 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,651 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,651 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,656 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 179.7 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,658 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,660 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.119.4:35659 (size: 49.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,661 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,661 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,662 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,663 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,672 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:39795 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,680 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,742 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 79 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,742 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,743 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.091 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,744 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,744 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,744 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.095068 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,837 INFO codegen.CodeGenerator: Code generated in 8.240545 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,866 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,867 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,868 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,868 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,868 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,869 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,881 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 48.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,883 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,883 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.119.4:35659 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,884 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,884 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,885 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,886 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,896 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:39795 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,984 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 98 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,984 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,985 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 0.115 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,986 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,987 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:43,987 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 0.120381 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,236 INFO codegen.CodeGenerator: Code generated in 41.176125 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,243 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,243 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,243 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,244 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,244 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,245 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,249 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 87.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,251 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,251 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.119.4:35659 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,252 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,252 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,252 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,254 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,266 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:39795 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,359 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 106 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,359 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,360 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.115 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,360 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,360 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,360 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,360 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,474 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,475 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,476 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,476 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,476 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,477 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,479 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 67.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,480 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,481 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.119.4:35659 (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,482 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,482 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,482 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,484 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,493 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:39795 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,497 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,503 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 20 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,503 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,504 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,504 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,504 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,505 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.030312 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,592 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,593 INFO scheduler.DAGScheduler: Registering RDD 150 (countByKey at ColumnProfiler.scala:592) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,593 INFO scheduler.DAGScheduler: Got job 25 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,594 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,594 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,594 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,595 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,602 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 41.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,604 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,604 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,605 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,606 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,606 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,607 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,622 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,690 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 83 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,690 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,691 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (countByKey at ColumnProfiler.scala:592) finished in 0.095 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,691 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,692 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,692 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 37)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,692 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,692 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,694 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,696 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,696 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,697 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,697 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,698 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,699 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,713 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,716 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,727 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 28 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,727 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,727 INFO scheduler.DAGScheduler: ResultStage 37 (countByKey at ColumnProfiler.scala:592) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,728 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,728 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,728 INFO scheduler.DAGScheduler: Job 25 finished: countByKey at ColumnProfiler.scala:592, took 0.135858 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,832 INFO scheduler.DAGScheduler: Registering RDD 156 (collect at AnalysisRunner.scala:326) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,832 INFO scheduler.DAGScheduler: Got map stage job 26 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,832 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,832 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,833 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,833 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,837 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 94.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,839 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,839 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.119.4:35659 (size: 30.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,840 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,840 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,841 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,842 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:44,856 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:39795 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,033 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 191 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,033 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,034 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326) finished in 0.200 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,034 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,034 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,034 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,034 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,061 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,062 INFO scheduler.DAGScheduler: Got job 27 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,062 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,062 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,062 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,062 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,070 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 179.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,073 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,073 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.119.4:35659 (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,074 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,074 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,074 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,076 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,085 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:39795 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,092 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,160 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 85 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,160 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,161 INFO scheduler.DAGScheduler: ResultStage 40 (collect at AnalysisRunner.scala:326) finished in 0.098 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,161 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,161 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,167 INFO scheduler.DAGScheduler: Job 27 finished: collect at AnalysisRunner.scala:326, took 0.105749 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,257 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,257 INFO scheduler.DAGScheduler: Got job 28 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,258 INFO scheduler.DAGScheduler: Final stage: ResultStage 41 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,258 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,258 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,258 INFO scheduler.DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,263 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 48.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,265 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,265 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.119.4:35659 (size: 19.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,266 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,266 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,266 INFO cluster.YarnScheduler: Adding task set 41.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,268 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 32) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,281 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:39795 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,326 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 32) in 59 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,326 INFO cluster.YarnScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,327 INFO scheduler.DAGScheduler: ResultStage 41 (treeReduce at KLLRunner.scala:107) finished in 0.067 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,327 INFO scheduler.DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,327 INFO cluster.YarnScheduler: Killing all running tasks in stage 41: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,327 INFO scheduler.DAGScheduler: Job 28 finished: treeReduce at KLLRunner.scala:107, took 0.070417 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,380 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:39795 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,422 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.0.119.4:35659 in memory (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,459 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.119.4:35659 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,467 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:39795 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,493 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.0.119.4:35659 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,494 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:39795 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,504 INFO scheduler.DAGScheduler: Registering RDD 174 (collect at AnalysisRunner.scala:326) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,505 INFO scheduler.DAGScheduler: Got map stage job 29 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,505 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,505 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,505 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,505 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,516 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:39795 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,517 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 87.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,519 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.119.4:35659 in memory (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,521 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,521 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.119.4:35659 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,522 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,522 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,522 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,523 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,535 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:39795 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,535 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:39795 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,542 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.119.4:35659 in memory (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,544 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.0.119.4:35659 in memory (size: 30.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,545 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:39795 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,555 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 32 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,556 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,558 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.119.4:35659 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,559 INFO scheduler.DAGScheduler: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326) finished in 0.053 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,560 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,560 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,560 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,561 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,561 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:39795 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,564 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.119.4:35659 in memory (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,564 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:39795 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,571 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.119.4:35659 in memory (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,571 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:39795 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,575 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.0.119.4:35659 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,579 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:39795 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,585 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.119.4:35659 in memory (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,587 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:39795 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,595 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.119.4:35659 in memory (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,596 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:39795 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,634 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,635 INFO scheduler.DAGScheduler: Got job 30 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,635 INFO scheduler.DAGScheduler: Final stage: ResultStage 44 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,635 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,636 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,636 INFO scheduler.DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,638 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 67.7 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,639 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,640 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.119.4:35659 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,641 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,641 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,641 INFO cluster.YarnScheduler: Adding task set 44.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,643 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 44.0 (TID 34) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,654 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:39795 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,658 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,668 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 44.0 (TID 34) in 26 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,668 INFO cluster.YarnScheduler: Removed TaskSet 44.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,669 INFO scheduler.DAGScheduler: ResultStage 44 (collect at AnalysisRunner.scala:326) finished in 0.032 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,670 INFO scheduler.DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,670 INFO cluster.YarnScheduler: Killing all running tasks in stage 44: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,670 INFO scheduler.DAGScheduler: Job 30 finished: collect at AnalysisRunner.scala:326, took 0.035667 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,720 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,721 INFO scheduler.DAGScheduler: Registering RDD 185 (countByKey at ColumnProfiler.scala:592) as input to shuffle 14\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,721 INFO scheduler.DAGScheduler: Got job 31 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,721 INFO scheduler.DAGScheduler: Final stage: ResultStage 46 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,721 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,721 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,722 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,726 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 41.9 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,728 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,728 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,728 INFO spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,729 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,729 INFO cluster.YarnScheduler: Adding task set 45.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,730 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 35) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,743 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,780 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 35) in 50 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,780 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,781 INFO scheduler.DAGScheduler: ShuffleMapStage 45 (countByKey at ColumnProfiler.scala:592) finished in 0.058 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,781 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,781 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,781 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 46)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,781 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,781 INFO scheduler.DAGScheduler: Submitting ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,783 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,785 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,786 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,786 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,786 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,787 INFO cluster.YarnScheduler: Adding task set 46.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,788 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 46.0 (TID 36) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,806 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,809 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,821 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 46.0 (TID 36) in 34 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,821 INFO cluster.YarnScheduler: Removed TaskSet 46.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,822 INFO scheduler.DAGScheduler: ResultStage 46 (countByKey at ColumnProfiler.scala:592) finished in 0.041 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,822 INFO scheduler.DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,822 INFO cluster.YarnScheduler: Killing all running tasks in stage 46: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,822 INFO scheduler.DAGScheduler: Job 31 finished: countByKey at ColumnProfiler.scala:592, took 0.102264 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,992 INFO scheduler.DAGScheduler: Registering RDD 191 (collect at AnalysisRunner.scala:326) as input to shuffle 15\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,992 INFO scheduler.DAGScheduler: Got map stage job 32 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,992 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,992 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,993 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,993 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,997 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 94.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,998 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,999 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.119.4:35659 (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:45,999 INFO spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,000 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,000 INFO cluster.YarnScheduler: Adding task set 47.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,001 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 47.0 (TID 37) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,015 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on algo-1:39795 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,446 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 47.0 (TID 37) in 445 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,446 INFO cluster.YarnScheduler: Removed TaskSet 47.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,446 INFO scheduler.DAGScheduler: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326) finished in 0.452 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,447 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,448 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,448 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,448 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,486 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,486 INFO scheduler.DAGScheduler: Got job 33 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,486 INFO scheduler.DAGScheduler: Final stage: ResultStage 49 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,487 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,487 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,487 INFO scheduler.DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,493 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 179.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,495 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,496 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.119.4:35659 (size: 49.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,496 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,496 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,496 INFO cluster.YarnScheduler: Adding task set 49.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,497 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 49.0 (TID 38) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,514 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on algo-1:39795 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,525 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,616 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 49.0 (TID 38) in 119 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,616 INFO cluster.YarnScheduler: Removed TaskSet 49.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,616 INFO scheduler.DAGScheduler: ResultStage 49 (collect at AnalysisRunner.scala:326) finished in 0.129 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,617 INFO scheduler.DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,617 INFO cluster.YarnScheduler: Killing all running tasks in stage 49: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,617 INFO scheduler.DAGScheduler: Job 33 finished: collect at AnalysisRunner.scala:326, took 0.130991 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,773 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,778 INFO scheduler.DAGScheduler: Got job 34 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,778 INFO scheduler.DAGScheduler: Final stage: ResultStage 50 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,778 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,779 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,779 INFO scheduler.DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[204] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,785 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 48.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,802 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,802 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.0.119.4:35659 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,803 INFO spark.SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,807 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[204] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,807 INFO cluster.YarnScheduler: Adding task set 50.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,808 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 50.0 (TID 39) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,818 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on algo-1:39795 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,871 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 50.0 (TID 39) in 63 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,872 INFO cluster.YarnScheduler: Removed TaskSet 50.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,873 INFO scheduler.DAGScheduler: ResultStage 50 (treeReduce at KLLRunner.scala:107) finished in 0.093 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,874 INFO scheduler.DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,874 INFO cluster.YarnScheduler: Killing all running tasks in stage 50: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,875 INFO scheduler.DAGScheduler: Job 34 finished: treeReduce at KLLRunner.scala:107, took 0.101931 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,997 INFO scheduler.DAGScheduler: Registering RDD 209 (collect at AnalysisRunner.scala:326) as input to shuffle 16\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,998 INFO scheduler.DAGScheduler: Got map stage job 35 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,998 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 51 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,998 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,999 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:46,999 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[209] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,003 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 87.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,004 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,005 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.0.119.4:35659 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,005 INFO spark.SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,005 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[209] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,005 INFO cluster.YarnScheduler: Adding task set 51.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,007 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 51.0 (TID 40) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,023 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on algo-1:39795 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,045 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 51.0 (TID 40) in 38 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,045 INFO cluster.YarnScheduler: Removed TaskSet 51.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,046 INFO scheduler.DAGScheduler: ShuffleMapStage 51 (collect at AnalysisRunner.scala:326) finished in 0.046 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,046 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,046 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,047 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,047 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,110 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,111 INFO scheduler.DAGScheduler: Got job 36 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,112 INFO scheduler.DAGScheduler: Final stage: ResultStage 53 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,112 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 52)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,112 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,113 INFO scheduler.DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[212] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,115 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 67.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,116 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,117 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.0.119.4:35659 (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,118 INFO spark.SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,118 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[212] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,118 INFO cluster.YarnScheduler: Adding task set 53.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,120 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 53.0 (TID 41) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,136 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on algo-1:39795 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,142 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,151 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 53.0 (TID 41) in 31 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,151 INFO cluster.YarnScheduler: Removed TaskSet 53.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,152 INFO scheduler.DAGScheduler: ResultStage 53 (collect at AnalysisRunner.scala:326) finished in 0.039 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,152 INFO scheduler.DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,152 INFO cluster.YarnScheduler: Killing all running tasks in stage 53: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,152 INFO scheduler.DAGScheduler: Job 36 finished: collect at AnalysisRunner.scala:326, took 0.041846 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,199 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,199 INFO scheduler.DAGScheduler: Registering RDD 220 (countByKey at ColumnProfiler.scala:592) as input to shuffle 17\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,200 INFO scheduler.DAGScheduler: Got job 37 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,200 INFO scheduler.DAGScheduler: Final stage: ResultStage 55 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,200 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 54)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,200 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 54)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,201 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[220] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,209 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 41.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,211 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,211 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,212 INFO spark.SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,212 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[220] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,212 INFO cluster.YarnScheduler: Adding task set 54.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,214 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 54.0 (TID 42) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,222 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,261 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 54.0 (TID 42) in 48 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,261 INFO cluster.YarnScheduler: Removed TaskSet 54.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,262 INFO scheduler.DAGScheduler: ShuffleMapStage 54 (countByKey at ColumnProfiler.scala:592) finished in 0.059 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,262 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,262 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,262 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 55)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,262 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,262 INFO scheduler.DAGScheduler: Submitting ResultStage 55 (ShuffledRDD[221] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,264 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,265 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,265 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,266 INFO spark.SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,266 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (ShuffledRDD[221] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,266 INFO cluster.YarnScheduler: Adding task set 55.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,267 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 55.0 (TID 43) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,274 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,277 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,284 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 55.0 (TID 43) in 17 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,285 INFO cluster.YarnScheduler: Removed TaskSet 55.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,285 INFO scheduler.DAGScheduler: ResultStage 55 (countByKey at ColumnProfiler.scala:592) finished in 0.022 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,286 INFO scheduler.DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,286 INFO cluster.YarnScheduler: Killing all running tasks in stage 55: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,286 INFO scheduler.DAGScheduler: Job 37 finished: countByKey at ColumnProfiler.scala:592, took 0.087618 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,386 INFO scheduler.DAGScheduler: Registering RDD 226 (collect at AnalysisRunner.scala:326) as input to shuffle 18\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,386 INFO scheduler.DAGScheduler: Got map stage job 38 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,386 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 56 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,386 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,386 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,387 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 56 (MapPartitionsRDD[226] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,390 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 94.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,393 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,394 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.0.119.4:35659 (size: 30.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,394 INFO spark.SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,394 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[226] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,394 INFO cluster.YarnScheduler: Adding task set 56.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,396 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 56.0 (TID 44) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,403 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on algo-1:39795 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,493 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 56.0 (TID 44) in 97 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,494 INFO scheduler.DAGScheduler: ShuffleMapStage 56 (collect at AnalysisRunner.scala:326) finished in 0.107 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,495 INFO cluster.YarnScheduler: Removed TaskSet 56.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,495 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,495 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,495 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,495 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,527 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,528 INFO scheduler.DAGScheduler: Got job 39 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,528 INFO scheduler.DAGScheduler: Final stage: ResultStage 58 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,528 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 57)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,528 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,528 INFO scheduler.DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[229] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,533 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 179.7 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,535 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,535 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.0.119.4:35659 (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,535 INFO spark.SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,535 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[229] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,536 INFO cluster.YarnScheduler: Adding task set 58.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,537 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 58.0 (TID 45) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,545 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on algo-1:39795 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,554 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,606 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 58.0 (TID 45) in 69 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,606 INFO cluster.YarnScheduler: Removed TaskSet 58.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,606 INFO scheduler.DAGScheduler: ResultStage 58 (collect at AnalysisRunner.scala:326) finished in 0.077 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,607 INFO scheduler.DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,607 INFO cluster.YarnScheduler: Killing all running tasks in stage 58: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,608 INFO scheduler.DAGScheduler: Job 39 finished: collect at AnalysisRunner.scala:326, took 0.080455 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,747 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.0.119.4:35659 in memory (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,749 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on algo-1:39795 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,760 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 10.0.119.4:35659 in memory (size: 27.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,760 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on algo-1:39795 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,769 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.0.119.4:35659 in memory (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,770 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on algo-1:39795 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,773 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.0.119.4:35659 in memory (size: 30.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,774 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on algo-1:39795 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,778 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.0.119.4:35659 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,778 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on algo-1:39795 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,780 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.0.119.4:35659 in memory (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,782 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on algo-1:39795 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,790 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.0.119.4:35659 in memory (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,792 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on algo-1:39795 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,795 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.0.119.4:35659 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,799 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on algo-1:39795 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,801 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.0.119.4:35659 in memory (size: 49.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,802 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on algo-1:39795 in memory (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,808 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.0.119.4:35659 in memory (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,810 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on algo-1:39795 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,812 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on 10.0.119.4:35659 in memory (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,813 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on algo-1:39795 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,815 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.0.119.4:35659 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,817 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on algo-1:39795 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,818 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 10.0.119.4:35659 in memory (size: 49.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,822 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,822 INFO scheduler.DAGScheduler: Got job 40 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,822 INFO scheduler.DAGScheduler: Final stage: ResultStage 59 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,822 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,823 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,823 INFO scheduler.DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[239] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,825 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on algo-1:39795 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,830 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 48.9 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,832 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,832 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.0.119.4:35659 (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,833 INFO spark.SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,833 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[239] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,833 INFO cluster.YarnScheduler: Adding task set 59.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,836 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 59.0 (TID 46) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,844 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on algo-1:39795 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,886 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 59.0 (TID 46) in 50 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,886 INFO cluster.YarnScheduler: Removed TaskSet 59.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,887 INFO scheduler.DAGScheduler: ResultStage 59 (treeReduce at KLLRunner.scala:107) finished in 0.063 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,887 INFO scheduler.DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,887 INFO cluster.YarnScheduler: Killing all running tasks in stage 59: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:47,887 INFO scheduler.DAGScheduler: Job 40 finished: treeReduce at KLLRunner.scala:107, took 0.065591 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,083 INFO scheduler.DAGScheduler: Registering RDD 244 (collect at AnalysisRunner.scala:326) as input to shuffle 19\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,083 INFO scheduler.DAGScheduler: Got map stage job 41 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,083 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 60 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,083 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,083 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,084 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 60 (MapPartitionsRDD[244] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,087 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 87.4 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,088 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,089 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.0.119.4:35659 (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,089 INFO spark.SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,090 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 60 (MapPartitionsRDD[244] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,090 INFO cluster.YarnScheduler: Adding task set 60.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,091 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 60.0 (TID 47) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,099 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on algo-1:39795 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,113 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 60.0 (TID 47) in 23 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,114 INFO cluster.YarnScheduler: Removed TaskSet 60.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,114 INFO scheduler.DAGScheduler: ShuffleMapStage 60 (collect at AnalysisRunner.scala:326) finished in 0.030 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,115 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,115 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,115 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,115 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,178 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,178 INFO scheduler.DAGScheduler: Got job 42 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,178 INFO scheduler.DAGScheduler: Final stage: ResultStage 62 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,178 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 61)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,178 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,179 INFO scheduler.DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[247] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,180 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 67.7 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,182 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,182 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.0.119.4:35659 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,182 INFO spark.SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,183 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[247] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,183 INFO cluster.YarnScheduler: Adding task set 62.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,184 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 62.0 (TID 48) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,191 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on algo-1:39795 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,194 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,199 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 62.0 (TID 48) in 15 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,200 INFO cluster.YarnScheduler: Removed TaskSet 62.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,200 INFO scheduler.DAGScheduler: ResultStage 62 (collect at AnalysisRunner.scala:326) finished in 0.021 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,200 INFO scheduler.DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,201 INFO cluster.YarnScheduler: Killing all running tasks in stage 62: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,201 INFO scheduler.DAGScheduler: Job 42 finished: collect at AnalysisRunner.scala:326, took 0.023399 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,246 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,247 INFO scheduler.DAGScheduler: Registering RDD 255 (countByKey at ColumnProfiler.scala:592) as input to shuffle 20\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,247 INFO scheduler.DAGScheduler: Got job 43 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,247 INFO scheduler.DAGScheduler: Final stage: ResultStage 64 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,247 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 63)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,247 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 63)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,248 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 63 (MapPartitionsRDD[255] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,256 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 41.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,257 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,262 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,264 INFO spark.SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,264 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 63 (MapPartitionsRDD[255] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,265 INFO cluster.YarnScheduler: Adding task set 63.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,266 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 63.0 (TID 49) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,273 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,300 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 63.0 (TID 49) in 35 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,300 INFO cluster.YarnScheduler: Removed TaskSet 63.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,301 INFO scheduler.DAGScheduler: ShuffleMapStage 63 (countByKey at ColumnProfiler.scala:592) finished in 0.052 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,301 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,301 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,301 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 64)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,301 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,301 INFO scheduler.DAGScheduler: Submitting ResultStage 64 (ShuffledRDD[256] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,303 INFO memory.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 5.1 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,304 INFO memory.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,304 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,304 INFO spark.SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,304 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (ShuffledRDD[256] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,304 INFO cluster.YarnScheduler: Adding task set 64.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,305 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 64.0 (TID 50) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,313 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,320 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,327 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 64.0 (TID 50) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,327 INFO cluster.YarnScheduler: Removed TaskSet 64.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,328 INFO scheduler.DAGScheduler: ResultStage 64 (countByKey at ColumnProfiler.scala:592) finished in 0.026 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,328 INFO scheduler.DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,328 INFO cluster.YarnScheduler: Killing all running tasks in stage 64: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,328 INFO scheduler.DAGScheduler: Job 43 finished: countByKey at ColumnProfiler.scala:592, took 0.082023 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,464 INFO scheduler.DAGScheduler: Registering RDD 261 (collect at AnalysisRunner.scala:326) as input to shuffle 21\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,464 INFO scheduler.DAGScheduler: Got map stage job 44 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,464 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 65 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,464 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,465 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,465 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 65 (MapPartitionsRDD[261] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,468 INFO memory.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 94.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,470 INFO memory.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,470 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 10.0.119.4:35659 (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,470 INFO spark.SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,471 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 65 (MapPartitionsRDD[261] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,471 INFO cluster.YarnScheduler: Adding task set 65.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,472 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 65.0 (TID 51) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,479 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on algo-1:39795 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,581 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 65.0 (TID 51) in 109 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,581 INFO cluster.YarnScheduler: Removed TaskSet 65.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,582 INFO scheduler.DAGScheduler: ShuffleMapStage 65 (collect at AnalysisRunner.scala:326) finished in 0.117 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,582 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,582 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,582 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,582 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,635 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,636 INFO scheduler.DAGScheduler: Got job 45 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,636 INFO scheduler.DAGScheduler: Final stage: ResultStage 67 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,636 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 66)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,636 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,637 INFO scheduler.DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[264] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,645 INFO memory.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 179.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,647 INFO memory.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,647 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 10.0.119.4:35659 (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,647 INFO spark.SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,647 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[264] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,649 INFO cluster.YarnScheduler: Adding task set 67.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,650 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 67.0 (TID 52) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,659 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on algo-1:39795 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,667 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,722 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 67.0 (TID 52) in 72 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,722 INFO cluster.YarnScheduler: Removed TaskSet 67.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,723 INFO scheduler.DAGScheduler: ResultStage 67 (collect at AnalysisRunner.scala:326) finished in 0.083 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,723 INFO scheduler.DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,723 INFO cluster.YarnScheduler: Killing all running tasks in stage 67: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,723 INFO scheduler.DAGScheduler: Job 45 finished: collect at AnalysisRunner.scala:326, took 0.087620 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,803 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,804 INFO scheduler.DAGScheduler: Got job 46 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,804 INFO scheduler.DAGScheduler: Final stage: ResultStage 68 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,804 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,804 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,804 INFO scheduler.DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[274] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,809 INFO memory.MemoryStore: Block broadcast_55 stored as values in memory (estimated size 48.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,810 INFO memory.MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,810 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on 10.0.119.4:35659 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,811 INFO spark.SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,811 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[274] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,811 INFO cluster.YarnScheduler: Adding task set 68.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,812 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 68.0 (TID 53) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,820 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on algo-1:39795 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,852 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 68.0 (TID 53) in 40 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,853 INFO cluster.YarnScheduler: Removed TaskSet 68.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,853 INFO scheduler.DAGScheduler: ResultStage 68 (treeReduce at KLLRunner.scala:107) finished in 0.048 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,854 INFO scheduler.DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,854 INFO cluster.YarnScheduler: Killing all running tasks in stage 68: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,855 INFO scheduler.DAGScheduler: Job 46 finished: treeReduce at KLLRunner.scala:107, took 0.051280 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,983 INFO scheduler.DAGScheduler: Registering RDD 279 (collect at AnalysisRunner.scala:326) as input to shuffle 22\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,983 INFO scheduler.DAGScheduler: Got map stage job 47 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,983 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 69 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,983 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,983 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,983 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 69 (MapPartitionsRDD[279] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,987 INFO memory.MemoryStore: Block broadcast_56 stored as values in memory (estimated size 87.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,988 INFO memory.MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,989 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on 10.0.119.4:35659 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,989 INFO spark.SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,989 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 69 (MapPartitionsRDD[279] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,989 INFO cluster.YarnScheduler: Adding task set 69.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:48,991 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 69.0 (TID 54) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,001 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on algo-1:39795 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,016 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 69.0 (TID 54) in 26 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,016 INFO cluster.YarnScheduler: Removed TaskSet 69.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,017 INFO scheduler.DAGScheduler: ShuffleMapStage 69 (collect at AnalysisRunner.scala:326) finished in 0.033 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,017 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,017 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,017 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,017 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,070 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,071 INFO scheduler.DAGScheduler: Got job 48 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,071 INFO scheduler.DAGScheduler: Final stage: ResultStage 71 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,071 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 70)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,071 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,072 INFO scheduler.DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[282] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,073 INFO memory.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 67.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,075 INFO memory.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,075 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 10.0.119.4:35659 (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,076 INFO spark.SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,076 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[282] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,076 INFO cluster.YarnScheduler: Adding task set 71.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,077 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 71.0 (TID 55) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,091 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on algo-1:39795 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,095 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,100 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 71.0 (TID 55) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,100 INFO cluster.YarnScheduler: Removed TaskSet 71.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,100 INFO scheduler.DAGScheduler: ResultStage 71 (collect at AnalysisRunner.scala:326) finished in 0.028 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,100 INFO scheduler.DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,100 INFO cluster.YarnScheduler: Killing all running tasks in stage 71: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,100 INFO scheduler.DAGScheduler: Job 48 finished: collect at AnalysisRunner.scala:326, took 0.029860 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,159 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,161 INFO scheduler.DAGScheduler: Registering RDD 290 (countByKey at ColumnProfiler.scala:592) as input to shuffle 23\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,161 INFO scheduler.DAGScheduler: Got job 49 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,161 INFO scheduler.DAGScheduler: Final stage: ResultStage 73 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,161 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 72)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,161 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 72)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,162 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 72 (MapPartitionsRDD[290] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,173 INFO memory.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 41.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,174 INFO memory.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,174 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,174 INFO spark.SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,174 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 72 (MapPartitionsRDD[290] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,175 INFO cluster.YarnScheduler: Adding task set 72.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,176 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 72.0 (TID 56) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,188 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,217 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 72.0 (TID 56) in 41 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,218 INFO cluster.YarnScheduler: Removed TaskSet 72.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,218 INFO scheduler.DAGScheduler: ShuffleMapStage 72 (countByKey at ColumnProfiler.scala:592) finished in 0.055 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,219 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,219 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,219 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 73)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,219 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,220 INFO scheduler.DAGScheduler: Submitting ResultStage 73 (ShuffledRDD[291] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,221 INFO memory.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,222 INFO memory.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,222 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,223 INFO spark.SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,223 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (ShuffledRDD[291] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,224 INFO cluster.YarnScheduler: Adding task set 73.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,228 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 73.0 (TID 57) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,237 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,240 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,248 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 73.0 (TID 57) in 20 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,249 INFO cluster.YarnScheduler: Removed TaskSet 73.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,249 INFO scheduler.DAGScheduler: ResultStage 73 (countByKey at ColumnProfiler.scala:592) finished in 0.029 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,249 INFO scheduler.DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,249 INFO cluster.YarnScheduler: Killing all running tasks in stage 73: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,249 INFO scheduler.DAGScheduler: Job 49 finished: countByKey at ColumnProfiler.scala:592, took 0.089899 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,329 INFO scheduler.DAGScheduler: Registering RDD 296 (collect at AnalysisRunner.scala:326) as input to shuffle 24\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,329 INFO scheduler.DAGScheduler: Got map stage job 50 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,329 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 74 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,329 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,330 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,330 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 74 (MapPartitionsRDD[296] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,334 INFO memory.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 94.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,335 INFO memory.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,336 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 10.0.119.4:35659 (size: 30.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,336 INFO spark.SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,337 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 74 (MapPartitionsRDD[296] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,337 INFO cluster.YarnScheduler: Adding task set 74.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,338 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 74.0 (TID 58) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,351 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on algo-1:39795 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,453 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 74.0 (TID 58) in 115 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,454 INFO cluster.YarnScheduler: Removed TaskSet 74.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,454 INFO scheduler.DAGScheduler: ShuffleMapStage 74 (collect at AnalysisRunner.scala:326) finished in 0.124 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,454 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,454 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,454 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,454 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,538 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,539 INFO scheduler.DAGScheduler: Got job 51 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,539 INFO scheduler.DAGScheduler: Final stage: ResultStage 76 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,539 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 75)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,540 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,540 INFO scheduler.DAGScheduler: Submitting ResultStage 76 (MapPartitionsRDD[299] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,543 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 10.0.119.4:35659 in memory (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,545 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on algo-1:39795 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,552 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on 10.0.119.4:35659 in memory (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,553 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on algo-1:39795 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,555 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on 10.0.119.4:35659 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,555 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on algo-1:39795 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,557 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on 10.0.119.4:35659 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,558 INFO memory.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 179.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,559 INFO storage.BlockManagerInfo: Removed broadcast_59_piece0 on algo-1:39795 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,560 INFO memory.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 49.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,560 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 10.0.119.4:35659 (size: 49.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,560 INFO spark.SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,560 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 76 (MapPartitionsRDD[299] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,560 INFO cluster.YarnScheduler: Adding task set 76.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,561 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 76.0 (TID 59) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,564 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on 10.0.119.4:35659 in memory (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,572 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on algo-1:39795 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,573 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 10.0.119.4:35659 in memory (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,574 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on algo-1:39795 (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,576 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on algo-1:39795 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,578 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on 10.0.119.4:35659 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,580 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on algo-1:39795 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,581 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 10.0.119.4:35659 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,583 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on algo-1:39795 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,585 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 10.0.119.4:35659 in memory (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,587 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 24 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,588 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on algo-1:39795 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,594 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 10.0.119.4:35659 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,595 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on algo-1:39795 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,609 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on 10.0.119.4:35659 in memory (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,612 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on algo-1:39795 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,613 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on 10.0.119.4:35659 in memory (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,615 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on algo-1:39795 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,617 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on 10.0.119.4:35659 in memory (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,619 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on algo-1:39795 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,688 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 76.0 (TID 59) in 127 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,688 INFO cluster.YarnScheduler: Removed TaskSet 76.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,689 INFO scheduler.DAGScheduler: ResultStage 76 (collect at AnalysisRunner.scala:326) finished in 0.147 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,690 INFO scheduler.DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,690 INFO cluster.YarnScheduler: Killing all running tasks in stage 76: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,691 INFO scheduler.DAGScheduler: Job 51 finished: collect at AnalysisRunner.scala:326, took 0.152211 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,790 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,790 INFO scheduler.DAGScheduler: Got job 52 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,790 INFO scheduler.DAGScheduler: Final stage: ResultStage 77 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,790 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,791 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,791 INFO scheduler.DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[309] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,795 INFO memory.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 48.9 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,796 INFO memory.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,796 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 10.0.119.4:35659 (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,797 INFO spark.SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,797 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[309] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,797 INFO cluster.YarnScheduler: Adding task set 77.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,798 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 77.0 (TID 60) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,807 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on algo-1:39795 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,878 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 77.0 (TID 60) in 80 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,878 INFO cluster.YarnScheduler: Removed TaskSet 77.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,879 INFO scheduler.DAGScheduler: ResultStage 77 (treeReduce at KLLRunner.scala:107) finished in 0.087 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,879 INFO scheduler.DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,879 INFO cluster.YarnScheduler: Killing all running tasks in stage 77: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:49,879 INFO scheduler.DAGScheduler: Job 52 finished: treeReduce at KLLRunner.scala:107, took 0.089210 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,007 INFO scheduler.DAGScheduler: Registering RDD 314 (collect at AnalysisRunner.scala:326) as input to shuffle 25\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,007 INFO scheduler.DAGScheduler: Got map stage job 53 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,007 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 78 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,007 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,008 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,008 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 78 (MapPartitionsRDD[314] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,010 INFO memory.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 87.4 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,011 INFO memory.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,012 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 10.0.119.4:35659 (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,012 INFO spark.SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,012 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 78 (MapPartitionsRDD[314] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,012 INFO cluster.YarnScheduler: Adding task set 78.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,013 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 78.0 (TID 61) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,023 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on algo-1:39795 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,042 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 78.0 (TID 61) in 29 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,044 INFO cluster.YarnScheduler: Removed TaskSet 78.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,045 INFO scheduler.DAGScheduler: ShuffleMapStage 78 (collect at AnalysisRunner.scala:326) finished in 0.037 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,045 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,045 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,045 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,045 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,092 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,093 INFO scheduler.DAGScheduler: Got job 54 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,093 INFO scheduler.DAGScheduler: Final stage: ResultStage 80 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,093 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 79)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,093 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,093 INFO scheduler.DAGScheduler: Submitting ResultStage 80 (MapPartitionsRDD[317] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,096 INFO memory.MemoryStore: Block broadcast_64 stored as values in memory (estimated size 67.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,098 INFO memory.MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,099 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 10.0.119.4:35659 (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,099 INFO spark.SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,099 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 80 (MapPartitionsRDD[317] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,100 INFO cluster.YarnScheduler: Adding task set 80.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,102 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 80.0 (TID 62) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,115 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on algo-1:39795 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,120 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 25 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,129 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 80.0 (TID 62) in 27 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,129 INFO cluster.YarnScheduler: Removed TaskSet 80.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,130 INFO scheduler.DAGScheduler: ResultStage 80 (collect at AnalysisRunner.scala:326) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,130 INFO scheduler.DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,130 INFO cluster.YarnScheduler: Killing all running tasks in stage 80: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,131 INFO scheduler.DAGScheduler: Job 54 finished: collect at AnalysisRunner.scala:326, took 0.038725 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,180 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,181 INFO scheduler.DAGScheduler: Registering RDD 325 (countByKey at ColumnProfiler.scala:592) as input to shuffle 26\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,181 INFO scheduler.DAGScheduler: Got job 55 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,181 INFO scheduler.DAGScheduler: Final stage: ResultStage 82 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,182 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 81)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,182 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 81)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,183 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 81 (MapPartitionsRDD[325] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,187 INFO memory.MemoryStore: Block broadcast_65 stored as values in memory (estimated size 41.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,188 INFO memory.MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,188 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,189 INFO spark.SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,189 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 81 (MapPartitionsRDD[325] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,190 INFO cluster.YarnScheduler: Adding task set 81.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,191 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 81.0 (TID 63) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,202 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,244 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 81.0 (TID 63) in 53 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,244 INFO cluster.YarnScheduler: Removed TaskSet 81.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,245 INFO scheduler.DAGScheduler: ShuffleMapStage 81 (countByKey at ColumnProfiler.scala:592) finished in 0.062 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,245 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,245 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,245 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 82)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,245 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,246 INFO scheduler.DAGScheduler: Submitting ResultStage 82 (ShuffledRDD[326] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,247 INFO memory.MemoryStore: Block broadcast_66 stored as values in memory (estimated size 5.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,248 INFO memory.MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,248 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,249 INFO spark.SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,250 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 82 (ShuffledRDD[326] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,250 INFO cluster.YarnScheduler: Adding task set 82.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,251 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 82.0 (TID 64) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,261 INFO storage.BlockManagerInfo: Added broadcast_66_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,264 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 26 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,271 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 82.0 (TID 64) in 20 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,271 INFO cluster.YarnScheduler: Removed TaskSet 82.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,272 INFO scheduler.DAGScheduler: ResultStage 82 (countByKey at ColumnProfiler.scala:592) finished in 0.026 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,272 INFO scheduler.DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,273 INFO cluster.YarnScheduler: Killing all running tasks in stage 82: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,273 INFO scheduler.DAGScheduler: Job 55 finished: countByKey at ColumnProfiler.scala:592, took 0.092918 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,449 INFO scheduler.DAGScheduler: Registering RDD 331 (collect at AnalysisRunner.scala:326) as input to shuffle 27\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,450 INFO scheduler.DAGScheduler: Got map stage job 56 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,450 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 83 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,450 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,450 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,450 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 83 (MapPartitionsRDD[331] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,454 INFO memory.MemoryStore: Block broadcast_67 stored as values in memory (estimated size 94.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,457 INFO memory.MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,459 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on 10.0.119.4:35659 (size: 30.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,460 INFO spark.SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,461 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 83 (MapPartitionsRDD[331] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,461 INFO cluster.YarnScheduler: Adding task set 83.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,462 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 83.0 (TID 65) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,470 INFO storage.BlockManagerInfo: Added broadcast_67_piece0 in memory on algo-1:39795 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,617 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 83.0 (TID 65) in 155 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,617 INFO cluster.YarnScheduler: Removed TaskSet 83.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,617 INFO scheduler.DAGScheduler: ShuffleMapStage 83 (collect at AnalysisRunner.scala:326) finished in 0.166 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,617 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,617 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,617 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,617 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,640 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,641 INFO scheduler.DAGScheduler: Got job 57 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,641 INFO scheduler.DAGScheduler: Final stage: ResultStage 85 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,641 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 84)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,641 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,641 INFO scheduler.DAGScheduler: Submitting ResultStage 85 (MapPartitionsRDD[334] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,646 INFO memory.MemoryStore: Block broadcast_68 stored as values in memory (estimated size 179.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,647 INFO memory.MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,648 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on 10.0.119.4:35659 (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,648 INFO spark.SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,649 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 85 (MapPartitionsRDD[334] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,649 INFO cluster.YarnScheduler: Adding task set 85.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,649 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 85.0 (TID 66) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,655 INFO storage.BlockManagerInfo: Added broadcast_68_piece0 in memory on algo-1:39795 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,663 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 27 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,735 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 85.0 (TID 66) in 86 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,735 INFO cluster.YarnScheduler: Removed TaskSet 85.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,736 INFO scheduler.DAGScheduler: ResultStage 85 (collect at AnalysisRunner.scala:326) finished in 0.094 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,736 INFO scheduler.DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,736 INFO cluster.YarnScheduler: Killing all running tasks in stage 85: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,737 INFO scheduler.DAGScheduler: Job 57 finished: collect at AnalysisRunner.scala:326, took 0.096451 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,873 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,875 INFO scheduler.DAGScheduler: Got job 58 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,875 INFO scheduler.DAGScheduler: Final stage: ResultStage 86 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,876 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,876 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,877 INFO scheduler.DAGScheduler: Submitting ResultStage 86 (MapPartitionsRDD[344] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,883 INFO memory.MemoryStore: Block broadcast_69 stored as values in memory (estimated size 48.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,884 INFO memory.MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,884 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on 10.0.119.4:35659 (size: 19.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,885 INFO spark.SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,885 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 86 (MapPartitionsRDD[344] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,888 INFO cluster.YarnScheduler: Adding task set 86.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,889 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 86.0 (TID 67) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,898 INFO storage.BlockManagerInfo: Added broadcast_69_piece0 in memory on algo-1:39795 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,947 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 86.0 (TID 67) in 59 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,947 INFO cluster.YarnScheduler: Removed TaskSet 86.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,948 INFO scheduler.DAGScheduler: ResultStage 86 (treeReduce at KLLRunner.scala:107) finished in 0.070 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,954 INFO scheduler.DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,955 INFO cluster.YarnScheduler: Killing all running tasks in stage 86: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:50,955 INFO scheduler.DAGScheduler: Job 58 finished: treeReduce at KLLRunner.scala:107, took 0.081055 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,081 INFO scheduler.DAGScheduler: Registering RDD 349 (collect at AnalysisRunner.scala:326) as input to shuffle 28\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,081 INFO scheduler.DAGScheduler: Got map stage job 59 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,081 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 87 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,081 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,081 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,082 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 87 (MapPartitionsRDD[349] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,084 INFO memory.MemoryStore: Block broadcast_70 stored as values in memory (estimated size 87.4 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,085 INFO memory.MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,086 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on 10.0.119.4:35659 (size: 27.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,086 INFO spark.SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,086 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 87 (MapPartitionsRDD[349] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,086 INFO cluster.YarnScheduler: Adding task set 87.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,087 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 87.0 (TID 68) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,096 INFO storage.BlockManagerInfo: Added broadcast_70_piece0 in memory on algo-1:39795 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,113 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 87.0 (TID 68) in 26 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,113 INFO cluster.YarnScheduler: Removed TaskSet 87.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,114 INFO scheduler.DAGScheduler: ShuffleMapStage 87 (collect at AnalysisRunner.scala:326) finished in 0.031 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,114 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,114 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,114 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,114 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,181 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,182 INFO scheduler.DAGScheduler: Got job 60 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,182 INFO scheduler.DAGScheduler: Final stage: ResultStage 89 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,182 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 88)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,182 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,183 INFO scheduler.DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[352] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,185 INFO memory.MemoryStore: Block broadcast_71 stored as values in memory (estimated size 67.7 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,187 INFO memory.MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,187 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on 10.0.119.4:35659 (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,188 INFO spark.SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,188 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[352] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,188 INFO cluster.YarnScheduler: Adding task set 89.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,190 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 89.0 (TID 69) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,197 INFO storage.BlockManagerInfo: Added broadcast_71_piece0 in memory on algo-1:39795 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,202 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 28 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,206 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 89.0 (TID 69) in 17 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,206 INFO cluster.YarnScheduler: Removed TaskSet 89.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,207 INFO scheduler.DAGScheduler: ResultStage 89 (collect at AnalysisRunner.scala:326) finished in 0.023 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,208 INFO scheduler.DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,208 INFO cluster.YarnScheduler: Killing all running tasks in stage 89: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,209 INFO scheduler.DAGScheduler: Job 60 finished: collect at AnalysisRunner.scala:326, took 0.027428 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,244 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,245 INFO scheduler.DAGScheduler: Registering RDD 360 (countByKey at ColumnProfiler.scala:592) as input to shuffle 29\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,245 INFO scheduler.DAGScheduler: Got job 61 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,245 INFO scheduler.DAGScheduler: Final stage: ResultStage 91 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,245 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 90)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,245 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 90)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,246 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 90 (MapPartitionsRDD[360] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,253 INFO memory.MemoryStore: Block broadcast_72 stored as values in memory (estimated size 41.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,254 INFO memory.MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,255 INFO storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,255 INFO spark.SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,256 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 90 (MapPartitionsRDD[360] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,256 INFO cluster.YarnScheduler: Adding task set 90.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,257 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 90.0 (TID 70) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,265 INFO storage.BlockManagerInfo: Added broadcast_72_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,281 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 90.0 (TID 70) in 24 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,281 INFO cluster.YarnScheduler: Removed TaskSet 90.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,282 INFO scheduler.DAGScheduler: ShuffleMapStage 90 (countByKey at ColumnProfiler.scala:592) finished in 0.035 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,282 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,282 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,282 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 91)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,282 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,282 INFO scheduler.DAGScheduler: Submitting ResultStage 91 (ShuffledRDD[361] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,283 INFO memory.MemoryStore: Block broadcast_73 stored as values in memory (estimated size 5.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,284 INFO memory.MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,284 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,285 INFO spark.SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,285 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (ShuffledRDD[361] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,285 INFO cluster.YarnScheduler: Adding task set 91.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,286 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 91.0 (TID 71) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,292 INFO storage.BlockManagerInfo: Added broadcast_73_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,294 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 29 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,300 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 91.0 (TID 71) in 14 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,300 INFO cluster.YarnScheduler: Removed TaskSet 91.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,301 INFO scheduler.DAGScheduler: ResultStage 91 (countByKey at ColumnProfiler.scala:592) finished in 0.018 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,301 INFO scheduler.DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,301 INFO cluster.YarnScheduler: Killing all running tasks in stage 91: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,301 INFO scheduler.DAGScheduler: Job 61 finished: countByKey at ColumnProfiler.scala:592, took 0.057099 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,366 INFO scheduler.DAGScheduler: Registering RDD 366 (collect at AnalysisRunner.scala:326) as input to shuffle 30\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,366 INFO scheduler.DAGScheduler: Got map stage job 62 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,366 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 92 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,366 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,368 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,368 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 92 (MapPartitionsRDD[366] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,371 INFO memory.MemoryStore: Block broadcast_74 stored as values in memory (estimated size 94.9 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,373 INFO memory.MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,374 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on 10.0.119.4:35659 (size: 30.1 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,374 INFO spark.SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,374 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 92 (MapPartitionsRDD[366] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,374 INFO cluster.YarnScheduler: Adding task set 92.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,376 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 92.0 (TID 72) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,383 INFO storage.BlockManagerInfo: Added broadcast_74_piece0 in memory on algo-1:39795 (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,448 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 92.0 (TID 72) in 72 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,448 INFO cluster.YarnScheduler: Removed TaskSet 92.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,449 INFO scheduler.DAGScheduler: ShuffleMapStage 92 (collect at AnalysisRunner.scala:326) finished in 0.080 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,449 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,449 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,449 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,449 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,474 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,474 INFO scheduler.DAGScheduler: Got job 63 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,474 INFO scheduler.DAGScheduler: Final stage: ResultStage 94 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,474 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 93)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,475 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,475 INFO scheduler.DAGScheduler: Submitting ResultStage 94 (MapPartitionsRDD[369] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,481 INFO memory.MemoryStore: Block broadcast_75 stored as values in memory (estimated size 179.7 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,484 INFO memory.MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,484 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on 10.0.119.4:35659 (size: 49.2 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,485 INFO spark.SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,485 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 94 (MapPartitionsRDD[369] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,485 INFO cluster.YarnScheduler: Adding task set 94.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,486 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 94.0 (TID 73) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,491 INFO storage.BlockManagerInfo: Added broadcast_75_piece0 in memory on algo-1:39795 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,500 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 30 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,572 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 94.0 (TID 73) in 86 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,573 INFO cluster.YarnScheduler: Removed TaskSet 94.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,573 INFO scheduler.DAGScheduler: ResultStage 94 (collect at AnalysisRunner.scala:326) finished in 0.098 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,574 INFO scheduler.DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,574 INFO cluster.YarnScheduler: Killing all running tasks in stage 94: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,574 INFO scheduler.DAGScheduler: Job 63 finished: collect at AnalysisRunner.scala:326, took 0.100294 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,688 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,689 INFO scheduler.DAGScheduler: Got job 64 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,689 INFO scheduler.DAGScheduler: Final stage: ResultStage 95 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,689 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,691 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on 10.0.119.4:35659 in memory (size: 19.9 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,691 INFO storage.BlockManagerInfo: Removed broadcast_71_piece0 on algo-1:39795 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,691 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,692 INFO scheduler.DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[379] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,697 INFO memory.MemoryStore: Block broadcast_76 stored as values in memory (estimated size 48.9 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,698 INFO storage.BlockManagerInfo: Removed broadcast_75_piece0 on 10.0.119.4:35659 in memory (size: 49.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,699 INFO storage.BlockManagerInfo: Removed broadcast_75_piece0 on algo-1:39795 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,700 INFO memory.MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,700 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on 10.0.119.4:35659 (size: 19.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,701 INFO spark.SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,701 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 95 (MapPartitionsRDD[379] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,701 INFO cluster.YarnScheduler: Adding task set 95.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,702 INFO storage.BlockManagerInfo: Removed broadcast_72_piece0 on 10.0.119.4:35659 in memory (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,703 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 95.0 (TID 74) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,704 INFO storage.BlockManagerInfo: Removed broadcast_72_piece0 on algo-1:39795 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,708 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on 10.0.119.4:35659 in memory (size: 19.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,708 INFO storage.BlockManagerInfo: Removed broadcast_69_piece0 on algo-1:39795 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,709 INFO storage.BlockManagerInfo: Added broadcast_76_piece0 in memory on algo-1:39795 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,710 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on 10.0.119.4:35659 in memory (size: 49.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,717 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on algo-1:39795 in memory (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,719 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on 10.0.119.4:35659 in memory (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,721 INFO storage.BlockManagerInfo: Removed broadcast_68_piece0 on algo-1:39795 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,722 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on 10.0.119.4:35659 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,723 INFO storage.BlockManagerInfo: Removed broadcast_64_piece0 on algo-1:39795 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,727 INFO storage.BlockManagerInfo: Removed broadcast_73_piece0 on 10.0.119.4:35659 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,730 INFO storage.BlockManagerInfo: Removed broadcast_73_piece0 on algo-1:39795 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,734 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on 10.0.119.4:35659 in memory (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,742 INFO storage.BlockManagerInfo: Removed broadcast_70_piece0 on algo-1:39795 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,743 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on 10.0.119.4:35659 in memory (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,745 INFO storage.BlockManagerInfo: Removed broadcast_63_piece0 on algo-1:39795 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,746 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 95.0 (TID 74) in 43 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,747 INFO cluster.YarnScheduler: Removed TaskSet 95.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,747 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on 10.0.119.4:35659 in memory (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,747 INFO scheduler.DAGScheduler: ResultStage 95 (treeReduce at KLLRunner.scala:107) finished in 0.055 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,748 INFO scheduler.DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,748 INFO cluster.YarnScheduler: Killing all running tasks in stage 95: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,748 INFO scheduler.DAGScheduler: Job 64 finished: treeReduce at KLLRunner.scala:107, took 0.059924 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,754 INFO storage.BlockManagerInfo: Removed broadcast_65_piece0 on algo-1:39795 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,759 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on 10.0.119.4:35659 in memory (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,762 INFO storage.BlockManagerInfo: Removed broadcast_74_piece0 on algo-1:39795 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,764 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on 10.0.119.4:35659 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,764 INFO storage.BlockManagerInfo: Removed broadcast_62_piece0 on algo-1:39795 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,767 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on 10.0.119.4:35659 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,769 INFO storage.BlockManagerInfo: Removed broadcast_66_piece0 on algo-1:39795 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,771 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on 10.0.119.4:35659 in memory (size: 30.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,772 INFO storage.BlockManagerInfo: Removed broadcast_67_piece0 on algo-1:39795 in memory (size: 30.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,850 INFO scheduler.DAGScheduler: Registering RDD 384 (collect at AnalysisRunner.scala:326) as input to shuffle 31\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,850 INFO scheduler.DAGScheduler: Got map stage job 65 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,850 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 96 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,850 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,851 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,851 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 96 (MapPartitionsRDD[384] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,854 INFO memory.MemoryStore: Block broadcast_77 stored as values in memory (estimated size 87.4 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,855 INFO memory.MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,856 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on 10.0.119.4:35659 (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,856 INFO spark.SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,857 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 96 (MapPartitionsRDD[384] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,857 INFO cluster.YarnScheduler: Adding task set 96.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,858 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 96.0 (TID 75) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,868 INFO storage.BlockManagerInfo: Added broadcast_77_piece0 in memory on algo-1:39795 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,883 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 96.0 (TID 75) in 25 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,883 INFO cluster.YarnScheduler: Removed TaskSet 96.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,884 INFO scheduler.DAGScheduler: ShuffleMapStage 96 (collect at AnalysisRunner.scala:326) finished in 0.032 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,884 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,884 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,884 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,884 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,924 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,925 INFO scheduler.DAGScheduler: Got job 66 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,926 INFO scheduler.DAGScheduler: Final stage: ResultStage 98 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,926 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 97)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,926 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,927 INFO scheduler.DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[387] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,928 INFO memory.MemoryStore: Block broadcast_78 stored as values in memory (estimated size 67.7 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,930 INFO memory.MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,930 INFO storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on 10.0.119.4:35659 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,931 INFO spark.SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,931 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 98 (MapPartitionsRDD[387] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,931 INFO cluster.YarnScheduler: Adding task set 98.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,932 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 98.0 (TID 76) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,938 INFO storage.BlockManagerInfo: Added broadcast_78_piece0 in memory on algo-1:39795 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,943 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 31 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,947 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 98.0 (TID 76) in 15 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,947 INFO cluster.YarnScheduler: Removed TaskSet 98.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,947 INFO scheduler.DAGScheduler: ResultStage 98 (collect at AnalysisRunner.scala:326) finished in 0.020 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,947 INFO scheduler.DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,947 INFO cluster.YarnScheduler: Killing all running tasks in stage 98: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,947 INFO scheduler.DAGScheduler: Job 66 finished: collect at AnalysisRunner.scala:326, took 0.023117 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,987 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,988 INFO scheduler.DAGScheduler: Registering RDD 395 (countByKey at ColumnProfiler.scala:592) as input to shuffle 32\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,988 INFO scheduler.DAGScheduler: Got job 67 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,988 INFO scheduler.DAGScheduler: Final stage: ResultStage 100 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,989 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 99)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,989 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 99)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,989 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 99 (MapPartitionsRDD[395] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,993 INFO memory.MemoryStore: Block broadcast_79 stored as values in memory (estimated size 41.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,994 INFO memory.MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,994 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,994 INFO spark.SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,994 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 99 (MapPartitionsRDD[395] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,994 INFO cluster.YarnScheduler: Adding task set 99.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:51,995 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 99.0 (TID 77) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,002 INFO storage.BlockManagerInfo: Added broadcast_79_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,026 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 99.0 (TID 77) in 31 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,026 INFO cluster.YarnScheduler: Removed TaskSet 99.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,027 INFO scheduler.DAGScheduler: ShuffleMapStage 99 (countByKey at ColumnProfiler.scala:592) finished in 0.037 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,027 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,027 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,027 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 100)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,027 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,027 INFO scheduler.DAGScheduler: Submitting ResultStage 100 (ShuffledRDD[396] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,028 INFO memory.MemoryStore: Block broadcast_80 stored as values in memory (estimated size 5.1 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,029 INFO memory.MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,030 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,030 INFO spark.SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,030 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 100 (ShuffledRDD[396] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,030 INFO cluster.YarnScheduler: Adding task set 100.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,031 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 100.0 (TID 78) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,036 INFO storage.BlockManagerInfo: Added broadcast_80_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,039 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 32 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,051 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 100.0 (TID 78) in 20 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,051 INFO cluster.YarnScheduler: Removed TaskSet 100.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,051 INFO scheduler.DAGScheduler: ResultStage 100 (countByKey at ColumnProfiler.scala:592) finished in 0.023 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,052 INFO scheduler.DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,052 INFO cluster.YarnScheduler: Killing all running tasks in stage 100: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,052 INFO scheduler.DAGScheduler: Job 67 finished: countByKey at ColumnProfiler.scala:592, took 0.064407 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,174 INFO scheduler.DAGScheduler: Registering RDD 401 (collect at AnalysisRunner.scala:326) as input to shuffle 33\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,174 INFO scheduler.DAGScheduler: Got map stage job 68 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,174 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 101 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,174 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,175 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,175 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 101 (MapPartitionsRDD[401] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,179 INFO memory.MemoryStore: Block broadcast_81 stored as values in memory (estimated size 94.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,182 INFO memory.MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,183 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on 10.0.119.4:35659 (size: 30.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,183 INFO spark.SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,183 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 101 (MapPartitionsRDD[401] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,183 INFO cluster.YarnScheduler: Adding task set 101.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,185 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 101.0 (TID 79) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,191 INFO storage.BlockManagerInfo: Added broadcast_81_piece0 in memory on algo-1:39795 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,294 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 101.0 (TID 79) in 109 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,294 INFO cluster.YarnScheduler: Removed TaskSet 101.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,294 INFO scheduler.DAGScheduler: ShuffleMapStage 101 (collect at AnalysisRunner.scala:326) finished in 0.118 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,294 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,294 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,294 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,294 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,317 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,317 INFO scheduler.DAGScheduler: Got job 69 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,317 INFO scheduler.DAGScheduler: Final stage: ResultStage 103 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,317 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 102)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,317 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,318 INFO scheduler.DAGScheduler: Submitting ResultStage 103 (MapPartitionsRDD[404] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,323 INFO memory.MemoryStore: Block broadcast_82 stored as values in memory (estimated size 179.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,325 INFO memory.MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 49.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,325 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on 10.0.119.4:35659 (size: 49.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,325 INFO spark.SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,326 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 103 (MapPartitionsRDD[404] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,326 INFO cluster.YarnScheduler: Adding task set 103.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,327 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 103.0 (TID 80) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,333 INFO storage.BlockManagerInfo: Added broadcast_82_piece0 in memory on algo-1:39795 (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,341 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 33 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,409 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 103.0 (TID 80) in 83 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,409 INFO cluster.YarnScheduler: Removed TaskSet 103.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,409 INFO scheduler.DAGScheduler: ResultStage 103 (collect at AnalysisRunner.scala:326) finished in 0.091 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,410 INFO scheduler.DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,410 INFO cluster.YarnScheduler: Killing all running tasks in stage 103: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,410 INFO scheduler.DAGScheduler: Job 69 finished: collect at AnalysisRunner.scala:326, took 0.093432 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,494 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,495 INFO scheduler.DAGScheduler: Got job 70 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,495 INFO scheduler.DAGScheduler: Final stage: ResultStage 104 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,495 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,495 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,495 INFO scheduler.DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[414] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,500 INFO memory.MemoryStore: Block broadcast_83 stored as values in memory (estimated size 48.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,501 INFO memory.MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,501 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on 10.0.119.4:35659 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,501 INFO spark.SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,501 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 104 (MapPartitionsRDD[414] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,502 INFO cluster.YarnScheduler: Adding task set 104.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,503 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 104.0 (TID 81) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,511 INFO storage.BlockManagerInfo: Added broadcast_83_piece0 in memory on algo-1:39795 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,558 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 104.0 (TID 81) in 55 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,558 INFO cluster.YarnScheduler: Removed TaskSet 104.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,559 INFO scheduler.DAGScheduler: ResultStage 104 (treeReduce at KLLRunner.scala:107) finished in 0.062 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,559 INFO scheduler.DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,559 INFO cluster.YarnScheduler: Killing all running tasks in stage 104: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,560 INFO scheduler.DAGScheduler: Job 70 finished: treeReduce at KLLRunner.scala:107, took 0.065600 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,684 INFO scheduler.DAGScheduler: Registering RDD 419 (collect at AnalysisRunner.scala:326) as input to shuffle 34\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,684 INFO scheduler.DAGScheduler: Got map stage job 71 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,684 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 105 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,684 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,685 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,685 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 105 (MapPartitionsRDD[419] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,687 INFO memory.MemoryStore: Block broadcast_84 stored as values in memory (estimated size 87.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,688 INFO memory.MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,688 INFO storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on 10.0.119.4:35659 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,689 INFO spark.SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,689 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 105 (MapPartitionsRDD[419] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,689 INFO cluster.YarnScheduler: Adding task set 105.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,690 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 105.0 (TID 82) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,697 INFO storage.BlockManagerInfo: Added broadcast_84_piece0 in memory on algo-1:39795 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,707 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 105.0 (TID 82) in 17 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,708 INFO cluster.YarnScheduler: Removed TaskSet 105.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,708 INFO scheduler.DAGScheduler: ShuffleMapStage 105 (collect at AnalysisRunner.scala:326) finished in 0.023 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,708 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,708 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,708 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,708 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,746 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,747 INFO scheduler.DAGScheduler: Got job 72 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,747 INFO scheduler.DAGScheduler: Final stage: ResultStage 107 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,747 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 106)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,747 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,748 INFO scheduler.DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[422] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,749 INFO memory.MemoryStore: Block broadcast_85 stored as values in memory (estimated size 67.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,751 INFO memory.MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,751 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on 10.0.119.4:35659 (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,751 INFO spark.SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,752 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 107 (MapPartitionsRDD[422] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,752 INFO cluster.YarnScheduler: Adding task set 107.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,753 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 107.0 (TID 83) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,762 INFO storage.BlockManagerInfo: Added broadcast_85_piece0 in memory on algo-1:39795 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,766 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,770 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 107.0 (TID 83) in 17 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,770 INFO cluster.YarnScheduler: Removed TaskSet 107.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,771 INFO scheduler.DAGScheduler: ResultStage 107 (collect at AnalysisRunner.scala:326) finished in 0.023 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,772 INFO scheduler.DAGScheduler: Job 72 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,772 INFO cluster.YarnScheduler: Killing all running tasks in stage 107: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,772 INFO scheduler.DAGScheduler: Job 72 finished: collect at AnalysisRunner.scala:326, took 0.025833 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,810 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,810 INFO scheduler.DAGScheduler: Registering RDD 430 (countByKey at ColumnProfiler.scala:592) as input to shuffle 35\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,810 INFO scheduler.DAGScheduler: Got job 73 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,810 INFO scheduler.DAGScheduler: Final stage: ResultStage 109 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,810 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 108)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,810 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 108)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,811 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 108 (MapPartitionsRDD[430] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,814 INFO memory.MemoryStore: Block broadcast_86 stored as values in memory (estimated size 41.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,815 INFO memory.MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,815 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,816 INFO spark.SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,816 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 108 (MapPartitionsRDD[430] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,816 INFO cluster.YarnScheduler: Adding task set 108.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,819 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 108.0 (TID 84) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,827 INFO storage.BlockManagerInfo: Added broadcast_86_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,851 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 108.0 (TID 84) in 32 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,851 INFO cluster.YarnScheduler: Removed TaskSet 108.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,852 INFO scheduler.DAGScheduler: ShuffleMapStage 108 (countByKey at ColumnProfiler.scala:592) finished in 0.039 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,852 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,852 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,852 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 109)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,852 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,852 INFO scheduler.DAGScheduler: Submitting ResultStage 109 (ShuffledRDD[431] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,853 INFO memory.MemoryStore: Block broadcast_87 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,855 INFO memory.MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,858 INFO storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,860 INFO spark.SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,860 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 109 (ShuffledRDD[431] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,860 INFO cluster.YarnScheduler: Adding task set 109.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,861 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 109.0 (TID 85) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,873 INFO storage.BlockManagerInfo: Added broadcast_87_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,875 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 35 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,885 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 109.0 (TID 85) in 24 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,885 INFO cluster.YarnScheduler: Removed TaskSet 109.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,886 INFO scheduler.DAGScheduler: ResultStage 109 (countByKey at ColumnProfiler.scala:592) finished in 0.032 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,886 INFO scheduler.DAGScheduler: Job 73 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,886 INFO cluster.YarnScheduler: Killing all running tasks in stage 109: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,887 INFO scheduler.DAGScheduler: Job 73 finished: countByKey at ColumnProfiler.scala:592, took 0.076966 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,950 INFO scheduler.DAGScheduler: Registering RDD 436 (collect at AnalysisRunner.scala:326) as input to shuffle 36\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,950 INFO scheduler.DAGScheduler: Got map stage job 74 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,950 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 110 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,950 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,950 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,950 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 110 (MapPartitionsRDD[436] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,954 INFO memory.MemoryStore: Block broadcast_88 stored as values in memory (estimated size 94.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,955 INFO memory.MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 30.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,955 INFO storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on 10.0.119.4:35659 (size: 30.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,956 INFO spark.SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,956 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 110 (MapPartitionsRDD[436] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,956 INFO cluster.YarnScheduler: Adding task set 110.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,957 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 110.0 (TID 86) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:52,970 INFO storage.BlockManagerInfo: Added broadcast_88_piece0 in memory on algo-1:39795 (size: 30.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,030 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 110.0 (TID 86) in 72 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,030 INFO cluster.YarnScheduler: Removed TaskSet 110.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,031 INFO scheduler.DAGScheduler: ShuffleMapStage 110 (collect at AnalysisRunner.scala:326) finished in 0.079 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,031 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,031 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,031 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,031 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,056 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,056 INFO scheduler.DAGScheduler: Got job 75 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,056 INFO scheduler.DAGScheduler: Final stage: ResultStage 112 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,056 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 111)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,057 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,057 INFO scheduler.DAGScheduler: Submitting ResultStage 112 (MapPartitionsRDD[439] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,061 INFO memory.MemoryStore: Block broadcast_89 stored as values in memory (estimated size 179.8 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,062 INFO memory.MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 49.2 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,063 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on 10.0.119.4:35659 (size: 49.2 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,063 INFO spark.SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,064 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 112 (MapPartitionsRDD[439] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,064 INFO cluster.YarnScheduler: Adding task set 112.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,065 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 112.0 (TID 87) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,071 INFO storage.BlockManagerInfo: Added broadcast_89_piece0 in memory on algo-1:39795 (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,079 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,135 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 112.0 (TID 87) in 70 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,135 INFO cluster.YarnScheduler: Removed TaskSet 112.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,136 INFO scheduler.DAGScheduler: ResultStage 112 (collect at AnalysisRunner.scala:326) finished in 0.079 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,136 INFO scheduler.DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,136 INFO cluster.YarnScheduler: Killing all running tasks in stage 112: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,136 INFO scheduler.DAGScheduler: Job 75 finished: collect at AnalysisRunner.scala:326, took 0.080181 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,195 INFO codegen.CodeGenerator: Code generated in 8.381442 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,217 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,218 INFO scheduler.DAGScheduler: Got job 76 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,218 INFO scheduler.DAGScheduler: Final stage: ResultStage 113 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,218 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,218 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,218 INFO scheduler.DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[449] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,222 INFO memory.MemoryStore: Block broadcast_90 stored as values in memory (estimated size 48.9 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,223 INFO memory.MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,224 INFO storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on 10.0.119.4:35659 (size: 19.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,224 INFO spark.SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,225 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 113 (MapPartitionsRDD[449] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,225 INFO cluster.YarnScheduler: Adding task set 113.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,226 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 113.0 (TID 88) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,234 INFO storage.BlockManagerInfo: Added broadcast_90_piece0 in memory on algo-1:39795 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,279 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 113.0 (TID 88) in 53 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,279 INFO cluster.YarnScheduler: Removed TaskSet 113.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,280 INFO scheduler.DAGScheduler: ResultStage 113 (treeReduce at KLLRunner.scala:107) finished in 0.061 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,281 INFO scheduler.DAGScheduler: Job 76 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,281 INFO cluster.YarnScheduler: Killing all running tasks in stage 113: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,282 INFO scheduler.DAGScheduler: Job 76 finished: treeReduce at KLLRunner.scala:107, took 0.064493 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,407 INFO storage.BlockManagerInfo: Removed broadcast_90_piece0 on 10.0.119.4:35659 in memory (size: 19.4 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,414 INFO storage.BlockManagerInfo: Removed broadcast_90_piece0 on algo-1:39795 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,415 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on 10.0.119.4:35659 in memory (size: 49.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,417 INFO storage.BlockManagerInfo: Removed broadcast_82_piece0 on algo-1:39795 in memory (size: 49.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,426 INFO storage.BlockManagerInfo: Removed broadcast_81_piece0 on 10.0.119.4:35659 in memory (size: 30.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,428 INFO storage.BlockManagerInfo: Removed broadcast_81_piece0 on algo-1:39795 in memory (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,430 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on 10.0.119.4:35659 in memory (size: 27.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,431 INFO storage.BlockManagerInfo: Removed broadcast_77_piece0 on algo-1:39795 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,434 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on 10.0.119.4:35659 in memory (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,436 INFO storage.BlockManagerInfo: Removed broadcast_76_piece0 on algo-1:39795 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,447 INFO storage.BlockManagerInfo: Removed broadcast_85_piece0 on 10.0.119.4:35659 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,448 INFO storage.BlockManagerInfo: Removed broadcast_85_piece0 on algo-1:39795 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,449 INFO codegen.CodeGenerator: Code generated in 32.839889 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,450 INFO storage.BlockManagerInfo: Removed broadcast_89_piece0 on 10.0.119.4:35659 in memory (size: 49.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,451 INFO storage.BlockManagerInfo: Removed broadcast_89_piece0 on algo-1:39795 in memory (size: 49.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,453 INFO storage.BlockManagerInfo: Removed broadcast_86_piece0 on 10.0.119.4:35659 in memory (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,453 INFO storage.BlockManagerInfo: Removed broadcast_86_piece0 on algo-1:39795 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,456 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on 10.0.119.4:35659 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,456 INFO storage.BlockManagerInfo: Removed broadcast_83_piece0 on algo-1:39795 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,458 INFO storage.BlockManagerInfo: Removed broadcast_87_piece0 on 10.0.119.4:35659 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,459 INFO storage.BlockManagerInfo: Removed broadcast_87_piece0 on algo-1:39795 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,461 INFO storage.BlockManagerInfo: Removed broadcast_88_piece0 on 10.0.119.4:35659 in memory (size: 30.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,462 INFO storage.BlockManagerInfo: Removed broadcast_88_piece0 on algo-1:39795 in memory (size: 30.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,464 INFO scheduler.DAGScheduler: Registering RDD 454 (collect at AnalysisRunner.scala:326) as input to shuffle 37\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,464 INFO scheduler.DAGScheduler: Got map stage job 77 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,464 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 114 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,464 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,464 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,465 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 114 (MapPartitionsRDD[454] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,467 INFO memory.MemoryStore: Block broadcast_91 stored as values in memory (estimated size 87.4 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,468 INFO memory.MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,470 INFO storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on 10.0.119.4:35659 (size: 27.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,472 INFO storage.BlockManagerInfo: Removed broadcast_79_piece0 on 10.0.119.4:35659 in memory (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,473 INFO spark.SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,473 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 114 (MapPartitionsRDD[454] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,474 INFO cluster.YarnScheduler: Adding task set 114.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,474 INFO storage.BlockManagerInfo: Removed broadcast_79_piece0 on algo-1:39795 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,475 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 114.0 (TID 89) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,478 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on 10.0.119.4:35659 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,483 INFO storage.BlockManagerInfo: Removed broadcast_80_piece0 on algo-1:39795 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,484 INFO storage.BlockManagerInfo: Added broadcast_91_piece0 in memory on algo-1:39795 (size: 27.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,487 INFO storage.BlockManagerInfo: Removed broadcast_78_piece0 on 10.0.119.4:35659 in memory (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,489 INFO storage.BlockManagerInfo: Removed broadcast_78_piece0 on algo-1:39795 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,497 INFO storage.BlockManagerInfo: Removed broadcast_84_piece0 on algo-1:39795 in memory (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,497 INFO storage.BlockManagerInfo: Removed broadcast_84_piece0 on 10.0.119.4:35659 in memory (size: 27.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,607 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 114.0 (TID 89) in 133 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,607 INFO cluster.YarnScheduler: Removed TaskSet 114.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,608 INFO scheduler.DAGScheduler: ShuffleMapStage 114 (collect at AnalysisRunner.scala:326) finished in 0.143 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,608 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,608 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,608 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,608 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,645 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,646 INFO scheduler.DAGScheduler: Got job 78 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,646 INFO scheduler.DAGScheduler: Final stage: ResultStage 116 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,646 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 115)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,646 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,646 INFO scheduler.DAGScheduler: Submitting ResultStage 116 (MapPartitionsRDD[457] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,648 INFO memory.MemoryStore: Block broadcast_92 stored as values in memory (estimated size 67.7 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,649 INFO memory.MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,649 INFO storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on 10.0.119.4:35659 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,649 INFO spark.SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,649 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 116 (MapPartitionsRDD[457] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,650 INFO cluster.YarnScheduler: Adding task set 116.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,650 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 116.0 (TID 90) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,656 INFO storage.BlockManagerInfo: Added broadcast_92_piece0 in memory on algo-1:39795 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,658 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,662 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 116.0 (TID 90) in 12 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,662 INFO cluster.YarnScheduler: Removed TaskSet 116.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,663 INFO scheduler.DAGScheduler: ResultStage 116 (collect at AnalysisRunner.scala:326) finished in 0.016 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,663 INFO scheduler.DAGScheduler: Job 78 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,663 INFO cluster.YarnScheduler: Killing all running tasks in stage 116: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,664 INFO scheduler.DAGScheduler: Job 78 finished: collect at AnalysisRunner.scala:326, took 0.018280 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,698 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,699 INFO scheduler.DAGScheduler: Registering RDD 465 (countByKey at ColumnProfiler.scala:592) as input to shuffle 38\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,699 INFO scheduler.DAGScheduler: Got job 79 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,699 INFO scheduler.DAGScheduler: Final stage: ResultStage 118 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,699 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 117)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,699 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 117)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,700 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 117 (MapPartitionsRDD[465] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,707 INFO memory.MemoryStore: Block broadcast_93 stored as values in memory (estimated size 41.9 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,708 INFO memory.MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,708 INFO storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,708 INFO spark.SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,708 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 117 (MapPartitionsRDD[465] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,708 INFO cluster.YarnScheduler: Adding task set 117.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,709 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 117.0 (TID 91) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,718 INFO storage.BlockManagerInfo: Added broadcast_93_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,743 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 117.0 (TID 91) in 34 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,743 INFO cluster.YarnScheduler: Removed TaskSet 117.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,743 INFO scheduler.DAGScheduler: ShuffleMapStage 117 (countByKey at ColumnProfiler.scala:592) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,743 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,744 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,744 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 118)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,744 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,744 INFO scheduler.DAGScheduler: Submitting ResultStage 118 (ShuffledRDD[466] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,745 INFO memory.MemoryStore: Block broadcast_94 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,746 INFO memory.MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,746 INFO storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,747 INFO spark.SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,747 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 118 (ShuffledRDD[466] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,747 INFO cluster.YarnScheduler: Adding task set 118.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,748 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 118.0 (TID 92) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,756 INFO storage.BlockManagerInfo: Added broadcast_94_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,758 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 38 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,768 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 118.0 (TID 92) in 20 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,768 INFO cluster.YarnScheduler: Removed TaskSet 118.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,769 INFO scheduler.DAGScheduler: ResultStage 118 (countByKey at ColumnProfiler.scala:592) finished in 0.023 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,769 INFO scheduler.DAGScheduler: Job 79 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,769 INFO cluster.YarnScheduler: Killing all running tasks in stage 118: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,769 INFO scheduler.DAGScheduler: Job 79 finished: countByKey at ColumnProfiler.scala:592, took 0.070969 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,882 INFO scheduler.DAGScheduler: Registering RDD 471 (collect at AnalysisRunner.scala:326) as input to shuffle 39\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,882 INFO scheduler.DAGScheduler: Got map stage job 80 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,882 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 119 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,882 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,882 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,882 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 119 (MapPartitionsRDD[471] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,886 INFO memory.MemoryStore: Block broadcast_95 stored as values in memory (estimated size 94.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,887 INFO memory.MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 30.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,888 INFO storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on 10.0.119.4:35659 (size: 30.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,888 INFO spark.SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,889 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 119 (MapPartitionsRDD[471] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,889 INFO cluster.YarnScheduler: Adding task set 119.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,890 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 119.0 (TID 93) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:53,899 INFO storage.BlockManagerInfo: Added broadcast_95_piece0 in memory on algo-1:39795 (size: 30.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,029 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 119.0 (TID 93) in 139 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,030 INFO cluster.YarnScheduler: Removed TaskSet 119.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,030 INFO scheduler.DAGScheduler: ShuffleMapStage 119 (collect at AnalysisRunner.scala:326) finished in 0.147 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,034 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,034 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,034 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,034 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,065 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,065 INFO scheduler.DAGScheduler: Got job 81 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,065 INFO scheduler.DAGScheduler: Final stage: ResultStage 121 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,065 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 120)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,065 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,065 INFO scheduler.DAGScheduler: Submitting ResultStage 121 (MapPartitionsRDD[474] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,069 INFO memory.MemoryStore: Block broadcast_96 stored as values in memory (estimated size 179.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,071 INFO memory.MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 49.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,071 INFO storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on 10.0.119.4:35659 (size: 49.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,071 INFO spark.SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,071 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 121 (MapPartitionsRDD[474] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,071 INFO cluster.YarnScheduler: Adding task set 121.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,072 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 121.0 (TID 94) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,077 INFO storage.BlockManagerInfo: Added broadcast_96_piece0 in memory on algo-1:39795 (size: 49.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,082 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 39 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,128 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 121.0 (TID 94) in 56 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,128 INFO cluster.YarnScheduler: Removed TaskSet 121.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,129 INFO scheduler.DAGScheduler: ResultStage 121 (collect at AnalysisRunner.scala:326) finished in 0.063 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,129 INFO scheduler.DAGScheduler: Job 81 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,129 INFO cluster.YarnScheduler: Killing all running tasks in stage 121: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,130 INFO scheduler.DAGScheduler: Job 81 finished: collect at AnalysisRunner.scala:326, took 0.065062 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,199 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,199 INFO scheduler.DAGScheduler: Got job 82 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,199 INFO scheduler.DAGScheduler: Final stage: ResultStage 122 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,199 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,199 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,200 INFO scheduler.DAGScheduler: Submitting ResultStage 122 (MapPartitionsRDD[484] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,205 INFO memory.MemoryStore: Block broadcast_97 stored as values in memory (estimated size 48.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,206 INFO memory.MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,207 INFO storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on 10.0.119.4:35659 (size: 19.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,207 INFO spark.SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,207 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 122 (MapPartitionsRDD[484] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,207 INFO cluster.YarnScheduler: Adding task set 122.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,208 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 122.0 (TID 95) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4969 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,214 INFO storage.BlockManagerInfo: Added broadcast_97_piece0 in memory on algo-1:39795 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,245 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 122.0 (TID 95) in 37 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,245 INFO cluster.YarnScheduler: Removed TaskSet 122.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,245 INFO scheduler.DAGScheduler: ResultStage 122 (treeReduce at KLLRunner.scala:107) finished in 0.045 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,246 INFO scheduler.DAGScheduler: Job 82 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,246 INFO cluster.YarnScheduler: Killing all running tasks in stage 122: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,246 INFO scheduler.DAGScheduler: Job 82 finished: treeReduce at KLLRunner.scala:107, took 0.047515 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,323 INFO scheduler.DAGScheduler: Registering RDD 489 (collect at AnalysisRunner.scala:326) as input to shuffle 40\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,323 INFO scheduler.DAGScheduler: Got map stage job 83 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,323 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 123 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,323 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,323 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,323 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 123 (MapPartitionsRDD[489] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,325 INFO memory.MemoryStore: Block broadcast_98 stored as values in memory (estimated size 87.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,327 INFO memory.MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,327 INFO storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on 10.0.119.4:35659 (size: 27.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,327 INFO spark.SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,327 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 123 (MapPartitionsRDD[489] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,327 INFO cluster.YarnScheduler: Adding task set 123.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,328 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 123.0 (TID 96) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,333 INFO storage.BlockManagerInfo: Added broadcast_98_piece0 in memory on algo-1:39795 (size: 27.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,346 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 123.0 (TID 96) in 18 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,346 INFO cluster.YarnScheduler: Removed TaskSet 123.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,347 INFO scheduler.DAGScheduler: ShuffleMapStage 123 (collect at AnalysisRunner.scala:326) finished in 0.023 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,347 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,348 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,348 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,348 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,378 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,379 INFO scheduler.DAGScheduler: Got job 84 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,379 INFO scheduler.DAGScheduler: Final stage: ResultStage 125 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,379 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 124)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,379 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,379 INFO scheduler.DAGScheduler: Submitting ResultStage 125 (MapPartitionsRDD[492] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,380 INFO memory.MemoryStore: Block broadcast_99 stored as values in memory (estimated size 67.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,381 INFO memory.MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,381 INFO storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on 10.0.119.4:35659 (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,381 INFO spark.SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,382 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 125 (MapPartitionsRDD[492] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,382 INFO cluster.YarnScheduler: Adding task set 125.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,382 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 125.0 (TID 97) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,388 INFO storage.BlockManagerInfo: Added broadcast_99_piece0 in memory on algo-1:39795 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,390 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 40 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,394 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 125.0 (TID 97) in 12 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,394 INFO cluster.YarnScheduler: Removed TaskSet 125.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,394 INFO scheduler.DAGScheduler: ResultStage 125 (collect at AnalysisRunner.scala:326) finished in 0.015 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,395 INFO scheduler.DAGScheduler: Job 84 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,395 INFO cluster.YarnScheduler: Killing all running tasks in stage 125: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,395 INFO scheduler.DAGScheduler: Job 84 finished: collect at AnalysisRunner.scala:326, took 0.016870 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,432 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,432 INFO scheduler.DAGScheduler: Registering RDD 500 (countByKey at ColumnProfiler.scala:592) as input to shuffle 41\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,432 INFO scheduler.DAGScheduler: Got job 85 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,432 INFO scheduler.DAGScheduler: Final stage: ResultStage 127 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,432 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 126)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,433 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 126)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,433 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 126 (MapPartitionsRDD[500] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,436 INFO memory.MemoryStore: Block broadcast_100 stored as values in memory (estimated size 41.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,437 INFO memory.MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,437 INFO storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on 10.0.119.4:35659 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,437 INFO spark.SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,438 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 126 (MapPartitionsRDD[500] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,438 INFO cluster.YarnScheduler: Adding task set 126.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,438 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 126.0 (TID 98) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,444 INFO storage.BlockManagerInfo: Added broadcast_100_piece0 in memory on algo-1:39795 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,461 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 126.0 (TID 98) in 23 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,461 INFO cluster.YarnScheduler: Removed TaskSet 126.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,462 INFO scheduler.DAGScheduler: ShuffleMapStage 126 (countByKey at ColumnProfiler.scala:592) finished in 0.029 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,462 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,462 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,462 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 127)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,462 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,463 INFO scheduler.DAGScheduler: Submitting ResultStage 127 (ShuffledRDD[501] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,464 INFO memory.MemoryStore: Block broadcast_101 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,465 INFO memory.MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,465 INFO storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on 10.0.119.4:35659 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,465 INFO spark.SparkContext: Created broadcast 101 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,466 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 127 (ShuffledRDD[501] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,466 INFO cluster.YarnScheduler: Adding task set 127.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,466 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 127.0 (TID 99) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,476 INFO storage.BlockManagerInfo: Added broadcast_101_piece0 in memory on algo-1:39795 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,480 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 41 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,486 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 127.0 (TID 99) in 20 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,486 INFO cluster.YarnScheduler: Removed TaskSet 127.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,487 INFO scheduler.DAGScheduler: ResultStage 127 (countByKey at ColumnProfiler.scala:592) finished in 0.023 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,487 INFO scheduler.DAGScheduler: Job 85 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,487 INFO cluster.YarnScheduler: Killing all running tasks in stage 127: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,487 INFO scheduler.DAGScheduler: Job 85 finished: countByKey at ColumnProfiler.scala:592, took 0.055482 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,728 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,757 INFO codegen.CodeGenerator: Code generated in 7.623978 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,761 INFO scheduler.DAGScheduler: Registering RDD 506 (count at StatsGenerator.scala:66) as input to shuffle 42\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,761 INFO scheduler.DAGScheduler: Got map stage job 86 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,761 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 128 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,761 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,761 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,761 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 128 (MapPartitionsRDD[506] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,763 INFO memory.MemoryStore: Block broadcast_102 stored as values in memory (estimated size 34.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,764 INFO memory.MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 13.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,764 INFO storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on 10.0.119.4:35659 (size: 13.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,764 INFO spark.SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,764 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 128 (MapPartitionsRDD[506] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,764 INFO cluster.YarnScheduler: Adding task set 128.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,765 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 128.0 (TID 100) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,773 INFO storage.BlockManagerInfo: Added broadcast_102_piece0 in memory on algo-1:39795 (size: 13.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,799 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 128.0 (TID 100) in 34 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,799 INFO cluster.YarnScheduler: Removed TaskSet 128.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,799 INFO scheduler.DAGScheduler: ShuffleMapStage 128 (count at StatsGenerator.scala:66) finished in 0.037 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,799 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,799 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,799 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,799 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,812 INFO codegen.CodeGenerator: Code generated in 8.180266 ms\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,822 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,822 INFO scheduler.DAGScheduler: Got job 87 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,822 INFO scheduler.DAGScheduler: Final stage: ResultStage 130 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,822 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 129)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,822 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,822 INFO scheduler.DAGScheduler: Submitting ResultStage 130 (MapPartitionsRDD[509] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,823 INFO memory.MemoryStore: Block broadcast_103 stored as values in memory (estimated size 11.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,824 INFO memory.MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,824 INFO storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on 10.0.119.4:35659 (size: 5.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,825 INFO spark.SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,825 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 130 (MapPartitionsRDD[509] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,825 INFO cluster.YarnScheduler: Adding task set 130.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,826 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 130.0 (TID 101) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,833 INFO storage.BlockManagerInfo: Added broadcast_103_piece0 in memory on algo-1:39795 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,840 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 42 to 10.0.119.4:46910\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,854 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 130.0 (TID 101) in 28 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,854 INFO cluster.YarnScheduler: Removed TaskSet 130.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,855 INFO scheduler.DAGScheduler: ResultStage 130 (count at StatsGenerator.scala:66) finished in 0.032 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,855 INFO scheduler.DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,855 INFO cluster.YarnScheduler: Killing all running tasks in stage 130: Stage finished\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:54,856 INFO scheduler.DAGScheduler: Job 87 finished: count at StatsGenerator.scala:66, took 0.034082 s\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,346 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,358 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,420 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,421 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,431 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,454 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,521 WARN nio.NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@594e4b0a.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,523 INFO nio.NioEventLoop: Migrated 0 channel(s) to the new Selector.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,545 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,548 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,565 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,574 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,634 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,634 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,634 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,664 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,666 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-2451e5b9-a4f1-4e9e-bb1e-3af5ee5cc68e\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,671 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1edeab05-94ed-4d17-b3e5-a37c11d9a931\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,763 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2023-05-14 05:45:55,763 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7f1b94770ed0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri + \"/training-dataset-with-header.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Files:\n",
      "sagemaker/DEMO-ModelMonitor/baselining/results/constraints.json\n",
      " sagemaker/DEMO-ModelMonitor/baselining/results/statistics.json\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>numerical_statistics.common.num_present</th>\n",
       "      <th>numerical_statistics.common.num_missing</th>\n",
       "      <th>numerical_statistics.mean</th>\n",
       "      <th>numerical_statistics.sum</th>\n",
       "      <th>numerical_statistics.std_dev</th>\n",
       "      <th>numerical_statistics.min</th>\n",
       "      <th>numerical_statistics.max</th>\n",
       "      <th>numerical_statistics.distribution.kll.buckets</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.c</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.k</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Churn</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.139306</td>\n",
       "      <td>325.0</td>\n",
       "      <td>0.346265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'count': 2008.0}, {'lower_bound': 0.1, 'upper_bound': 0.2, 'count': 0.0}, {'lower_bound': 0.2, 'upper_bound': 0.3, 'count': 0.0}, {'lower_bound': 0.3, 'upper_bound': 0.4, 'count': 0.0}, {'lower_bound': 0.4, 'upper_bound': 0.5, 'count': 0.0}, {'lower_bound': 0.5, 'upper_bound': 0.6, 'count': 0.0}, {'lower_bound': 0.6, 'upper_bound': 0.7, 'count': 0.0}, {'lower_bound': 0.7, 'upper_bound': 0.8, 'count': 0.0}, {'lower_bound': 0.8, 'upper_bound': 0.9, 'count': 0.0}, {'lower_bound': 0.9, 'upper_bound': 1.0, 'count': 325.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Account Length</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>101.276897</td>\n",
       "      <td>236279.0</td>\n",
       "      <td>39.552442</td>\n",
       "      <td>1.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>[{'lower_bound': 1.0, 'upper_bound': 25.2, 'count': 70.0}, {'lower_bound': 25.2, 'upper_bound': 49.4, 'count': 150.0}, {'lower_bound': 49.4, 'upper_bound': 73.6, 'count': 353.0}, {'lower_bound': 73.6, 'upper_bound': 97.8, 'count': 518.0}, {'lower_bound': 97.8, 'upper_bound': 122.0, 'count': 538.0}, {'lower_bound': 122.0, 'upper_bound': 146.2, 'count': 401.0}, {'lower_bound': 146.2, 'upper_bound': 170.4, 'count': 208.0}, {'lower_bound': 170.4, 'upper_bound': 194.6, 'count': 72.0}, {'lower_bound': 194.6, 'upper_bound': 218.8, 'count': 19.0}, {'lower_bound': 218.8, 'upper_bound': 243.0, 'count': 4.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[119.0, 100.0, 111.0, 181.0, 95.0, 104.0, 70.0, 120.0, 88.0, 111.0, 33.0, 106.0, 54.0, 87.0, 94.0, 135.0, 107.0, 159.0, 106.0, 136.0, 116.0, 115.0, 103.0, 95.0, 115.0, 143.0, 48.0, 94.0, 153.0, 94.0, 107.0, 91.0, 141.0, 58.0, 49.0, 41.0, 137.0, 111.0, 71.0, 43.0, 97.0, 3.0, 124.0, 86.0, 87.0, 83.0, 67.0, 46.0, 129.0, 90.0, 97.0, 87.0, 141.0, 136.0, 88.0, 170.0, 44.0, 121.0, 111.0, 105.0, 112.0, 73.0, 147.0, 66.0, 136.0, 119.0, 135.0, 102.0, 169.0, 60.0, 73.0, 83.0, 90.0, 148.0, 59.0, 152.0, 136.0, 112.0, 122.0, 44.0, 122.0, 89.0, 176.0, 64.0, 112.0, 133.0, 52.0, 91.0, 127.0, 153.0, 117.0, 163.0, 76.0, 80.0, 136.0, 91.0, 143.0, 125.0, 126.0, 87.0, ...], [1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 5.0, 6.0, 7.0, 9.0, 9.0, 10.0, 11.0, 13.0, 13.0, 13.0, 15.0, 16.0, 16.0, 16.0, 17.0, 19.0, 19.0, 20.0, 21.0, 21.0, 21.0, 22.0, 23.0, 24.0, 24.0, 25.0, 26.0, 27.0, 27.0, 28.0, 28.0, 29.0, 30.0, 31.0, 31.0, 32.0, 32.0, 32.0, 33.0, 33.0, 34.0, 35.0, 35.0, 35.0, 36.0, 36.0, 36.0, 36.0, 37.0, 37.0, 37.0, 38.0, 38.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 41.0, 41.0, 41.0, 41.0, 42.0, 42.0, 43.0, 43.0, 43.0, 44.0, 44.0, 44.0, 45.0, 45.0, 45.0, 45.0, 46.0, 46.0, 46.0, 46.0, 47.0, 47.0, 47.0, 48.0, 48.0, 48.0, 48.0, 49.0, 49.0, 50.0, 50.0, 51.0, 51.0, 51.0, ...]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VMail Message</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>8.214316</td>\n",
       "      <td>19164.0</td>\n",
       "      <td>13.776908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 5.1, 'count': 1684.0}, {'lower_bound': 5.1, 'upper_bound': 10.2, 'count': 2.0}, {'lower_bound': 10.2, 'upper_bound': 15.3, 'count': 15.0}, {'lower_bound': 15.3, 'upper_bound': 20.4, 'count': 52.0}, {'lower_bound': 20.4, 'upper_bound': 25.5, 'count': 127.0}, {'lower_bound': 25.5, 'upper_bound': 30.6, 'count': 171.0}, {'lower_bound': 30.6, 'upper_bound': 35.7, 'count': 135.0}, {'lower_bound': 35.7, 'upper_bound': 40.8, 'count': 106.0}, {'lower_bound': 40.8, 'upper_bound': 45.9, 'count': 32.0}, {'lower_bound': 45.9, 'upper_bound': 51.0, 'count': 9.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[19.0, 0.0, 0.0, 40.0, 36.0, 0.0, 0.0, 24.0, 0.0, 0.0, 35.0, 0.0, 0.0, 0.0, 0.0, 41.0, 0.0, 0.0, 0.0, 24.0, 0.0, 33.0, 0.0, 37.0, 0.0, 0.0, 43.0, 0.0, 31.0, 28.0, 0.0, 37.0, 0.0, 0.0, 28.0, 34.0, 0.0, 0.0, 0.0, 35.0, 0.0, 36.0, 0.0, 29.0, 0.0, 30.0, 35.0, 0.0, 33.0, 0.0, 0.0, 0.0, 37.0, 0.0, 0.0, 0.0, 0.0, 24.0, 0.0, 24.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.0, 0.0, 0.0, 29.0, 20.0, 33.0, 0.0, 0.0, 0.0, 23.0, 0.0, 0.0, 22.0, 0.0, 32.0, 0.0, 34.0, 0.0, 22.0, 0.0, 0.0, 0.0, 0.0, 27.0, 0.0, 0.0, 29.0, 0.0, 0.0, ...], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Day Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>180.226489</td>\n",
       "      <td>420468.4</td>\n",
       "      <td>53.987179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>350.8</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 35.08, 'count': 14.0}, {'lower_bound': 35.08, 'upper_bound': 70.16, 'count': 48.0}, {'lower_bound': 70.16, 'upper_bound': 105.24000000000001, 'count': 130.0}, {'lower_bound': 105.24000000000001, 'upper_bound': 140.32, 'count': 318.0}, {'lower_bound': 140.32, 'upper_bound': 175.4, 'count': 565.0}, {'lower_bound': 175.4, 'upper_bound': 210.48000000000002, 'count': 587.0}, {'lower_bound': 210.48000000000002, 'upper_bound': 245.56, 'count': 423.0}, {'lower_bound': 245.56, 'upper_bound': 280.64, 'count': 180.0}, {'lower_bound': 280.64, 'upper_bound': 315.72, 'count': 58.0}, {'lower_bound': 315.72, 'upper_bound': 350.8, 'count': 10.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[178.1, 160.3, 197.1, 105.2, 283.1, 113.6, 232.1, 212.7, 73.3, 176.9, 161.9, 128.6, 190.5, 223.2, 157.9, 173.1, 273.5, 275.8, 119.2, 174.6, 133.3, 145.0, 150.6, 220.2, 109.7, 155.4, 172.0, 235.6, 218.5, 92.7, 90.7, 162.3, 146.5, 210.1, 214.4, 194.4, 237.3, 255.9, 197.9, 200.2, 120.8, 118.1, 131.8, 225.4, 205.2, 272.5, 181.1, 122.2, 119.6, 109.6, 112.7, 136.3, 185.4, 199.6, 218.2, 259.9, 143.2, 218.2, 249.8, 274.7, 167.8, 182.3, 157.0, 207.7, 250.2, 81.9, 246.8, 103.1, 147.2, 252.7, 192.2, 226.4, 145.5, 178.3, 133.1, 214.6, 203.9, 185.4, 140.0, 240.3, 134.2, 141.1, 47.4, 200.4, 167.6, 221.1, 165.5, 175.3, 146.7, 167.7, 184.5, 202.9, 273.3, 194.8, 187.7, 133.7, 209.1, 260.8, 211.6, 156.8, ...], [0.0, 2.6, 7.9, 17.6, 19.5, 27.0, 34.0, 37.8, 40.9, 45.0, 47.8, 49.9, 51.1, 51.8, 54.7, 55.3, 55.6, 57.5, 58.4, 58.9, 60.0, 61.3, 61.9, 62.4, 62.8, 62.9, 64.9, 67.4, 68.5, 70.7, 70.9, 72.5, 72.8, 75.8, 77.6, 78.6, 80.3, 81.3, 81.7, 82.3, 82.5, 82.6, 83.2, 83.8, 84.8, 85.7, 85.9, 86.0, 86.3, 87.2, 87.7, 88.5, 89.5, 89.7, 90.0, 91.5, 92.3, 92.8, 93.4, 93.8, 94.7, 95.0, 95.5, 95.9, 96.3, 96.8, 97.5, 98.0, 98.2, 98.4, 99.4, 99.9, 100.8, 101.1, 101.7, 102.1, 102.6, 102.8, 103.2, 103.4, 103.5, 103.7, 104.6, 104.7, 104.9, 105.0, 105.3, 105.8, 105.9, 106.4, 106.7, 107.5, 107.8, 108.3, 108.6, 109.1, 109.4, 109.5, 110.1, 110.5, ...]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Day Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>100.259323</td>\n",
       "      <td>233905.0</td>\n",
       "      <td>20.165008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 16.5, 'count': 2.0}, {'lower_bound': 16.5, 'upper_bound': 33.0, 'count': 0.0}, {'lower_bound': 33.0, 'upper_bound': 49.5, 'count': 14.0}, {'lower_bound': 49.5, 'upper_bound': 66.0, 'count': 80.0}, {'lower_bound': 66.0, 'upper_bound': 82.5, 'count': 344.0}, {'lower_bound': 82.5, 'upper_bound': 99.0, 'count': 636.0}, {'lower_bound': 99.0, 'upper_bound': 115.5, 'count': 737.0}, {'lower_bound': 115.5, 'upper_bound': 132.0, 'count': 377.0}, {'lower_bound': 132.0, 'upper_bound': 148.5, 'count': 127.0}, {'lower_bound': 148.5, 'upper_bound': 165.0, 'count': 16.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[110.0, 138.0, 117.0, 61.0, 112.0, 87.0, 122.0, 73.0, 86.0, 128.0, 85.0, 83.0, 108.0, 109.0, 105.0, 85.0, 104.0, 103.0, 142.0, 76.0, 94.0, 72.0, 125.0, 109.0, 148.0, 112.0, 111.0, 131.0, 130.0, 107.0, 90.0, 107.0, 121.0, 126.0, 78.0, 63.0, 103.0, 97.0, 108.0, 105.0, 96.0, 117.0, 82.0, 79.0, 106.0, 105.0, 59.0, 67.0, 104.0, 88.0, 119.0, 97.0, 87.0, 89.0, 76.0, 68.0, 77.0, 88.0, 109.0, 99.0, 88.0, 115.0, 79.0, 85.0, 121.0, 75.0, 129.0, 70.0, 115.0, 97.0, 86.0, 117.0, 92.0, 98.0, 114.0, 108.0, 106.0, 114.0, 101.0, 146.0, 85.0, 92.0, 125.0, 80.0, 100.0, 137.0, 78.0, 96.0, 91.0, 104.0, 97.0, 100.0, 66.0, 116.0, 84.0, 75.0, 127.0, 81.0, 70.0, 93.0, ...], [0.0, 35.0, 40.0, 44.0, 45.0, 49.0, 51.0, 52.0, 53.0, 54.0, 54.0, 55.0, 55.0, 55.0, 56.0, 56.0, 57.0, 57.0, 58.0, 58.0, 59.0, 59.0, 60.0, 61.0, 61.0, 61.0, 61.0, 61.0, 61.0, 62.0, 62.0, 62.0, 63.0, 63.0, 63.0, 63.0, 64.0, 65.0, 65.0, 65.0, 65.0, 65.0, 66.0, 66.0, 66.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 68.0, 68.0, 68.0, 68.0, 69.0, 69.0, 69.0, 69.0, 70.0, 70.0, 70.0, 70.0, 70.0, 70.0, 70.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, ...]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Eve Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>200.050107</td>\n",
       "      <td>466716.9</td>\n",
       "      <td>50.015928</td>\n",
       "      <td>31.2</td>\n",
       "      <td>361.8</td>\n",
       "      <td>[{'lower_bound': 31.2, 'upper_bound': 64.26, 'count': 7.0}, {'lower_bound': 64.26, 'upper_bound': 97.32000000000001, 'count': 43.0}, {'lower_bound': 97.32000000000001, 'upper_bound': 130.38, 'count': 135.0}, {'lower_bound': 130.38, 'upper_bound': 163.44, 'count': 360.0}, {'lower_bound': 163.44, 'upper_bound': 196.5, 'count': 555.0}, {'lower_bound': 196.5, 'upper_bound': 229.56, 'count': 587.0}, {'lower_bound': 229.56, 'upper_bound': 262.62, 'count': 404.0}, {'lower_bound': 262.62, 'upper_bound': 295.68, 'count': 178.0}, {'lower_bound': 295.68, 'upper_bound': 328.74, 'count': 49.0}, {'lower_bound': 328.74, 'upper_bound': 361.8, 'count': 15.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[212.8, 221.3, 227.8, 341.3, 286.2, 158.6, 292.3, 257.5, 161.4, 102.8, 151.2, 134.0, 259.7, 127.5, 155.0, 203.9, 183.8, 189.5, 228.4, 176.6, 247.8, 194.5, 169.1, 185.3, 223.8, 290.9, 200.2, 194.8, 134.2, 127.8, 207.5, 233.9, 169.9, 248.9, 235.2, 254.9, 176.7, 204.1, 181.5, 244.4, 169.8, 221.5, 284.3, 187.1, 99.5, 253.0, 215.9, 167.2, 278.7, 137.6, 217.7, 172.2, 178.5, 211.4, 169.3, 245.0, 169.8, 348.5, 242.4, 193.5, 247.9, 199.2, 103.1, 196.7, 267.1, 253.8, 187.8, 275.0, 161.9, 221.1, 168.6, 234.7, 217.7, 282.6, 221.2, 96.6, 187.6, 191.4, 196.4, 164.6, 227.3, 249.1, 167.8, 131.1, 154.5, 264.9, 205.5, 262.3, 203.5, 246.8, 351.6, 178.6, 263.6, 209.9, 221.0, 195.3, 106.1, 163.7, 216.9, 215.8, ...], [31.2, 58.9, 60.8, 65.2, 66.5, 71.0, 73.2, 75.3, 77.1, 78.3, 79.3, 80.6, 82.2, 83.9, 87.6, 88.6, 89.7, 90.0, 90.5, 92.0, 93.7, 95.1, 98.3, 101.3, 102.2, 102.6, 103.4, 105.5, 105.7, 106.2, 106.8, 107.9, 108.2, 109.9, 110.2, 110.8, 112.5, 113.2, 113.3, 114.3, 114.5, 114.7, 115.0, 115.7, 116.5, 116.6, 117.0, 117.9, 118.0, 118.5, 118.7, 118.9, 119.3, 119.6, 120.0, 120.3, 120.4, 120.5, 120.7, 121.0, 121.6, 122.2, 122.8, 123.0, 123.4, 123.5, 123.5, 123.9, 123.9, 124.4, 126.0, 126.9, 127.3, 127.8, 128.7, 128.9, 129.1, 129.3, 129.4, 129.8, 130.1, 130.2, 130.7, 131.1, 131.4, 131.7, 131.8, 132.3, 132.5, 132.9, 133.0, 133.4, 133.9, 134.1, 134.3, 134.5, 134.7, 134.9, 135.0, 135.2, ...]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eve Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>99.573939</td>\n",
       "      <td>232306.0</td>\n",
       "      <td>19.675578</td>\n",
       "      <td>12.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>[{'lower_bound': 12.0, 'upper_bound': 27.8, 'count': 2.0}, {'lower_bound': 27.8, 'upper_bound': 43.6, 'count': 2.0}, {'lower_bound': 43.6, 'upper_bound': 59.4, 'count': 44.0}, {'lower_bound': 59.4, 'upper_bound': 75.2, 'count': 195.0}, {'lower_bound': 75.2, 'upper_bound': 91.0, 'count': 530.0}, {'lower_bound': 91.0, 'upper_bound': 106.8, 'count': 708.0}, {'lower_bound': 106.8, 'upper_bound': 122.6, 'count': 576.0}, {'lower_bound': 122.6, 'upper_bound': 138.4, 'count': 215.0}, {'lower_bound': 138.4, 'upper_bound': 154.2, 'count': 56.0}, {'lower_bound': 154.2, 'upper_bound': 170.0, 'count': 5.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[100.0, 92.0, 128.0, 79.0, 86.0, 98.0, 112.0, 103.0, 82.0, 56.0, 82.0, 114.0, 108.0, 86.0, 101.0, 107.0, 68.0, 108.0, 139.0, 114.0, 126.0, 157.0, 126.0, 99.0, 87.0, 92.0, 64.0, 107.0, 103.0, 86.0, 109.0, 115.0, 125.0, 108.0, 100.0, 110.0, 84.0, 129.0, 109.0, 88.0, 101.0, 125.0, 119.0, 112.0, 122.0, 83.0, 116.0, 62.0, 88.0, 108.0, 109.0, 108.0, 128.0, 96.0, 60.0, 122.0, 114.0, 108.0, 106.0, 118.0, 81.0, 97.0, 94.0, 112.0, 118.0, 114.0, 121.0, 129.0, 123.0, 121.0, 116.0, 97.0, 114.0, 110.0, 82.0, 82.0, 99.0, 119.0, 77.0, 83.0, 132.0, 126.0, 90.0, 84.0, 90.0, 99.0, 89.0, 122.0, 78.0, 91.0, 80.0, 46.0, 121.0, 93.0, 147.0, 87.0, 80.0, 112.0, 80.0, 68.0, ...], [12.0, 42.0, 44.0, 48.0, 48.0, 48.0, 50.0, 52.0, 52.0, 53.0, 54.0, 56.0, 56.0, 57.0, 58.0, 58.0, 58.0, 58.0, 59.0, 59.0, 59.0, 60.0, 60.0, 60.0, 60.0, 60.0, 61.0, 61.0, 61.0, 62.0, 62.0, 63.0, 63.0, 63.0, 63.0, 63.0, 64.0, 64.0, 64.0, 65.0, 65.0, 65.0, 65.0, 65.0, 66.0, 66.0, 66.0, 66.0, 66.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 68.0, 68.0, 68.0, 69.0, 69.0, 69.0, 69.0, 70.0, 70.0, 70.0, 70.0, 70.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, ...]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Night Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>201.388598</td>\n",
       "      <td>469839.6</td>\n",
       "      <td>50.627961</td>\n",
       "      <td>23.2</td>\n",
       "      <td>395.0</td>\n",
       "      <td>[{'lower_bound': 23.2, 'upper_bound': 60.379999999999995, 'count': 9.0}, {'lower_bound': 60.379999999999995, 'upper_bound': 97.56, 'count': 35.0}, {'lower_bound': 97.56, 'upper_bound': 134.74, 'count': 171.0}, {'lower_bound': 134.74, 'upper_bound': 171.92, 'count': 463.0}, {'lower_bound': 171.92, 'upper_bound': 209.1, 'count': 623.0}, {'lower_bound': 209.1, 'upper_bound': 246.28, 'count': 590.0}, {'lower_bound': 246.28, 'upper_bound': 283.46, 'count': 325.0}, {'lower_bound': 283.46, 'upper_bound': 320.64, 'count': 98.0}, {'lower_bound': 320.64, 'upper_bound': 357.82, 'count': 15.0}, {'lower_bound': 357.82, 'upper_bound': 395.0, 'count': 4.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[226.3, 150.4, 214.0, 165.7, 261.7, 187.7, 201.2, 227.8, 239.6, 213.7, 191.0, 210.6, 141.5, 289.3, 189.6, 122.2, 153.8, 223.9, 197.9, 214.4, 219.0, 242.3, 221.2, 205.1, 240.3, 228.4, 233.1, 170.6, 118.9, 225.6, 169.4, 277.4, 238.8, 158.6, 206.2, 160.2, 263.4, 171.3, 281.4, 207.2, 194.1, 103.9, 305.5, 281.1, 189.5, 180.8, 216.3, 194.8, 263.4, 159.7, 152.1, 137.5, 218.3, 72.4, 141.1, 134.4, 215.8, 212.6, 231.8, 299.6, 155.1, 120.2, 211.8, 261.7, 151.0, 213.1, 154.5, 141.1, 142.1, 109.9, 139.8, 133.6, 146.9, 181.0, 131.6, 170.7, 101.7, 144.0, 120.1, 240.7, 122.4, 136.0, 163.1, 230.7, 281.4, 168.9, 213.6, 143.9, 203.4, 203.9, 215.8, 203.8, 165.2, 194.1, 145.7, 280.5, 179.6, 271.7, 153.5, 223.3, ...], [23.2, 45.0, 53.3, 56.6, 61.4, 65.8, 71.1, 75.8, 77.3, 79.3, 80.2, 82.4, 88.2, 89.7, 91.6, 94.3, 94.9, 95.3, 96.4, 97.4, 98.6, 99.3, 100.9, 102.0, 102.1, 103.7, 104.5, 104.7, 104.8, 105.4, 105.6, 107.3, 107.9, 108.1, 108.9, 109.6, 109.6, 109.6, 110.1, 110.4, 111.2, 111.6, 111.7, 112.9, 113.5, 114.2, 114.3, 114.5, 115.7, 116.1, 116.4, 117.0, 117.8, 117.9, 118.0, 118.3, 119.1, 119.1, 119.4, 120.0, 121.0, 121.1, 122.0, 122.3, 122.6, 123.0, 123.4, 124.0, 125.6, 126.3, 126.9, 127.1, 127.4, 127.7, 127.9, 128.2, 128.4, 128.7, 128.9, 129.1, 129.6, 129.6, 129.7, 129.9, 130.6, 130.9, 132.0, 132.5, 132.6, 132.9, 133.1, 133.4, 133.7, 134.0, 134.2, 134.3, 134.6, 134.9, 135.0, 135.0, ...]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Night Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>100.227175</td>\n",
       "      <td>233830.0</td>\n",
       "      <td>19.282029</td>\n",
       "      <td>42.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>[{'lower_bound': 42.0, 'upper_bound': 55.3, 'count': 20.0}, {'lower_bound': 55.3, 'upper_bound': 68.6, 'count': 101.0}, {'lower_bound': 68.6, 'upper_bound': 81.9, 'count': 281.0}, {'lower_bound': 81.9, 'upper_bound': 95.2, 'count': 540.0}, {'lower_bound': 95.2, 'upper_bound': 108.5, 'count': 604.0}, {'lower_bound': 108.5, 'upper_bound': 121.8, 'count': 493.0}, {'lower_bound': 121.8, 'upper_bound': 135.1, 'count': 212.0}, {'lower_bound': 135.1, 'upper_bound': 148.4, 'count': 66.0}, {'lower_bound': 148.4, 'upper_bound': 161.7, 'count': 15.0}, {'lower_bound': 161.7, 'upper_bound': 175.0, 'count': 1.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[123.0, 120.0, 101.0, 97.0, 129.0, 87.0, 112.0, 119.0, 76.0, 84.0, 131.0, 113.0, 111.0, 83.0, 84.0, 78.0, 67.0, 93.0, 61.0, 91.0, 78.0, 138.0, 104.0, 82.0, 96.0, 91.0, 96.0, 93.0, 105.0, 86.0, 96.0, 94.0, 112.0, 88.0, 107.0, 115.0, 81.0, 84.0, 56.0, 97.0, 63.0, 89.0, 101.0, 112.0, 75.0, 123.0, 106.0, 98.0, 175.0, 121.0, 76.0, 101.0, 107.0, 84.0, 99.0, 121.0, 77.0, 118.0, 78.0, 109.0, 108.0, 113.0, 96.0, 83.0, 114.0, 125.0, 109.0, 92.0, 103.0, 100.0, 87.0, 82.0, 123.0, 98.0, 103.0, 145.0, 107.0, 78.0, 133.0, 106.0, 96.0, 73.0, 107.0, 67.0, 107.0, 108.0, 124.0, 76.0, 110.0, 117.0, 90.0, 116.0, 84.0, 100.0, 110.0, 89.0, 90.0, 117.0, 60.0, 77.0, ...], [44.0, 49.0, 50.0, 51.0, 52.0, 53.0, 53.0, 54.0, 55.0, 57.0, 57.0, 57.0, 58.0, 58.0, 59.0, 59.0, 59.0, 60.0, 60.0, 60.0, 61.0, 61.0, 61.0, 62.0, 62.0, 63.0, 63.0, 63.0, 63.0, 64.0, 64.0, 64.0, 64.0, 65.0, 65.0, 65.0, 66.0, 66.0, 66.0, 66.0, 67.0, 67.0, 67.0, 67.0, 67.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 69.0, 69.0, 69.0, 69.0, 69.0, 70.0, 70.0, 70.0, 70.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 75.0, ...]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Intl Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>10.253065</td>\n",
       "      <td>23920.4</td>\n",
       "      <td>2.778766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 1.8399999999999999, 'count': 15.0}, {'lower_bound': 1.8399999999999999, 'upper_bound': 3.6799999999999997, 'count': 15.0}, {'lower_bound': 3.6799999999999997, 'upper_bound': 5.52, 'count': 79.0}, {'lower_bound': 5.52, 'upper_bound': 7.359999999999999, 'count': 226.0}, {'lower_bound': 7.359999999999999, 'upper_bound': 9.2, 'count': 427.0}, {'lower_bound': 9.2, 'upper_bound': 11.04, 'count': 644.0}, {'lower_bound': 11.04, 'upper_bound': 12.879999999999999, 'count': 529.0}, {'lower_bound': 12.879999999999999, 'upper_bound': 14.719999999999999, 'count': 296.0}, {'lower_bound': 14.719999999999999, 'upper_bound': 16.56, 'count': 82.0}, {'lower_bound': 16.56, 'upper_bound': 18.4, 'count': 20.0}]</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[10.0, 11.2, 9.3, 6.3, 11.3, 10.5, 0.0, 9.7, 8.2, 10.5, 8.5, 11.4, 9.7, 14.5, 8.0, 14.6, 11.0, 7.4, 8.4, 8.8, 11.3, 14.2, 10.4, 4.1, 15.4, 13.9, 8.0, 8.6, 9.4, 9.9, 5.6, 9.2, 8.2, 14.4, 8.0, 17.2, 14.2, 12.3, 6.7, 11.6, 11.9, 11.9, 11.3, 12.9, 13.4, 8.7, 16.9, 9.7, 5.9, 11.0, 6.5, 7.1, 8.0, 11.0, 8.0, 8.4, 7.6, 7.5, 11.6, 10.8, 11.9, 18.0, 7.1, 6.8, 13.0, 8.9, 12.6, 11.2, 7.2, 12.4, 9.4, 10.8, 10.9, 11.4, 6.8, 7.9, 10.5, 10.0, 9.7, 10.6, 8.5, 10.8, 10.5, 7.6, 17.3, 15.4, 12.2, 5.6, 13.7, 7.5, 8.7, 12.8, 12.0, 12.8, 10.0, 5.9, 14.0, 17.0, 7.8, 7.6, ...], [0.0, 0.0, 0.0, 0.0, 0.0, 1.1, 2.0, 2.5, 2.9, 3.3, 3.5, 3.6, 3.7, 3.8, 3.8, 4.1, 4.2, 4.2, 4.3, 4.3, 4.4, 4.5, 4.6, 4.7, 4.7, 4.7, 4.8, 4.8, 4.9, 5.0, 5.0, 5.0, 5.1, 5.1, 5.1, 5.2, 5.3, 5.3, 5.3, 5.3, 5.3, 5.4, 5.4, 5.4, 5.4, 5.5, 5.5, 5.5, 5.5, 5.6, 5.6, 5.6, 5.7, 5.7, 5.8, 5.8, 5.8, 5.8, 5.9, 5.9, 5.9, 5.9, 5.9, 5.9, 5.9, 6.0, 6.0, 6.0, 6.1, 6.1, 6.1, 6.1, 6.2, 6.2, 6.3, 6.3, 6.3, 6.3, 6.3, 6.3, 6.4, 6.4, 6.4, 6.4, 6.4, 6.4, 6.4, 6.5, 6.5, 6.5, 6.5, 6.5, 6.5, 6.6, 6.6, 6.6, 6.6, 6.6, 6.6, 6.6, ...]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name inferred_type  numerical_statistics.common.num_present  \\\n",
       "0           Churn      Integral                                     2333   \n",
       "1  Account Length      Integral                                     2333   \n",
       "2   VMail Message      Integral                                     2333   \n",
       "3        Day Mins    Fractional                                     2333   \n",
       "4       Day Calls      Integral                                     2333   \n",
       "5        Eve Mins    Fractional                                     2333   \n",
       "6       Eve Calls      Integral                                     2333   \n",
       "7      Night Mins    Fractional                                     2333   \n",
       "8     Night Calls      Integral                                     2333   \n",
       "9       Intl Mins    Fractional                                     2333   \n",
       "\n",
       "   numerical_statistics.common.num_missing  numerical_statistics.mean  \\\n",
       "0                                        0                   0.139306   \n",
       "1                                        0                 101.276897   \n",
       "2                                        0                   8.214316   \n",
       "3                                        0                 180.226489   \n",
       "4                                        0                 100.259323   \n",
       "5                                        0                 200.050107   \n",
       "6                                        0                  99.573939   \n",
       "7                                        0                 201.388598   \n",
       "8                                        0                 100.227175   \n",
       "9                                        0                  10.253065   \n",
       "\n",
       "   numerical_statistics.sum  numerical_statistics.std_dev  \\\n",
       "0                     325.0                      0.346265   \n",
       "1                  236279.0                     39.552442   \n",
       "2                   19164.0                     13.776908   \n",
       "3                  420468.4                     53.987179   \n",
       "4                  233905.0                     20.165008   \n",
       "5                  466716.9                     50.015928   \n",
       "6                  232306.0                     19.675578   \n",
       "7                  469839.6                     50.627961   \n",
       "8                  233830.0                     19.282029   \n",
       "9                   23920.4                      2.778766   \n",
       "\n",
       "   numerical_statistics.min  numerical_statistics.max  \\\n",
       "0                       0.0                       1.0   \n",
       "1                       1.0                     243.0   \n",
       "2                       0.0                      51.0   \n",
       "3                       0.0                     350.8   \n",
       "4                       0.0                     165.0   \n",
       "5                      31.2                     361.8   \n",
       "6                      12.0                     170.0   \n",
       "7                      23.2                     395.0   \n",
       "8                      42.0                     175.0   \n",
       "9                       0.0                      18.4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    numerical_statistics.distribution.kll.buckets  \\\n",
       "0                                                                                                                                                                           [{'lower_bound': 0.0, 'upper_bound': 0.1, 'count': 2008.0}, {'lower_bound': 0.1, 'upper_bound': 0.2, 'count': 0.0}, {'lower_bound': 0.2, 'upper_bound': 0.3, 'count': 0.0}, {'lower_bound': 0.3, 'upper_bound': 0.4, 'count': 0.0}, {'lower_bound': 0.4, 'upper_bound': 0.5, 'count': 0.0}, {'lower_bound': 0.5, 'upper_bound': 0.6, 'count': 0.0}, {'lower_bound': 0.6, 'upper_bound': 0.7, 'count': 0.0}, {'lower_bound': 0.7, 'upper_bound': 0.8, 'count': 0.0}, {'lower_bound': 0.8, 'upper_bound': 0.9, 'count': 0.0}, {'lower_bound': 0.9, 'upper_bound': 1.0, 'count': 325.0}]   \n",
       "1                                                                                                                                   [{'lower_bound': 1.0, 'upper_bound': 25.2, 'count': 70.0}, {'lower_bound': 25.2, 'upper_bound': 49.4, 'count': 150.0}, {'lower_bound': 49.4, 'upper_bound': 73.6, 'count': 353.0}, {'lower_bound': 73.6, 'upper_bound': 97.8, 'count': 518.0}, {'lower_bound': 97.8, 'upper_bound': 122.0, 'count': 538.0}, {'lower_bound': 122.0, 'upper_bound': 146.2, 'count': 401.0}, {'lower_bound': 146.2, 'upper_bound': 170.4, 'count': 208.0}, {'lower_bound': 170.4, 'upper_bound': 194.6, 'count': 72.0}, {'lower_bound': 194.6, 'upper_bound': 218.8, 'count': 19.0}, {'lower_bound': 218.8, 'upper_bound': 243.0, 'count': 4.0}]   \n",
       "2                                                                                                                                                 [{'lower_bound': 0.0, 'upper_bound': 5.1, 'count': 1684.0}, {'lower_bound': 5.1, 'upper_bound': 10.2, 'count': 2.0}, {'lower_bound': 10.2, 'upper_bound': 15.3, 'count': 15.0}, {'lower_bound': 15.3, 'upper_bound': 20.4, 'count': 52.0}, {'lower_bound': 20.4, 'upper_bound': 25.5, 'count': 127.0}, {'lower_bound': 25.5, 'upper_bound': 30.6, 'count': 171.0}, {'lower_bound': 30.6, 'upper_bound': 35.7, 'count': 135.0}, {'lower_bound': 35.7, 'upper_bound': 40.8, 'count': 106.0}, {'lower_bound': 40.8, 'upper_bound': 45.9, 'count': 32.0}, {'lower_bound': 45.9, 'upper_bound': 51.0, 'count': 9.0}]   \n",
       "3                                                              [{'lower_bound': 0.0, 'upper_bound': 35.08, 'count': 14.0}, {'lower_bound': 35.08, 'upper_bound': 70.16, 'count': 48.0}, {'lower_bound': 70.16, 'upper_bound': 105.24000000000001, 'count': 130.0}, {'lower_bound': 105.24000000000001, 'upper_bound': 140.32, 'count': 318.0}, {'lower_bound': 140.32, 'upper_bound': 175.4, 'count': 565.0}, {'lower_bound': 175.4, 'upper_bound': 210.48000000000002, 'count': 587.0}, {'lower_bound': 210.48000000000002, 'upper_bound': 245.56, 'count': 423.0}, {'lower_bound': 245.56, 'upper_bound': 280.64, 'count': 180.0}, {'lower_bound': 280.64, 'upper_bound': 315.72, 'count': 58.0}, {'lower_bound': 315.72, 'upper_bound': 350.8, 'count': 10.0}]   \n",
       "4                                                                                                                                         [{'lower_bound': 0.0, 'upper_bound': 16.5, 'count': 2.0}, {'lower_bound': 16.5, 'upper_bound': 33.0, 'count': 0.0}, {'lower_bound': 33.0, 'upper_bound': 49.5, 'count': 14.0}, {'lower_bound': 49.5, 'upper_bound': 66.0, 'count': 80.0}, {'lower_bound': 66.0, 'upper_bound': 82.5, 'count': 344.0}, {'lower_bound': 82.5, 'upper_bound': 99.0, 'count': 636.0}, {'lower_bound': 99.0, 'upper_bound': 115.5, 'count': 737.0}, {'lower_bound': 115.5, 'upper_bound': 132.0, 'count': 377.0}, {'lower_bound': 132.0, 'upper_bound': 148.5, 'count': 127.0}, {'lower_bound': 148.5, 'upper_bound': 165.0, 'count': 16.0}]   \n",
       "5                                                                                      [{'lower_bound': 31.2, 'upper_bound': 64.26, 'count': 7.0}, {'lower_bound': 64.26, 'upper_bound': 97.32000000000001, 'count': 43.0}, {'lower_bound': 97.32000000000001, 'upper_bound': 130.38, 'count': 135.0}, {'lower_bound': 130.38, 'upper_bound': 163.44, 'count': 360.0}, {'lower_bound': 163.44, 'upper_bound': 196.5, 'count': 555.0}, {'lower_bound': 196.5, 'upper_bound': 229.56, 'count': 587.0}, {'lower_bound': 229.56, 'upper_bound': 262.62, 'count': 404.0}, {'lower_bound': 262.62, 'upper_bound': 295.68, 'count': 178.0}, {'lower_bound': 295.68, 'upper_bound': 328.74, 'count': 49.0}, {'lower_bound': 328.74, 'upper_bound': 361.8, 'count': 15.0}]   \n",
       "6                                                                                                                                       [{'lower_bound': 12.0, 'upper_bound': 27.8, 'count': 2.0}, {'lower_bound': 27.8, 'upper_bound': 43.6, 'count': 2.0}, {'lower_bound': 43.6, 'upper_bound': 59.4, 'count': 44.0}, {'lower_bound': 59.4, 'upper_bound': 75.2, 'count': 195.0}, {'lower_bound': 75.2, 'upper_bound': 91.0, 'count': 530.0}, {'lower_bound': 91.0, 'upper_bound': 106.8, 'count': 708.0}, {'lower_bound': 106.8, 'upper_bound': 122.6, 'count': 576.0}, {'lower_bound': 122.6, 'upper_bound': 138.4, 'count': 215.0}, {'lower_bound': 138.4, 'upper_bound': 154.2, 'count': 56.0}, {'lower_bound': 154.2, 'upper_bound': 170.0, 'count': 5.0}]   \n",
       "7                                                                                      [{'lower_bound': 23.2, 'upper_bound': 60.379999999999995, 'count': 9.0}, {'lower_bound': 60.379999999999995, 'upper_bound': 97.56, 'count': 35.0}, {'lower_bound': 97.56, 'upper_bound': 134.74, 'count': 171.0}, {'lower_bound': 134.74, 'upper_bound': 171.92, 'count': 463.0}, {'lower_bound': 171.92, 'upper_bound': 209.1, 'count': 623.0}, {'lower_bound': 209.1, 'upper_bound': 246.28, 'count': 590.0}, {'lower_bound': 246.28, 'upper_bound': 283.46, 'count': 325.0}, {'lower_bound': 283.46, 'upper_bound': 320.64, 'count': 98.0}, {'lower_bound': 320.64, 'upper_bound': 357.82, 'count': 15.0}, {'lower_bound': 357.82, 'upper_bound': 395.0, 'count': 4.0}]   \n",
       "8                                                                                                                                  [{'lower_bound': 42.0, 'upper_bound': 55.3, 'count': 20.0}, {'lower_bound': 55.3, 'upper_bound': 68.6, 'count': 101.0}, {'lower_bound': 68.6, 'upper_bound': 81.9, 'count': 281.0}, {'lower_bound': 81.9, 'upper_bound': 95.2, 'count': 540.0}, {'lower_bound': 95.2, 'upper_bound': 108.5, 'count': 604.0}, {'lower_bound': 108.5, 'upper_bound': 121.8, 'count': 493.0}, {'lower_bound': 121.8, 'upper_bound': 135.1, 'count': 212.0}, {'lower_bound': 135.1, 'upper_bound': 148.4, 'count': 66.0}, {'lower_bound': 148.4, 'upper_bound': 161.7, 'count': 15.0}, {'lower_bound': 161.7, 'upper_bound': 175.0, 'count': 1.0}]   \n",
       "9  [{'lower_bound': 0.0, 'upper_bound': 1.8399999999999999, 'count': 15.0}, {'lower_bound': 1.8399999999999999, 'upper_bound': 3.6799999999999997, 'count': 15.0}, {'lower_bound': 3.6799999999999997, 'upper_bound': 5.52, 'count': 79.0}, {'lower_bound': 5.52, 'upper_bound': 7.359999999999999, 'count': 226.0}, {'lower_bound': 7.359999999999999, 'upper_bound': 9.2, 'count': 427.0}, {'lower_bound': 9.2, 'upper_bound': 11.04, 'count': 644.0}, {'lower_bound': 11.04, 'upper_bound': 12.879999999999999, 'count': 529.0}, {'lower_bound': 12.879999999999999, 'upper_bound': 14.719999999999999, 'count': 296.0}, {'lower_bound': 14.719999999999999, 'upper_bound': 16.56, 'count': 82.0}, {'lower_bound': 16.56, 'upper_bound': 18.4, 'count': 20.0}]   \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.c  \\\n",
       "0                                                       0.64   \n",
       "1                                                       0.64   \n",
       "2                                                       0.64   \n",
       "3                                                       0.64   \n",
       "4                                                       0.64   \n",
       "5                                                       0.64   \n",
       "6                                                       0.64   \n",
       "7                                                       0.64   \n",
       "8                                                       0.64   \n",
       "9                                                       0.64   \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.k  \\\n",
       "0                                                     2048.0   \n",
       "1                                                     2048.0   \n",
       "2                                                     2048.0   \n",
       "3                                                     2048.0   \n",
       "4                                                     2048.0   \n",
       "5                                                     2048.0   \n",
       "6                                                     2048.0   \n",
       "7                                                     2048.0   \n",
       "8                                                     2048.0   \n",
       "9                                                     2048.0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 numerical_statistics.distribution.kll.sketch.data  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                           [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]]  \n",
       "1                                                                                                                                        [[119.0, 100.0, 111.0, 181.0, 95.0, 104.0, 70.0, 120.0, 88.0, 111.0, 33.0, 106.0, 54.0, 87.0, 94.0, 135.0, 107.0, 159.0, 106.0, 136.0, 116.0, 115.0, 103.0, 95.0, 115.0, 143.0, 48.0, 94.0, 153.0, 94.0, 107.0, 91.0, 141.0, 58.0, 49.0, 41.0, 137.0, 111.0, 71.0, 43.0, 97.0, 3.0, 124.0, 86.0, 87.0, 83.0, 67.0, 46.0, 129.0, 90.0, 97.0, 87.0, 141.0, 136.0, 88.0, 170.0, 44.0, 121.0, 111.0, 105.0, 112.0, 73.0, 147.0, 66.0, 136.0, 119.0, 135.0, 102.0, 169.0, 60.0, 73.0, 83.0, 90.0, 148.0, 59.0, 152.0, 136.0, 112.0, 122.0, 44.0, 122.0, 89.0, 176.0, 64.0, 112.0, 133.0, 52.0, 91.0, 127.0, 153.0, 117.0, 163.0, 76.0, 80.0, 136.0, 91.0, 143.0, 125.0, 126.0, 87.0, ...], [1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 5.0, 6.0, 7.0, 9.0, 9.0, 10.0, 11.0, 13.0, 13.0, 13.0, 15.0, 16.0, 16.0, 16.0, 17.0, 19.0, 19.0, 20.0, 21.0, 21.0, 21.0, 22.0, 23.0, 24.0, 24.0, 25.0, 26.0, 27.0, 27.0, 28.0, 28.0, 29.0, 30.0, 31.0, 31.0, 32.0, 32.0, 32.0, 33.0, 33.0, 34.0, 35.0, 35.0, 35.0, 36.0, 36.0, 36.0, 36.0, 37.0, 37.0, 37.0, 38.0, 38.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 41.0, 41.0, 41.0, 41.0, 42.0, 42.0, 43.0, 43.0, 43.0, 44.0, 44.0, 44.0, 45.0, 45.0, 45.0, 45.0, 46.0, 46.0, 46.0, 46.0, 47.0, 47.0, 47.0, 48.0, 48.0, 48.0, 48.0, 49.0, 49.0, 50.0, 50.0, 51.0, 51.0, 51.0, ...]]  \n",
       "2                                                                                                                                                                                                                                                                                                                                                        [[19.0, 0.0, 0.0, 40.0, 36.0, 0.0, 0.0, 24.0, 0.0, 0.0, 35.0, 0.0, 0.0, 0.0, 0.0, 41.0, 0.0, 0.0, 0.0, 24.0, 0.0, 33.0, 0.0, 37.0, 0.0, 0.0, 43.0, 0.0, 31.0, 28.0, 0.0, 37.0, 0.0, 0.0, 28.0, 34.0, 0.0, 0.0, 0.0, 35.0, 0.0, 36.0, 0.0, 29.0, 0.0, 30.0, 35.0, 0.0, 33.0, 0.0, 0.0, 0.0, 37.0, 0.0, 0.0, 0.0, 0.0, 24.0, 0.0, 24.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.0, 0.0, 0.0, 29.0, 20.0, 33.0, 0.0, 0.0, 0.0, 23.0, 0.0, 0.0, 22.0, 0.0, 32.0, 0.0, 34.0, 0.0, 22.0, 0.0, 0.0, 0.0, 0.0, 27.0, 0.0, 0.0, 29.0, 0.0, 0.0, ...], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...]]  \n",
       "3                                                           [[178.1, 160.3, 197.1, 105.2, 283.1, 113.6, 232.1, 212.7, 73.3, 176.9, 161.9, 128.6, 190.5, 223.2, 157.9, 173.1, 273.5, 275.8, 119.2, 174.6, 133.3, 145.0, 150.6, 220.2, 109.7, 155.4, 172.0, 235.6, 218.5, 92.7, 90.7, 162.3, 146.5, 210.1, 214.4, 194.4, 237.3, 255.9, 197.9, 200.2, 120.8, 118.1, 131.8, 225.4, 205.2, 272.5, 181.1, 122.2, 119.6, 109.6, 112.7, 136.3, 185.4, 199.6, 218.2, 259.9, 143.2, 218.2, 249.8, 274.7, 167.8, 182.3, 157.0, 207.7, 250.2, 81.9, 246.8, 103.1, 147.2, 252.7, 192.2, 226.4, 145.5, 178.3, 133.1, 214.6, 203.9, 185.4, 140.0, 240.3, 134.2, 141.1, 47.4, 200.4, 167.6, 221.1, 165.5, 175.3, 146.7, 167.7, 184.5, 202.9, 273.3, 194.8, 187.7, 133.7, 209.1, 260.8, 211.6, 156.8, ...], [0.0, 2.6, 7.9, 17.6, 19.5, 27.0, 34.0, 37.8, 40.9, 45.0, 47.8, 49.9, 51.1, 51.8, 54.7, 55.3, 55.6, 57.5, 58.4, 58.9, 60.0, 61.3, 61.9, 62.4, 62.8, 62.9, 64.9, 67.4, 68.5, 70.7, 70.9, 72.5, 72.8, 75.8, 77.6, 78.6, 80.3, 81.3, 81.7, 82.3, 82.5, 82.6, 83.2, 83.8, 84.8, 85.7, 85.9, 86.0, 86.3, 87.2, 87.7, 88.5, 89.5, 89.7, 90.0, 91.5, 92.3, 92.8, 93.4, 93.8, 94.7, 95.0, 95.5, 95.9, 96.3, 96.8, 97.5, 98.0, 98.2, 98.4, 99.4, 99.9, 100.8, 101.1, 101.7, 102.1, 102.6, 102.8, 103.2, 103.4, 103.5, 103.7, 104.6, 104.7, 104.9, 105.0, 105.3, 105.8, 105.9, 106.4, 106.7, 107.5, 107.8, 108.3, 108.6, 109.1, 109.4, 109.5, 110.1, 110.5, ...]]  \n",
       "4                                                                                                                                  [[110.0, 138.0, 117.0, 61.0, 112.0, 87.0, 122.0, 73.0, 86.0, 128.0, 85.0, 83.0, 108.0, 109.0, 105.0, 85.0, 104.0, 103.0, 142.0, 76.0, 94.0, 72.0, 125.0, 109.0, 148.0, 112.0, 111.0, 131.0, 130.0, 107.0, 90.0, 107.0, 121.0, 126.0, 78.0, 63.0, 103.0, 97.0, 108.0, 105.0, 96.0, 117.0, 82.0, 79.0, 106.0, 105.0, 59.0, 67.0, 104.0, 88.0, 119.0, 97.0, 87.0, 89.0, 76.0, 68.0, 77.0, 88.0, 109.0, 99.0, 88.0, 115.0, 79.0, 85.0, 121.0, 75.0, 129.0, 70.0, 115.0, 97.0, 86.0, 117.0, 92.0, 98.0, 114.0, 108.0, 106.0, 114.0, 101.0, 146.0, 85.0, 92.0, 125.0, 80.0, 100.0, 137.0, 78.0, 96.0, 91.0, 104.0, 97.0, 100.0, 66.0, 116.0, 84.0, 75.0, 127.0, 81.0, 70.0, 93.0, ...], [0.0, 35.0, 40.0, 44.0, 45.0, 49.0, 51.0, 52.0, 53.0, 54.0, 54.0, 55.0, 55.0, 55.0, 56.0, 56.0, 57.0, 57.0, 58.0, 58.0, 59.0, 59.0, 60.0, 61.0, 61.0, 61.0, 61.0, 61.0, 61.0, 62.0, 62.0, 62.0, 63.0, 63.0, 63.0, 63.0, 64.0, 65.0, 65.0, 65.0, 65.0, 65.0, 66.0, 66.0, 66.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 68.0, 68.0, 68.0, 68.0, 69.0, 69.0, 69.0, 69.0, 70.0, 70.0, 70.0, 70.0, 70.0, 70.0, 70.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, ...]]  \n",
       "5    [[212.8, 221.3, 227.8, 341.3, 286.2, 158.6, 292.3, 257.5, 161.4, 102.8, 151.2, 134.0, 259.7, 127.5, 155.0, 203.9, 183.8, 189.5, 228.4, 176.6, 247.8, 194.5, 169.1, 185.3, 223.8, 290.9, 200.2, 194.8, 134.2, 127.8, 207.5, 233.9, 169.9, 248.9, 235.2, 254.9, 176.7, 204.1, 181.5, 244.4, 169.8, 221.5, 284.3, 187.1, 99.5, 253.0, 215.9, 167.2, 278.7, 137.6, 217.7, 172.2, 178.5, 211.4, 169.3, 245.0, 169.8, 348.5, 242.4, 193.5, 247.9, 199.2, 103.1, 196.7, 267.1, 253.8, 187.8, 275.0, 161.9, 221.1, 168.6, 234.7, 217.7, 282.6, 221.2, 96.6, 187.6, 191.4, 196.4, 164.6, 227.3, 249.1, 167.8, 131.1, 154.5, 264.9, 205.5, 262.3, 203.5, 246.8, 351.6, 178.6, 263.6, 209.9, 221.0, 195.3, 106.1, 163.7, 216.9, 215.8, ...], [31.2, 58.9, 60.8, 65.2, 66.5, 71.0, 73.2, 75.3, 77.1, 78.3, 79.3, 80.6, 82.2, 83.9, 87.6, 88.6, 89.7, 90.0, 90.5, 92.0, 93.7, 95.1, 98.3, 101.3, 102.2, 102.6, 103.4, 105.5, 105.7, 106.2, 106.8, 107.9, 108.2, 109.9, 110.2, 110.8, 112.5, 113.2, 113.3, 114.3, 114.5, 114.7, 115.0, 115.7, 116.5, 116.6, 117.0, 117.9, 118.0, 118.5, 118.7, 118.9, 119.3, 119.6, 120.0, 120.3, 120.4, 120.5, 120.7, 121.0, 121.6, 122.2, 122.8, 123.0, 123.4, 123.5, 123.5, 123.9, 123.9, 124.4, 126.0, 126.9, 127.3, 127.8, 128.7, 128.9, 129.1, 129.3, 129.4, 129.8, 130.1, 130.2, 130.7, 131.1, 131.4, 131.7, 131.8, 132.3, 132.5, 132.9, 133.0, 133.4, 133.9, 134.1, 134.3, 134.5, 134.7, 134.9, 135.0, 135.2, ...]]  \n",
       "6                                                                                                                           [[100.0, 92.0, 128.0, 79.0, 86.0, 98.0, 112.0, 103.0, 82.0, 56.0, 82.0, 114.0, 108.0, 86.0, 101.0, 107.0, 68.0, 108.0, 139.0, 114.0, 126.0, 157.0, 126.0, 99.0, 87.0, 92.0, 64.0, 107.0, 103.0, 86.0, 109.0, 115.0, 125.0, 108.0, 100.0, 110.0, 84.0, 129.0, 109.0, 88.0, 101.0, 125.0, 119.0, 112.0, 122.0, 83.0, 116.0, 62.0, 88.0, 108.0, 109.0, 108.0, 128.0, 96.0, 60.0, 122.0, 114.0, 108.0, 106.0, 118.0, 81.0, 97.0, 94.0, 112.0, 118.0, 114.0, 121.0, 129.0, 123.0, 121.0, 116.0, 97.0, 114.0, 110.0, 82.0, 82.0, 99.0, 119.0, 77.0, 83.0, 132.0, 126.0, 90.0, 84.0, 90.0, 99.0, 89.0, 122.0, 78.0, 91.0, 80.0, 46.0, 121.0, 93.0, 147.0, 87.0, 80.0, 112.0, 80.0, 68.0, ...], [12.0, 42.0, 44.0, 48.0, 48.0, 48.0, 50.0, 52.0, 52.0, 53.0, 54.0, 56.0, 56.0, 57.0, 58.0, 58.0, 58.0, 58.0, 59.0, 59.0, 59.0, 60.0, 60.0, 60.0, 60.0, 60.0, 61.0, 61.0, 61.0, 62.0, 62.0, 63.0, 63.0, 63.0, 63.0, 63.0, 64.0, 64.0, 64.0, 65.0, 65.0, 65.0, 65.0, 65.0, 66.0, 66.0, 66.0, 66.0, 66.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 67.0, 68.0, 68.0, 68.0, 69.0, 69.0, 69.0, 69.0, 70.0, 70.0, 70.0, 70.0, 70.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, ...]]  \n",
       "7  [[226.3, 150.4, 214.0, 165.7, 261.7, 187.7, 201.2, 227.8, 239.6, 213.7, 191.0, 210.6, 141.5, 289.3, 189.6, 122.2, 153.8, 223.9, 197.9, 214.4, 219.0, 242.3, 221.2, 205.1, 240.3, 228.4, 233.1, 170.6, 118.9, 225.6, 169.4, 277.4, 238.8, 158.6, 206.2, 160.2, 263.4, 171.3, 281.4, 207.2, 194.1, 103.9, 305.5, 281.1, 189.5, 180.8, 216.3, 194.8, 263.4, 159.7, 152.1, 137.5, 218.3, 72.4, 141.1, 134.4, 215.8, 212.6, 231.8, 299.6, 155.1, 120.2, 211.8, 261.7, 151.0, 213.1, 154.5, 141.1, 142.1, 109.9, 139.8, 133.6, 146.9, 181.0, 131.6, 170.7, 101.7, 144.0, 120.1, 240.7, 122.4, 136.0, 163.1, 230.7, 281.4, 168.9, 213.6, 143.9, 203.4, 203.9, 215.8, 203.8, 165.2, 194.1, 145.7, 280.5, 179.6, 271.7, 153.5, 223.3, ...], [23.2, 45.0, 53.3, 56.6, 61.4, 65.8, 71.1, 75.8, 77.3, 79.3, 80.2, 82.4, 88.2, 89.7, 91.6, 94.3, 94.9, 95.3, 96.4, 97.4, 98.6, 99.3, 100.9, 102.0, 102.1, 103.7, 104.5, 104.7, 104.8, 105.4, 105.6, 107.3, 107.9, 108.1, 108.9, 109.6, 109.6, 109.6, 110.1, 110.4, 111.2, 111.6, 111.7, 112.9, 113.5, 114.2, 114.3, 114.5, 115.7, 116.1, 116.4, 117.0, 117.8, 117.9, 118.0, 118.3, 119.1, 119.1, 119.4, 120.0, 121.0, 121.1, 122.0, 122.3, 122.6, 123.0, 123.4, 124.0, 125.6, 126.3, 126.9, 127.1, 127.4, 127.7, 127.9, 128.2, 128.4, 128.7, 128.9, 129.1, 129.6, 129.6, 129.7, 129.9, 130.6, 130.9, 132.0, 132.5, 132.6, 132.9, 133.1, 133.4, 133.7, 134.0, 134.2, 134.3, 134.6, 134.9, 135.0, 135.0, ...]]  \n",
       "8                                                                                                                                  [[123.0, 120.0, 101.0, 97.0, 129.0, 87.0, 112.0, 119.0, 76.0, 84.0, 131.0, 113.0, 111.0, 83.0, 84.0, 78.0, 67.0, 93.0, 61.0, 91.0, 78.0, 138.0, 104.0, 82.0, 96.0, 91.0, 96.0, 93.0, 105.0, 86.0, 96.0, 94.0, 112.0, 88.0, 107.0, 115.0, 81.0, 84.0, 56.0, 97.0, 63.0, 89.0, 101.0, 112.0, 75.0, 123.0, 106.0, 98.0, 175.0, 121.0, 76.0, 101.0, 107.0, 84.0, 99.0, 121.0, 77.0, 118.0, 78.0, 109.0, 108.0, 113.0, 96.0, 83.0, 114.0, 125.0, 109.0, 92.0, 103.0, 100.0, 87.0, 82.0, 123.0, 98.0, 103.0, 145.0, 107.0, 78.0, 133.0, 106.0, 96.0, 73.0, 107.0, 67.0, 107.0, 108.0, 124.0, 76.0, 110.0, 117.0, 90.0, 116.0, 84.0, 100.0, 110.0, 89.0, 90.0, 117.0, 60.0, 77.0, ...], [44.0, 49.0, 50.0, 51.0, 52.0, 53.0, 53.0, 54.0, 55.0, 57.0, 57.0, 57.0, 58.0, 58.0, 59.0, 59.0, 59.0, 60.0, 60.0, 60.0, 61.0, 61.0, 61.0, 62.0, 62.0, 63.0, 63.0, 63.0, 63.0, 64.0, 64.0, 64.0, 64.0, 65.0, 65.0, 65.0, 66.0, 66.0, 66.0, 66.0, 67.0, 67.0, 67.0, 67.0, 67.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 69.0, 69.0, 69.0, 69.0, 69.0, 70.0, 70.0, 70.0, 70.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 73.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 75.0, ...]]  \n",
       "9                                                                                                                                                                                                                                                                                                                                      [[10.0, 11.2, 9.3, 6.3, 11.3, 10.5, 0.0, 9.7, 8.2, 10.5, 8.5, 11.4, 9.7, 14.5, 8.0, 14.6, 11.0, 7.4, 8.4, 8.8, 11.3, 14.2, 10.4, 4.1, 15.4, 13.9, 8.0, 8.6, 9.4, 9.9, 5.6, 9.2, 8.2, 14.4, 8.0, 17.2, 14.2, 12.3, 6.7, 11.6, 11.9, 11.9, 11.3, 12.9, 13.4, 8.7, 16.9, 9.7, 5.9, 11.0, 6.5, 7.1, 8.0, 11.0, 8.0, 8.4, 7.6, 7.5, 11.6, 10.8, 11.9, 18.0, 7.1, 6.8, 13.0, 8.9, 12.6, 11.2, 7.2, 12.4, 9.4, 10.8, 10.9, 11.4, 6.8, 7.9, 10.5, 10.0, 9.7, 10.6, 8.5, 10.8, 10.5, 7.6, 17.3, 15.4, 12.2, 5.6, 13.7, 7.5, 8.7, 12.8, 12.0, 12.8, 10.0, 5.9, 14.0, 17.0, 7.8, 7.6, ...], [0.0, 0.0, 0.0, 0.0, 0.0, 1.1, 2.0, 2.5, 2.9, 3.3, 3.5, 3.6, 3.7, 3.8, 3.8, 4.1, 4.2, 4.2, 4.3, 4.3, 4.4, 4.5, 4.6, 4.7, 4.7, 4.7, 4.8, 4.8, 4.9, 5.0, 5.0, 5.0, 5.1, 5.1, 5.1, 5.2, 5.3, 5.3, 5.3, 5.3, 5.3, 5.4, 5.4, 5.4, 5.4, 5.5, 5.5, 5.5, 5.5, 5.6, 5.6, 5.6, 5.7, 5.7, 5.8, 5.8, 5.8, 5.8, 5.9, 5.9, 5.9, 5.9, 5.9, 5.9, 5.9, 6.0, 6.0, 6.0, 6.1, 6.1, 6.1, 6.1, 6.2, 6.2, 6.3, 6.3, 6.3, 6.3, 6.3, 6.3, 6.4, 6.4, 6.4, 6.4, 6.4, 6.4, 6.4, 6.5, 6.5, 6.5, 6.5, 6.5, 6.5, 6.6, 6.6, 6.6, 6.6, 6.6, 6.6, 6.6, ...]]  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>completeness</th>\n",
       "      <th>num_constraints.is_non_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Churn</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Account Length</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VMail Message</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Day Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Day Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Eve Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Eve Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Night Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Night Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Intl Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name inferred_type  completeness  num_constraints.is_non_negative\n",
       "0           Churn      Integral           1.0                             True\n",
       "1  Account Length      Integral           1.0                             True\n",
       "2   VMail Message      Integral           1.0                             True\n",
       "3        Day Mins    Fractional           1.0                             True\n",
       "4       Day Calls      Integral           1.0                             True\n",
       "5        Eve Mins    Fractional           1.0                             True\n",
       "6       Eve Calls      Integral           1.0                             True\n",
       "7      Night Mins    Fractional           1.0                             True\n",
       "8     Night Calls      Integral           1.0                             True\n",
       "9       Intl Mins    Fractional           1.0                             True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_df = pd.json_normalize(\n",
    "    baseline_job.suggested_constraints().body_dict[\"features\"]\n",
    ")\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Monitoring Schedule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a model monitoring schedule. Use the baseline resources (constraints and statistics) to compare against the batch transform inference inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from sagemaker.model_monitor import BatchTransformInput\n",
    "from sagemaker.model_monitor import MonitoringDatasetFormat\n",
    "from time import gmtime, strftime\n",
    "\n",
    "statistics_path = \"{}/statistics.json\".format(baseline_results_uri)\n",
    "constraints_path = \"{}/constraints.json\".format(baseline_results_uri)\n",
    "\n",
    "mon_schedule_name = \"DEMO-xgb-churn-pred-model-monitor-schedule-\" + strftime(\n",
    "    \"%Y-%m-%d-%H-%M-%S\", gmtime()\n",
    ")\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    batch_transform_input=BatchTransformInput(\n",
    "        data_captured_destination_s3_uri=s3_capture_upload_path,\n",
    "        destination=\"/opt/ml/processing/input\",\n",
    "        dataset_format=MonitoringDatasetFormat.csv(header=False),\n",
    "    ),\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=statistics_path,\n",
    "    constraints=constraints_path,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Describe and inspect the schedule\n",
    "\n",
    "Once you describe, observe that the MonitoringScheduleStatus changes to Scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule status: Pending\n"
     ]
    }
   ],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print(\"Schedule status: {}\".format(desc_schedule_result[\"MonitoringScheduleStatus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List executions\n",
    "The schedule starts jobs at the previously specified intervals. Here, you list the latest five executions. Note that if you are kicking this off after creating the hourly schedule, you might find the executions empty. You might have to wait until you cross the hour boundary (in UTC) to see executions kick off. The code below has the logic for waiting.\n",
    "\n",
    "Note: Even for an hourly schedule, Amazon SageMaker has a buffer period of 20 minutes to schedule your execution. You might see your execution start in anywhere from zero to ~20 minutes from the hour boundary. This is expected and done for load balancing in the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "We created a hourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer.\n",
      "We will have to wait till we hit the hour...\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "No executions found for schedule. monitoring_schedule_name: DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32\n",
      "Waiting for the 1st execution to happen...\n",
      "Waiting for the 1st execution to happen...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "mon_executions = my_default_monitor.list_executions()\n",
    "print(\n",
    "    \"We created a hourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer.\\nWe will have to wait till we hit the hour...\"\n",
    ")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = my_default_monitor.list_executions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect a specific execution (latest execution)\n",
    "In the previous cell, you picked up the latest completed or failed scheduled execution. Here are the possible terminal states and what each of them mean: \n",
    "* Completed - This means the monitoring execution completed and no issues were found in the violations report.\n",
    "* CompletedWithViolations - This means the execution completed, but constraint violations were detected.\n",
    "* Failed - The monitoring execution failed, maybe due to client error (perhaps incorrect role permissions) or infrastructure issues. Further examination of FailureReason and ExitMessage is necessary to identify what exactly happened.\n",
    "* Stopped - job exceeded max runtime or was manually stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................................!Latest execution status: Completed\n",
      "Latest execution result: CompletedWithViolations: Job completed successfully with 1 violations.\n"
     ]
    }
   ],
   "source": [
    "latest_execution = mon_executions[\n",
    "    -1\n",
    "]  # latest execution's index is -1, second to last is -2 and so on..\n",
    "# time.sleep(60)\n",
    "latest_execution.wait(logs=False)\n",
    "\n",
    "print(\"Latest execution status: {}\".format(latest_execution.describe()[\"ProcessingJobStatus\"]))\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()[\"ExitMessage\"]))\n",
    "\n",
    "latest_job = latest_execution.describe()\n",
    "if latest_job[\"ProcessingJobStatus\"] != \"Completed\":\n",
    "    print(\n",
    "        \"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Uri: s3://sagemaker-us-east-1-369074678854/sagemaker/DEMO-ModelMonitor/reports/DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32/2023/05/14/06\n"
     ]
    }
   ],
   "source": [
    "report_uri = latest_execution.output.destination\n",
    "print(\"Report Uri: {}\".format(report_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the generated reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report bucket: sagemaker-us-east-1-369074678854\n",
      "Report key: sagemaker/DEMO-ModelMonitor/reports/DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32/2023/05/14/06\n",
      "Found Report Files:\n",
      "sagemaker/DEMO-ModelMonitor/reports/DEMO-xgb-churn-pred-model-monitor-schedule-2023-05-14-05-46-32/2023/05/14/06/constraint_violations.json\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip(\"/\")\n",
    "print(\"Report bucket: {}\".format(report_bucket))\n",
    "print(\"Report key: {}\".format(report_key))\n",
    "\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violations report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any violations compared to the baseline, they will be listed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>constraint_check_type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Missing columns</td>\n",
       "      <td>missing_column_check</td>\n",
       "      <td>There are missing columns in current dataset. Number of columns in current dataset: 69, Number of columns in baseline constraints: 70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_name constraint_check_type  \\\n",
       "0  Missing columns  missing_column_check   \n",
       "\n",
       "                                                                                                                             description  \n",
       "0  There are missing columns in current dataset. Number of columns in current dataset: 69, Number of columns in baseline constraints: 70  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "constraints_df = pd.json_normalize(violations.body_dict[\"violations\"])\n",
    "constraints_df.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other commands\n",
    "We can also start and stop the monitoring schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_default_monitor.stop_monitoring_schedule()\n",
    "# my_default_monitor.start_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Delete the resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_default_monitor.stop_monitoring_schedule()\n",
    "# my_default_monitor.delete_monitoring_schedule()\n",
    "# time.sleep(60)  # actually wait for the deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictor.delete_model()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
